From 150d54abfdec7aa8a221d7bb4ed742fef4917822 Mon Sep 17 00:00:00 2001
From: Alyssa Rosenzweig <alyssa@rosenzweig.io>
Date: Mon, 5 Nov 2018 21:23:13 -0800
Subject: [PATCH 073/151] panfrost: Import driver

---
 src/gallium/drivers/panfrost/include/mali-int.h    |   30 +
 src/gallium/drivers/panfrost/include/mali-ioctl.h  | 1017 ++++++
 src/gallium/drivers/panfrost/include/mali-job.h    | 1159 +++++++
 src/gallium/drivers/panfrost/meson.build           |   92 +
 src/gallium/drivers/panfrost/midgard/assemble.py   |  643 ++++
 .../panfrost/midgard/blend-constcolor-zero.asm     |   17 +
 src/gallium/drivers/panfrost/midgard/blend.frag    |   11 +
 src/gallium/drivers/panfrost/midgard/blendify.sh   |    1 +
 src/gallium/drivers/panfrost/midgard/cmdline.c     |  140 +
 src/gallium/drivers/panfrost/midgard/cppwrap.cpp   |    9 +
 src/gallium/drivers/panfrost/midgard/disassemble.c |  952 ++++++
 src/gallium/drivers/panfrost/midgard/disassemble.h |    2 +
 src/gallium/drivers/panfrost/midgard/helpers.h     |  220 ++
 .../drivers/panfrost/midgard/midgard-parse.h       |   70 +
 src/gallium/drivers/panfrost/midgard/midgard.h     |  470 +++
 .../drivers/panfrost/midgard/midgard_compile.c     | 3296 ++++++++++++++++++++
 .../drivers/panfrost/midgard/midgard_compile.h     |   76 +
 src/gallium/drivers/panfrost/midgard/midgard_nir.h |    5 +
 .../panfrost/midgard/midgard_nir_algebraic.py      |   71 +
 src/gallium/drivers/panfrost/pan_allocate.c        |  124 +
 src/gallium/drivers/panfrost/pan_assemble.c        |  170 +
 src/gallium/drivers/panfrost/pan_blend_shaders.c   |  107 +
 src/gallium/drivers/panfrost/pan_blend_shaders.h   |   26 +
 src/gallium/drivers/panfrost/pan_blending.c        |  463 +++
 src/gallium/drivers/panfrost/pan_blending.h        |   24 +
 src/gallium/drivers/panfrost/pan_context.c         | 2798 +++++++++++++++++
 src/gallium/drivers/panfrost/pan_context.h         |  322 ++
 src/gallium/drivers/panfrost/pan_nondrm.c          |   94 +
 src/gallium/drivers/panfrost/pan_nondrm.h          |   72 +
 src/gallium/drivers/panfrost/pan_public.h          |   18 +
 src/gallium/drivers/panfrost/pan_screen.c          |  580 ++++
 src/gallium/drivers/panfrost/pan_screen.h          |   50 +
 src/gallium/drivers/panfrost/pan_slowfb.c          |   73 +
 src/gallium/drivers/panfrost/pan_slowfb.h          |   26 +
 src/gallium/drivers/panfrost/pan_swizzle.c         |  229 ++
 src/gallium/drivers/panfrost/pan_swizzle.h         |   32 +
 src/gallium/drivers/panfrost/pan_texture.c         |  118 +
 src/gallium/drivers/panfrost/pan_texture.h         |   36 +
 src/gallium/drivers/panfrost/panwrap/meson.build   |   18 +
 .../drivers/panfrost/panwrap/panwrap-decoder.c     | 1974 ++++++++++++
 .../drivers/panfrost/panwrap/panwrap-decoder.h     |   25 +
 .../drivers/panfrost/panwrap/panwrap-mmap.c        |  282 ++
 .../drivers/panfrost/panwrap/panwrap-mmap.h        |  132 +
 .../drivers/panfrost/panwrap/panwrap-syscall.c     |  452 +++
 .../drivers/panfrost/panwrap/panwrap-util.c        |  133 +
 .../drivers/panfrost/panwrap/panwrap-util.h        |   58 +
 src/gallium/drivers/panfrost/panwrap/panwrap.h     |   61 +
 47 files changed, 16778 insertions(+)
 create mode 100644 src/gallium/drivers/panfrost/include/mali-int.h
 create mode 100644 src/gallium/drivers/panfrost/include/mali-ioctl.h
 create mode 100644 src/gallium/drivers/panfrost/include/mali-job.h
 create mode 100644 src/gallium/drivers/panfrost/meson.build
 create mode 100644 src/gallium/drivers/panfrost/midgard/assemble.py
 create mode 100644 src/gallium/drivers/panfrost/midgard/blend-constcolor-zero.asm
 create mode 100644 src/gallium/drivers/panfrost/midgard/blend.frag
 create mode 100755 src/gallium/drivers/panfrost/midgard/blendify.sh
 create mode 100644 src/gallium/drivers/panfrost/midgard/cmdline.c
 create mode 100644 src/gallium/drivers/panfrost/midgard/cppwrap.cpp
 create mode 100644 src/gallium/drivers/panfrost/midgard/disassemble.c
 create mode 100644 src/gallium/drivers/panfrost/midgard/disassemble.h
 create mode 100644 src/gallium/drivers/panfrost/midgard/helpers.h
 create mode 100644 src/gallium/drivers/panfrost/midgard/midgard-parse.h
 create mode 100644 src/gallium/drivers/panfrost/midgard/midgard.h
 create mode 100644 src/gallium/drivers/panfrost/midgard/midgard_compile.c
 create mode 100644 src/gallium/drivers/panfrost/midgard/midgard_compile.h
 create mode 100644 src/gallium/drivers/panfrost/midgard/midgard_nir.h
 create mode 100644 src/gallium/drivers/panfrost/midgard/midgard_nir_algebraic.py
 create mode 100644 src/gallium/drivers/panfrost/pan_allocate.c
 create mode 100644 src/gallium/drivers/panfrost/pan_assemble.c
 create mode 100644 src/gallium/drivers/panfrost/pan_blend_shaders.c
 create mode 100644 src/gallium/drivers/panfrost/pan_blend_shaders.h
 create mode 100644 src/gallium/drivers/panfrost/pan_blending.c
 create mode 100644 src/gallium/drivers/panfrost/pan_blending.h
 create mode 100644 src/gallium/drivers/panfrost/pan_context.c
 create mode 100644 src/gallium/drivers/panfrost/pan_context.h
 create mode 100644 src/gallium/drivers/panfrost/pan_nondrm.c
 create mode 100644 src/gallium/drivers/panfrost/pan_nondrm.h
 create mode 100644 src/gallium/drivers/panfrost/pan_public.h
 create mode 100644 src/gallium/drivers/panfrost/pan_screen.c
 create mode 100644 src/gallium/drivers/panfrost/pan_screen.h
 create mode 100644 src/gallium/drivers/panfrost/pan_slowfb.c
 create mode 100644 src/gallium/drivers/panfrost/pan_slowfb.h
 create mode 100644 src/gallium/drivers/panfrost/pan_swizzle.c
 create mode 100644 src/gallium/drivers/panfrost/pan_swizzle.h
 create mode 100644 src/gallium/drivers/panfrost/pan_texture.c
 create mode 100644 src/gallium/drivers/panfrost/pan_texture.h
 create mode 100644 src/gallium/drivers/panfrost/panwrap/meson.build
 create mode 100644 src/gallium/drivers/panfrost/panwrap/panwrap-decoder.c
 create mode 100644 src/gallium/drivers/panfrost/panwrap/panwrap-decoder.h
 create mode 100644 src/gallium/drivers/panfrost/panwrap/panwrap-mmap.c
 create mode 100644 src/gallium/drivers/panfrost/panwrap/panwrap-mmap.h
 create mode 100644 src/gallium/drivers/panfrost/panwrap/panwrap-syscall.c
 create mode 100644 src/gallium/drivers/panfrost/panwrap/panwrap-util.c
 create mode 100644 src/gallium/drivers/panfrost/panwrap/panwrap-util.h
 create mode 100644 src/gallium/drivers/panfrost/panwrap/panwrap.h

diff --git a/src/gallium/drivers/panfrost/include/mali-int.h b/src/gallium/drivers/panfrost/include/mali-int.h
new file mode 100644
index 0000000..77dd9ac
--- /dev/null
+++ b/src/gallium/drivers/panfrost/include/mali-int.h
@@ -0,0 +1,30 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __MALI_INT_H_
+#define __MALI_INT_H_
+
+#include <inttypes.h>
+
+typedef uint8_t  u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+
+typedef int8_t  s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+
+#endif
diff --git a/src/gallium/drivers/panfrost/include/mali-ioctl.h b/src/gallium/drivers/panfrost/include/mali-ioctl.h
new file mode 100644
index 0000000..6c33fe3
--- /dev/null
+++ b/src/gallium/drivers/panfrost/include/mali-ioctl.h
@@ -0,0 +1,1017 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+/**
+ * Definitions for all of the ioctls for the original open source bifrost GPU
+ * kernel driver, written by ARM.
+ */
+
+#ifndef __MALI_IOCTL_H__
+#define __MALI_IOCTL_H__
+
+#include "mali-int.h"
+
+#define MALI_GPU_NUM_TEXTURE_FEATURES_REGISTERS 3
+#define MALI_GPU_MAX_JOB_SLOTS 16
+#define MALI_MAX_COHERENT_GROUPS 16
+
+typedef u8 mali_atom_id;
+
+/**
+ * Since these structs are passed to and from the kernel we need to make sure
+ * that we get the size of each struct to match exactly what the kernel is
+ * expecting. So, when editing this file make sure to add static asserts that
+ * check each struct's size against the arg length you see in strace.
+ */
+
+enum mali_ioctl_mem_flags {
+	/* IN */
+	MALI_MEM_PROT_CPU_RD = (1U << 0),      /**< Read access CPU side */
+	MALI_MEM_PROT_CPU_WR = (1U << 1),      /**< Write access CPU side */
+	MALI_MEM_PROT_GPU_RD = (1U << 2),      /**< Read access GPU side */
+	MALI_MEM_PROT_GPU_WR = (1U << 3),      /**< Write access GPU side */
+	MALI_MEM_PROT_GPU_EX = (1U << 4),      /**< Execute allowed on the GPU
+						    side */
+
+	MALI_MEM_GROW_ON_GPF = (1U << 9),      /**< Grow backing store on GPU
+						    Page Fault */
+
+	MALI_MEM_COHERENT_SYSTEM = (1U << 10), /**< Page coherence Outer
+						    shareable, if available */
+	MALI_MEM_COHERENT_LOCAL = (1U << 11),  /**< Page coherence Inner
+						    shareable */
+	MALI_MEM_CACHED_CPU = (1U << 12),      /**< Should be cached on the
+						    CPU */
+
+	/* IN/OUT */
+	MALI_MEM_SAME_VA = (1U << 13), /**< Must have same VA on both the GPU
+					    and the CPU */
+	/* OUT */
+	MALI_MEM_NEED_MMAP = (1U << 14), /**< Must call mmap to acquire a GPU
+					     address for the alloc */
+	/* IN */
+	MALI_MEM_COHERENT_SYSTEM_REQUIRED = (1U << 15), /**< Page coherence
+					     Outer shareable, required. */
+	MALI_MEM_SECURE = (1U << 16),          /**< Secure memory */
+	MALI_MEM_DONT_NEED = (1U << 17),       /**< Not needed physical
+						    memory */
+	MALI_MEM_IMPORT_SHARED = (1U << 18),   /**< Must use shared CPU/GPU zone
+						    (SAME_VA zone) but doesn't
+						    require the addresses to
+						    be the same */
+};
+
+#define MALI_IOCTL_MEM_FLAGS_IN_MASK                                          \
+	(MALI_MEM_PROT_CPU_RD | MALI_MEM_PROT_CPU_WR |                        \
+	 MALI_MEM_PROT_GPU_RD | MALI_MEM_PROT_GPU_WR | MALI_MEM_PROT_GPU_EX | \
+	 MALI_MEM_GROW_ON_GPF |                                               \
+	 MALI_MEM_COHERENT_SYSTEM | MALI_MEM_COHERENT_LOCAL |                 \
+	 MALI_MEM_CACHED_CPU |                                                \
+	 MALI_MEM_COHERENT_SYSTEM_REQUIRED | MALI_MEM_SECURE |                \
+	 MALI_MEM_DONT_NEED | MALI_MEM_IMPORT_SHARED)
+#define MALI_MEM_MAP_TRACKING_HANDLE (3ull << 12)
+
+enum mali_ioctl_coherency_mode {
+	COHERENCY_ACE_LITE = 0,
+	COHERENCY_ACE      = 1,
+	COHERENCY_NONE     = 31
+};
+
+/*
+ * Mali Atom priority
+ *
+ * Only certain priority levels are actually implemented, as specified by the
+ * MALI_JD_PRIO_<...> definitions below. It is undefined to use a priority
+ * level that is not one of those defined below.
+ *
+ * Priority levels only affect scheduling between atoms of the same type within
+ * a mali context, and only after the atoms have had dependencies resolved.
+ * Fragment atoms does not affect non-frament atoms with lower priorities, and
+ * the other way around. For example, a low priority atom that has had its
+ * dependencies resolved might run before a higher priority atom that has not
+ * had its dependencies resolved.
+ *
+ * The scheduling between mali contexts/processes and between atoms from
+ * different mali contexts/processes is unaffected by atom priority.
+ *
+ * The atoms are scheduled as follows with respect to their priorities:
+ * - Let atoms 'X' and 'Y' be for the same job slot who have dependencies
+ *   resolved, and atom 'X' has a higher priority than atom 'Y'
+ * - If atom 'Y' is currently running on the HW, then it is interrupted to
+ *   allow atom 'X' to run soon after
+ * - If instead neither atom 'Y' nor atom 'X' are running, then when choosing
+ *   the next atom to run, atom 'X' will always be chosen instead of atom 'Y'
+ * - Any two atoms that have the same priority could run in any order with
+ *   respect to each other. That is, there is no ordering constraint between
+ *   atoms of the same priority.
+ */
+typedef u8 mali_jd_prio;
+#define MALI_JD_PRIO_MEDIUM  ((mali_jd_prio)0)
+#define MALI_JD_PRIO_HIGH    ((mali_jd_prio)1)
+#define MALI_JD_PRIO_LOW     ((mali_jd_prio)2)
+
+/**
+ * @brief Job dependency type.
+ *
+ * A flags field will be inserted into the atom structure to specify whether a
+ * dependency is a data or ordering dependency (by putting it before/after
+ * 'core_req' in the structure it should be possible to add without changing
+ * the structure size).  When the flag is set for a particular dependency to
+ * signal that it is an ordering only dependency then errors will not be
+ * propagated.
+ */
+typedef u8 mali_jd_dep_type;
+#define MALI_JD_DEP_TYPE_INVALID  (0)       /**< Invalid dependency */
+#define MALI_JD_DEP_TYPE_DATA     (1U << 0) /**< Data dependency */
+#define MALI_JD_DEP_TYPE_ORDER    (1U << 1) /**< Order dependency */
+
+/**
+ * @brief Job chain hardware requirements.
+ *
+ * A job chain must specify what GPU features it needs to allow the
+ * driver to schedule the job correctly.  By not specifying the
+ * correct settings can/will cause an early job termination.  Multiple
+ * values can be ORed together to specify multiple requirements.
+ * Special case is ::MALI_JD_REQ_DEP, which is used to express complex
+ * dependencies, and that doesn't execute anything on the hardware.
+ */
+typedef u32 mali_jd_core_req;
+
+/* Requirements that come from the HW */
+
+/**
+ * No requirement, dependency only
+ */
+#define MALI_JD_REQ_DEP ((mali_jd_core_req)0)
+
+/**
+ * Requires fragment shaders
+ */
+#define MALI_JD_REQ_FS  ((mali_jd_core_req)1 << 0)
+
+/**
+ * Requires compute shaders
+ * This covers any of the following Midgard Job types:
+ * - Vertex Shader Job
+ * - Geometry Shader Job
+ * - An actual Compute Shader Job
+ *
+ * Compare this with @ref MALI_JD_REQ_ONLY_COMPUTE, which specifies that the
+ * job is specifically just the "Compute Shader" job type, and not the "Vertex
+ * Shader" nor the "Geometry Shader" job type.
+ */
+#define MALI_JD_REQ_CS  ((mali_jd_core_req)1 << 1)
+#define MALI_JD_REQ_T   ((mali_jd_core_req)1 << 2)   /**< Requires tiling */
+#define MALI_JD_REQ_CF  ((mali_jd_core_req)1 << 3)   /**< Requires cache flushes */
+#define MALI_JD_REQ_V   ((mali_jd_core_req)1 << 4)   /**< Requires value writeback */
+
+/* SW-only requirements - the HW does not expose these as part of the job slot
+ * capabilities */
+
+/* Requires fragment job with AFBC encoding */
+#define MALI_JD_REQ_FS_AFBC  ((mali_jd_core_req)1 << 13)
+
+/**
+ * SW-only requirement: coalesce completion events.
+ * If this bit is set then completion of this atom will not cause an event to
+ * be sent to userspace, whether successful or not; completion events will be
+ * deferred until an atom completes which does not have this bit set.
+ *
+ * This bit may not be used in combination with MALI_JD_REQ_EXTERNAL_RESOURCES.
+ */
+#define MALI_JD_REQ_EVENT_COALESCE ((mali_jd_core_req)1 << 5)
+
+/**
+ * SW Only requirement: the job chain requires a coherent core group. We don't
+ * mind which coherent core group is used.
+ */
+#define MALI_JD_REQ_COHERENT_GROUP  ((mali_jd_core_req)1 << 6)
+
+/**
+ * SW Only requirement: The performance counters should be enabled only when
+ * they are needed, to reduce power consumption.
+ */
+
+#define MALI_JD_REQ_PERMON               ((mali_jd_core_req)1 << 7)
+
+/**
+ * SW Only requirement: External resources are referenced by this atom.  When
+ * external resources are referenced no syncsets can be bundled with the atom
+ * but should instead be part of a NULL jobs inserted into the dependency
+ * tree.  The first pre_dep object must be configured for the external
+ * resouces to use, the second pre_dep object can be used to create other
+ * dependencies.
+ *
+ * This bit may not be used in combination with MALI_JD_REQ_EVENT_COALESCE.
+ */
+#define MALI_JD_REQ_EXTERNAL_RESOURCES   ((mali_jd_core_req)1 << 8)
+
+/**
+ * SW Only requirement: Software defined job. Jobs with this bit set will not
+ * be submitted to the hardware but will cause some action to happen within
+ * the driver
+ */
+#define MALI_JD_REQ_SOFT_JOB        ((mali_jd_core_req)1 << 9)
+
+#define MALI_JD_REQ_SOFT_DUMP_CPU_GPU_TIME      (MALI_JD_REQ_SOFT_JOB | 0x1)
+#define MALI_JD_REQ_SOFT_FENCE_TRIGGER          (MALI_JD_REQ_SOFT_JOB | 0x2)
+#define MALI_JD_REQ_SOFT_FENCE_WAIT             (MALI_JD_REQ_SOFT_JOB | 0x3)
+
+/**
+ * SW Only requirement : Replay job.
+ *
+ * If the preceding job fails, the replay job will cause the jobs specified in
+ * the list of mali_jd_replay_payload pointed to by the jc pointer to be
+ * replayed.
+ *
+ * A replay job will only cause jobs to be replayed up to MALIP_JD_REPLAY_LIMIT
+ * times. If a job fails more than MALIP_JD_REPLAY_LIMIT times then the replay
+ * job is failed, as well as any following dependencies.
+ *
+ * The replayed jobs will require a number of atom IDs. If there are not enough
+ * free atom IDs then the replay job will fail.
+ *
+ * If the preceding job does not fail, then the replay job is returned as
+ * completed.
+ *
+ * The replayed jobs will never be returned to userspace. The preceding failed
+ * job will be returned to userspace as failed; the status of this job should
+ * be ignored. Completion should be determined by the status of the replay soft
+ * job.
+ *
+ * In order for the jobs to be replayed, the job headers will have to be
+ * modified. The Status field will be reset to NOT_STARTED. If the Job Type
+ * field indicates a Vertex Shader Job then it will be changed to Null Job.
+ *
+ * The replayed jobs have the following assumptions :
+ *
+ * - No external resources. Any required external resources will be held by the
+ *   replay atom.
+ * - Pre-dependencies are created based on job order.
+ * - Atom numbers are automatically assigned.
+ * - device_nr is set to 0. This is not relevant as
+ *   MALI_JD_REQ_SPECIFIC_COHERENT_GROUP should not be set.
+ * - Priority is inherited from the replay job.
+ */
+#define MALI_JD_REQ_SOFT_REPLAY                 (MALI_JD_REQ_SOFT_JOB | 0x4)
+/**
+ * SW only requirement: event wait/trigger job.
+ *
+ * - MALI_JD_REQ_SOFT_EVENT_WAIT: this job will block until the event is set.
+ * - MALI_JD_REQ_SOFT_EVENT_SET: this job sets the event, thus unblocks the
+ *   other waiting jobs. It completes immediately.
+ * - MALI_JD_REQ_SOFT_EVENT_RESET: this job resets the event, making it
+ *   possible for other jobs to wait upon. It completes immediately.
+ */
+#define MALI_JD_REQ_SOFT_EVENT_WAIT             (MALI_JD_REQ_SOFT_JOB | 0x5)
+#define MALI_JD_REQ_SOFT_EVENT_SET              (MALI_JD_REQ_SOFT_JOB | 0x6)
+#define MALI_JD_REQ_SOFT_EVENT_RESET            (MALI_JD_REQ_SOFT_JOB | 0x7)
+
+#define MALI_JD_REQ_SOFT_DEBUG_COPY             (MALI_JD_REQ_SOFT_JOB | 0x8)
+
+/**
+ * SW only requirement: Just In Time allocation
+ *
+ * This job requests a JIT allocation based on the request in the
+ * @base_jit_alloc_info structure which is passed via the jc element of
+ * the atom.
+ *
+ * It should be noted that the id entry in @base_jit_alloc_info must not
+ * be reused until it has been released via @MALI_JD_REQ_SOFT_JIT_FREE.
+ *
+ * Should this soft job fail it is expected that a @MALI_JD_REQ_SOFT_JIT_FREE
+ * soft job to free the JIT allocation is still made.
+ *
+ * The job will complete immediately.
+ */
+#define MALI_JD_REQ_SOFT_JIT_ALLOC              (MALI_JD_REQ_SOFT_JOB | 0x9)
+/**
+ * SW only requirement: Just In Time free
+ *
+ * This job requests a JIT allocation created by @MALI_JD_REQ_SOFT_JIT_ALLOC
+ * to be freed. The ID of the JIT allocation is passed via the jc element of
+ * the atom.
+ *
+ * The job will complete immediately.
+ */
+#define MALI_JD_REQ_SOFT_JIT_FREE               (MALI_JD_REQ_SOFT_JOB | 0xa)
+
+/**
+ * SW only requirement: Map external resource
+ *
+ * This job requests external resource(s) are mapped once the dependencies
+ * of the job have been satisfied. The list of external resources are
+ * passed via the jc element of the atom which is a pointer to a
+ * @base_external_resource_list.
+ */
+#define MALI_JD_REQ_SOFT_EXT_RES_MAP            (MALI_JD_REQ_SOFT_JOB | 0xb)
+/**
+ * SW only requirement: Unmap external resource
+ *
+ * This job requests external resource(s) are unmapped once the dependencies
+ * of the job has been satisfied. The list of external resources are
+ * passed via the jc element of the atom which is a pointer to a
+ * @base_external_resource_list.
+ */
+#define MALI_JD_REQ_SOFT_EXT_RES_UNMAP          (MALI_JD_REQ_SOFT_JOB | 0xc)
+
+/**
+ * HW Requirement: Requires Compute shaders (but not Vertex or Geometry Shaders)
+ *
+ * This indicates that the Job Chain contains Midgard Jobs of the 'Compute
+ * Shaders' type.
+ *
+ * In contrast to @ref MALI_JD_REQ_CS, this does \b not indicate that the Job
+ * Chain contains 'Geometry Shader' or 'Vertex Shader' jobs.
+ */
+#define MALI_JD_REQ_ONLY_COMPUTE    ((mali_jd_core_req)1 << 10)
+
+/**
+ * HW Requirement: Use the mali_jd_atom::device_nr field to specify a
+ * particular core group
+ *
+ * If both @ref MALI_JD_REQ_COHERENT_GROUP and this flag are set, this flag
+ * takes priority
+ *
+ * This is only guaranteed to work for @ref MALI_JD_REQ_ONLY_COMPUTE atoms.
+ *
+ * If the core availability policy is keeping the required core group turned
+ * off, then the job will fail with a @ref MALI_JD_EVENT_PM_EVENT error code.
+ */
+#define MALI_JD_REQ_SPECIFIC_COHERENT_GROUP ((mali_jd_core_req)1 << 11)
+
+/**
+ * SW Flag: If this bit is set then the successful completion of this atom
+ * will not cause an event to be sent to userspace
+ */
+#define MALI_JD_REQ_EVENT_ONLY_ON_FAILURE   ((mali_jd_core_req)1 << 12)
+
+/**
+ * SW Flag: If this bit is set then completion of this atom will not cause an
+ * event to be sent to userspace, whether successful or not.
+ */
+#define MALI_JD_REQ_EVENT_NEVER ((mali_jd_core_req)1 << 14)
+
+/**
+ * SW Flag: Skip GPU cache clean and invalidation before starting a GPU job.
+ *
+ * If this bit is set then the GPU's cache will not be cleaned and invalidated
+ * until a GPU job starts which does not have this bit set or a job completes
+ * which does not have the @ref MALI_JD_REQ_SKIP_CACHE_END bit set. Do not use if
+ * the CPU may have written to memory addressed by the job since the last job
+ * without this bit set was submitted.
+ */
+#define MALI_JD_REQ_SKIP_CACHE_START ((mali_jd_core_req)1 << 15)
+
+/**
+ * SW Flag: Skip GPU cache clean and invalidation after a GPU job completes.
+ *
+ * If this bit is set then the GPU's cache will not be cleaned and invalidated
+ * until a GPU job completes which does not have this bit set or a job starts
+ * which does not have the @ref MALI_JD_REQ_SKIP_CACHE_START bti set. Do not
+ * use if the CPU may read from or partially overwrite memory addressed by the
+ * job before the next job without this bit set completes.
+ */
+#define MALI_JD_REQ_SKIP_CACHE_END ((mali_jd_core_req)1 << 16)
+
+/**
+ * These requirement bits are currently unused in mali_jd_core_req
+ */
+#define MALIP_JD_REQ_RESERVED \
+	(~(MALI_JD_REQ_ATOM_TYPE | MALI_JD_REQ_EXTERNAL_RESOURCES | \
+	MALI_JD_REQ_EVENT_ONLY_ON_FAILURE | MALIP_JD_REQ_EVENT_NEVER | \
+	MALI_JD_REQ_EVENT_COALESCE | \
+	MALI_JD_REQ_COHERENT_GROUP | MALI_JD_REQ_SPECIFIC_COHERENT_GROUP | \
+	MALI_JD_REQ_FS_AFBC | MALI_JD_REQ_PERMON | \
+	MALI_JD_REQ_SKIP_CACHE_START | MALI_JD_REQ_SKIP_CACHE_END))
+
+/**
+ * Mask of all bits in mali_jd_core_req that control the type of the atom.
+ *
+ * This allows dependency only atoms to have flags set
+ */
+#define MALI_JD_REQ_ATOM_TYPE \
+	(MALI_JD_REQ_FS | MALI_JD_REQ_CS | MALI_JD_REQ_T | MALI_JD_REQ_CF | \
+	MALI_JD_REQ_V | MALI_JD_REQ_SOFT_JOB | MALI_JD_REQ_ONLY_COMPUTE)
+
+/**
+ * Mask of all bits in mali_jd_core_req that control the type of a soft job.
+ */
+#define MALI_JD_REQ_SOFT_JOB_TYPE (MALI_JD_REQ_SOFT_JOB | 0x1f)
+
+/*
+ * Returns non-zero value if core requirements passed define a soft job or
+ * a dependency only job.
+ */
+#define MALI_JD_REQ_SOFT_JOB_OR_DEP(core_req) \
+	((core_req & MALI_JD_REQ_SOFT_JOB) || \
+	(core_req & MALI_JD_REQ_ATOM_TYPE) == MALI_JD_REQ_DEP)
+
+/**
+ * @brief The payload for a replay job. This must be in GPU memory.
+ */
+struct mali_jd_replay_payload {
+	/**
+	 * Pointer to the first entry in the mali_jd_replay_jc list.  These
+	 * will be replayed in @b reverse order (so that extra ones can be added
+	 * to the head in future soft jobs without affecting this soft job)
+	 */
+	u64 tiler_jc_list;
+
+	/**
+	 * Pointer to the fragment job chain.
+	 */
+	u64 fragment_jc;
+
+	/**
+	 * Pointer to the tiler heap free FBD field to be modified.
+	 */
+	u64 tiler_heap_free;
+
+	/**
+	 * Hierarchy mask for the replayed fragment jobs. May be zero.
+	 */
+	u16 fragment_hierarchy_mask;
+
+	/**
+	 * Hierarchy mask for the replayed tiler jobs. May be zero.
+	 */
+	u16 tiler_hierarchy_mask;
+
+	/**
+	 * Default weight to be used for hierarchy levels not in the original
+	 * mask.
+	 */
+	u32 hierarchy_default_weight;
+
+	/**
+	 * Core requirements for the tiler job chain
+	 */
+	mali_jd_core_req tiler_core_req;
+
+	/**
+	 * Core requirements for the fragment job chain
+	 */
+	mali_jd_core_req fragment_core_req;
+};
+
+/**
+ * @brief An entry in the linked list of job chains to be replayed. This must
+ *        be in GPU memory.
+ */
+struct mali_jd_replay_jc {
+	/**
+	 * Pointer to next entry in the list. A setting of NULL indicates the
+	 * end of the list.
+	 */
+	u64 next;
+
+	/**
+	 * Pointer to the job chain.
+	 */
+	u64 jc;
+};
+
+/* Capabilities of a job slot as reported by JS_FEATURES registers */
+
+#define JS_FEATURE_NULL_JOB              (1u << 1)
+#define JS_FEATURE_SET_VALUE_JOB         (1u << 2)
+#define JS_FEATURE_CACHE_FLUSH_JOB       (1u << 3)
+#define JS_FEATURE_COMPUTE_JOB           (1u << 4)
+#define JS_FEATURE_VERTEX_JOB            (1u << 5)
+#define JS_FEATURE_GEOMETRY_JOB          (1u << 6)
+#define JS_FEATURE_TILER_JOB             (1u << 7)
+#define JS_FEATURE_FUSED_JOB             (1u << 8)
+#define JS_FEATURE_FRAGMENT_JOB          (1u << 9)
+
+struct mali_gpu_core_props {
+	/**
+	 * Product specific value.
+	 */
+	u32 product_id;
+
+	/**
+	 * Status of the GPU release.
+	 * No defined values, but starts at 0 and increases by one for each
+	 * release status (alpha, beta, EAC, etc.).
+	 * 4 bit values (0-15).
+	 */
+	u16 version_status;
+
+	/**
+	 * Minor release number of the GPU. "P" part of an "RnPn" release
+	 * number.
+	 * 8 bit values (0-255).
+	 */
+	u16 minor_revision;
+
+	/**
+	 * Major release number of the GPU. "R" part of an "RnPn" release
+	 * number.
+	 * 4 bit values (0-15).
+	 */
+	u16 major_revision;
+
+	u16 :16;
+
+	/**
+	 * @usecase GPU clock speed is not specified in the Midgard
+	 * Architecture, but is <b>necessary for OpenCL's clGetDeviceInfo()
+	 * function</b>.
+	 */
+	u32 gpu_speed_mhz;
+
+	/**
+	 * @usecase GPU clock max/min speed is required for computing
+	 * best/worst case in tasks as job scheduling ant irq_throttling. (It
+	 * is not specified in the Midgard Architecture).
+	 */
+	u32 gpu_freq_khz_max;
+	u32 gpu_freq_khz_min;
+
+	/**
+	 * Size of the shader program counter, in bits.
+	 */
+	u32 log2_program_counter_size;
+
+	/**
+	 * TEXTURE_FEATURES_x registers, as exposed by the GPU. This is a
+	 * bitpattern where a set bit indicates that the format is supported.
+	 *
+	 * Before using a texture format, it is recommended that the
+	 * corresponding bit be checked.
+	 */
+	u32 texture_features[MALI_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+
+	/**
+	 * Theoretical maximum memory available to the GPU. It is unlikely
+	 * that a client will be able to allocate all of this memory for their
+	 * own purposes, but this at least provides an upper bound on the
+	 * memory available to the GPU.
+	 *
+	 * This is required for OpenCL's clGetDeviceInfo() call when
+	 * CL_DEVICE_GLOBAL_MEM_SIZE is requested, for OpenCL GPU devices. The
+	 * client will not be expecting to allocate anywhere near this value.
+	 */
+	u64 gpu_available_memory_size;
+};
+
+struct mali_gpu_l2_cache_props {
+	u8 log2_line_size;
+	u8 log2_cache_size;
+	u8 num_l2_slices; /* Number of L2C slices. 1 or higher */
+	u64 :40;
+};
+
+struct mali_gpu_tiler_props {
+	u32 bin_size_bytes;	/* Max is 4*2^15 */
+	u32 max_active_levels;	/* Max is 2^15 */
+};
+
+struct mali_gpu_thread_props {
+	u32 max_threads;            /* Max. number of threads per core */
+	u32 max_workgroup_size;     /* Max. number of threads per workgroup */
+	u32 max_barrier_size;       /* Max. number of threads that can
+				       synchronize on a simple barrier */
+	u16 max_registers;          /* Total size [1..65535] of the register
+				       file available per core. */
+	u8  max_task_queue;         /* Max. tasks [1..255] which may be sent
+				       to a core before it becomes blocked. */
+	u8  max_thread_group_split; /* Max. allowed value [1..15] of the
+				       Thread Group Split field. */
+	enum {
+		MALI_GPU_IMPLEMENTATION_UNKNOWN = 0,
+		MALI_GPU_IMPLEMENTATION_SILICON = 1,
+		MALI_GPU_IMPLEMENTATION_FPGA    = 2,
+		MALI_GPU_IMPLEMENTATION_SW      = 3,
+	} impl_tech :8;
+	u64 :56;
+};
+
+/**
+ * @brief descriptor for a coherent group
+ *
+ * \c core_mask exposes all cores in that coherent group, and \c num_cores
+ * provides a cached population-count for that mask.
+ *
+ * @note Whilst all cores are exposed in the mask, not all may be available to
+ * the application, depending on the Kernel Power policy.
+ *
+ * @note if u64s must be 8-byte aligned, then this structure has 32-bits of
+ * wastage.
+ */
+struct mali_ioctl_gpu_coherent_group {
+	u64 core_mask;	       /**< Core restriction mask required for the
+				 group */
+	u16 num_cores;	       /**< Number of cores in the group */
+	u64 :48;
+};
+
+/**
+ * @brief Coherency group information
+ *
+ * Note that the sizes of the members could be reduced. However, the \c group
+ * member might be 8-byte aligned to ensure the u64 core_mask is 8-byte
+ * aligned, thus leading to wastage if the other members sizes were reduced.
+ *
+ * The groups are sorted by core mask. The core masks are non-repeating and do
+ * not intersect.
+ */
+struct mali_gpu_coherent_group_info {
+	u32 num_groups;
+
+	/**
+	 * Number of core groups (coherent or not) in the GPU. Equivalent to
+	 * the number of L2 Caches.
+	 *
+	 * The GPU Counter dumping writes 2048 bytes per core group,
+	 * regardless of whether the core groups are coherent or not. Hence
+	 * this member is needed to calculate how much memory is required for
+	 * dumping.
+	 *
+	 * @note Do not use it to work out how many valid elements are in the
+	 * group[] member. Use num_groups instead.
+	 */
+	u32 num_core_groups;
+
+	/**
+	 * Coherency features of the memory, accessed by @ref gpu_mem_features
+	 * methods
+	 */
+	u32 coherency;
+
+	u32 :32;
+
+	/**
+	 * Descriptors of coherent groups
+	 */
+	struct mali_ioctl_gpu_coherent_group group[MALI_MAX_COHERENT_GROUPS];
+};
+
+/**
+ * A complete description of the GPU's Hardware Configuration Discovery
+ * registers.
+ *
+ * The information is presented inefficiently for access. For frequent access,
+ * the values should be better expressed in an unpacked form in the
+ * base_gpu_props structure.
+ *
+ * @usecase The raw properties in @ref gpu_raw_gpu_props are necessary to
+ * allow a user of the Mali Tools (e.g. PAT) to determine "Why is this device
+ * behaving differently?". In this case, all information about the
+ * configuration is potentially useful, but it <b>does not need to be processed
+ * by the driver</b>. Instead, the raw registers can be processed by the Mali
+ * Tools software on the host PC.
+ *
+ */
+struct mali_gpu_raw_props {
+	u64 shader_present;
+	u64 tiler_present;
+	u64 l2_present;
+	u64 stack_present;
+
+	u32 l2_features;
+	u32 suspend_size; /* API 8.2+ */
+	u32 mem_features;
+	u32 mmu_features;
+
+	u32 as_present;
+
+	u32 js_present;
+	u32 js_features[MALI_GPU_MAX_JOB_SLOTS];
+	u32 tiler_features;
+	u32 texture_features[3];
+
+	u32 gpu_id;
+
+	u32 thread_max_threads;
+	u32 thread_max_workgroup_size;
+	u32 thread_max_barrier_size;
+	u32 thread_features;
+
+	/*
+	 * Note: This is the _selected_ coherency mode rather than the
+	 * available modes as exposed in the coherency_features register.
+	 */
+	u32 coherency_mode;
+};
+
+
+typedef u64 mali_ptr;
+
+#define MALI_PTR_FMT "0x%" PRIx64
+#define MALI_SHORT_PTR_FMT "0x%" PRIxPTR
+
+#ifdef __LP64__
+#define PAD_CPU_PTR(p) p
+#else
+#define PAD_CPU_PTR(p) p; u32 :32;
+#endif
+
+/* FIXME: Again, they don't specify any of these as packed structs. However,
+ * looking at these structs I'm worried that there is already spots where the
+ * compiler is potentially sticking in padding...
+ * Going to try something a little crazy, and just hope that our compiler
+ * happens to add the same kind of offsets since we can't really compare sizes
+ */
+
+/*
+ * Blob provided by the driver to store callback driver, not actually modified
+ * by the driver itself
+ */
+struct mali_jd_udata {
+	u64 blob[2];
+};
+
+struct mali_jd_dependency {
+	mali_atom_id  atom_id;               /**< An atom number */
+	mali_jd_dep_type dependency_type;    /**< Dependency type */
+};
+
+#define MALI_EXT_RES_MAX 10
+
+/* The original header never explicitly defines any values for these. In C,
+ * this -should- expand to SHARED == 0 and EXCLUSIVE == 1, so the only flag we
+ * actually need to decode here is EXCLUSIVE
+ */
+enum mali_external_resource_access {
+	MALI_EXT_RES_ACCESS_SHARED,
+	MALI_EXT_RES_ACCESS_EXCLUSIVE,
+};
+
+/* An aligned address to the resource | mali_external_resource_access */
+typedef u64 mali_external_resource;
+
+struct mali_jd_atom_v2 {
+	mali_ptr jc;           /**< job-chain GPU address */
+	struct mali_jd_udata udata;	    /**< user data */
+	PAD_CPU_PTR(mali_external_resource *ext_res_list); /**< list of external resources */
+	u16 nr_ext_res;			    /**< nr of external resources */
+	u16 compat_core_req;	            /**< core requirements which
+					      correspond to the legacy support
+					      for UK 10.2 */
+	struct mali_jd_dependency pre_dep[2];  /**< pre-dependencies, one need to
+					      use SETTER function to assign
+					      this field, this is done in
+					      order to reduce possibility of
+					      improper assigment of a
+					      dependency field */
+	mali_atom_id atom_number;	    /**< unique number to identify the
+					      atom */
+	mali_jd_prio prio;                  /**< Atom priority. Refer to @ref
+					      mali_jd_prio for more details */
+	u8 device_nr;			    /**< coregroup when
+					      BASE_JD_REQ_SPECIFIC_COHERENT_GROUP
+					      specified */
+	u8 :8;
+	mali_jd_core_req core_req;          /**< core requirements */
+} __attribute__((packed));
+
+/**
+ * enum mali_error - Mali error codes shared with userspace
+ *
+ * This is subset of those common Mali errors that can be returned to userspace.
+ * Values of matching user and kernel space enumerators MUST be the same.
+ * MALI_ERROR_NONE is guaranteed to be 0.
+ *
+ * @MALI_ERROR_NONE: Success
+ * @MALI_ERROR_OUT_OF_GPU_MEMORY: Not used in the kernel driver
+ * @MALI_ERROR_OUT_OF_MEMORY: Memory allocation failure
+ * @MALI_ERROR_FUNCTION_FAILED: Generic error code
+ */
+enum mali_error {
+	MALI_ERROR_NONE = 0,
+	MALI_ERROR_OUT_OF_GPU_MEMORY,
+	MALI_ERROR_OUT_OF_MEMORY,
+	MALI_ERROR_FUNCTION_FAILED,
+};
+
+/**
+ * Header used by all ioctls
+ */
+union mali_ioctl_header {
+	/* [in] The ID of the UK function being called */
+	u32 id :32;
+	/* [out] The return value of the UK function that was called */
+	enum mali_error rc :32;
+
+	u64 :64;
+} __attribute__((packed));
+
+struct mali_ioctl_get_version {
+	union mali_ioctl_header header;
+	u16 major; /* [out] */
+	u16 minor; /* [out] */
+	u32 :32;
+} __attribute__((packed));
+
+struct mali_ioctl_mem_alloc {
+	union mali_ioctl_header header;
+	/* [in] */
+	u64 va_pages;
+	u64 commit_pages;
+	u64 extent;
+	/* [in/out] */
+	u64 flags;
+	/* [out] */
+	mali_ptr gpu_va;
+	u16 va_alignment;
+
+	u32 :32;
+	u16 :16;
+} __attribute__((packed));
+
+struct mali_mem_import_user_buffer {
+	u64 ptr;
+	u64 length;
+};
+
+struct mali_ioctl_mem_import {
+	union mali_ioctl_header header;
+	/* [in] */
+	u64 phandle;
+	enum {
+		MALI_MEM_IMPORT_TYPE_INVALID = 0,
+		MALI_MEM_IMPORT_TYPE_UMP = 1,
+		MALI_MEM_IMPORT_TYPE_UMM = 2,
+		MALI_MEM_IMPORT_TYPE_USER_BUFFER = 3,
+	} type :32;
+	u32 :32;
+	/* [in/out] */
+	u64 flags;
+	/* [out] */
+	u64 gpu_va;
+	u64 va_pages;
+} __attribute__((packed));
+
+struct mali_ioctl_mem_commit {
+	union mali_ioctl_header header;
+	/* [in] */
+	mali_ptr gpu_addr;
+	u64 pages;
+	/* [out] */
+	u32 result_subcode;
+	u32 :32;
+} __attribute__((packed));
+
+enum mali_ioctl_mem_query_type {
+	MALI_MEM_QUERY_COMMIT_SIZE = 1,
+	MALI_MEM_QUERY_VA_SIZE     = 2,
+	MALI_MEM_QUERY_FLAGS       = 3
+};
+
+struct mali_ioctl_mem_query {
+	union mali_ioctl_header header;
+	/* [in] */
+	mali_ptr gpu_addr;
+	enum mali_ioctl_mem_query_type query : 32;
+	u32 :32;
+	/* [out] */
+	u64 value;
+} __attribute__((packed));
+
+struct mali_ioctl_mem_free {
+	union mali_ioctl_header header;
+	mali_ptr gpu_addr; /* [in] */
+} __attribute__((packed));
+/* FIXME: Size unconfirmed (haven't seen in a trace yet) */
+
+struct mali_ioctl_mem_flags_change {
+	union mali_ioctl_header header;
+	/* [in] */
+	mali_ptr gpu_va;
+	u64 flags;
+	u64 mask;
+} __attribute__((packed));
+/* FIXME: Size unconfirmed (haven't seen in a trace yet) */
+
+struct mali_ioctl_mem_alias {
+	union mali_ioctl_header header;
+	/* [in/out] */
+	u64 flags;
+	/* [in] */
+	u64 stride;
+	u64 nents;
+	u64 ai;
+	/* [out] */
+	mali_ptr gpu_va;
+	u64 va_pages;
+} __attribute__((packed));
+
+struct mali_ioctl_sync {
+	union mali_ioctl_header header;
+	mali_ptr handle;
+	PAD_CPU_PTR(void* user_addr);
+	u64 size;
+	enum {
+		MALI_SYNC_TO_DEVICE = 1,
+		MALI_SYNC_TO_CPU = 2,
+	} type :8;
+	u64 :56;
+} __attribute__((packed));
+
+struct mali_ioctl_gpu_props_reg_dump {
+	union mali_ioctl_header header;
+	struct mali_gpu_core_props core;
+	struct mali_gpu_l2_cache_props l2;
+	u64 :64;
+	struct mali_gpu_tiler_props tiler;
+	struct mali_gpu_thread_props thread;
+
+	struct mali_gpu_raw_props raw;
+
+	/** This must be last member of the structure */
+	struct mali_gpu_coherent_group_info coherency_info;
+} __attribute__((packed));
+
+struct mali_ioctl_set_flags {
+	union mali_ioctl_header header;
+	u32 create_flags; /* [in] */
+	u32 :32;
+} __attribute__((packed));
+
+struct mali_ioctl_stream_create {
+	union mali_ioctl_header header;
+	/* [in] */
+	char name[32];
+	/* [out] */
+	s32 fd;
+	u32 :32;
+} __attribute__((packed));
+
+struct mali_ioctl_job_submit {
+	union mali_ioctl_header header;
+	/* [in] */
+	PAD_CPU_PTR(struct mali_jd_atom_v2 *addr);
+	u32 nr_atoms;
+	u32 stride;
+} __attribute__((packed));
+
+struct mali_ioctl_get_context_id {
+	union mali_ioctl_header header;
+	/* [out] */
+	s64 id;
+} __attribute__((packed));
+
+#undef PAD_PTR
+
+/* For ioctl's we haven't written decoding stuff for yet */
+typedef struct {
+	union mali_ioctl_header header;
+} __ioctl_placeholder;
+
+#define MALI_IOCTL_TYPE_BASE  0x80
+#define MALI_IOCTL_TYPE_MAX   0x82
+#define MALI_IOCTL_TYPE_COUNT (MALI_IOCTL_TYPE_MAX - MALI_IOCTL_TYPE_BASE + 1)
+
+#define MALI_IOCTL_GET_VERSION             (_IOWR(0x80,  0, struct mali_ioctl_get_version))
+#define MALI_IOCTL_MEM_ALLOC               (_IOWR(0x82,  0, struct mali_ioctl_mem_alloc))
+#define MALI_IOCTL_MEM_IMPORT              (_IOWR(0x82,  1, struct mali_ioctl_mem_import))
+#define MALI_IOCTL_MEM_COMMIT              (_IOWR(0x82,  2, struct mali_ioctl_mem_commit))
+#define MALI_IOCTL_MEM_QUERY               (_IOWR(0x82,  3, struct mali_ioctl_mem_query))
+#define MALI_IOCTL_MEM_FREE                (_IOWR(0x82,  4, struct mali_ioctl_mem_free))
+#define MALI_IOCTL_MEM_FLAGS_CHANGE        (_IOWR(0x82,  5, struct mali_ioctl_mem_flags_change))
+#define MALI_IOCTL_MEM_ALIAS               (_IOWR(0x82,  6, struct mali_ioctl_mem_alias))
+#define MALI_IOCTL_SYNC                    (_IOWR(0x82,  8, struct mali_ioctl_sync))
+#define MALI_IOCTL_POST_TERM               (_IOWR(0x82,  9, __ioctl_placeholder))
+#define MALI_IOCTL_HWCNT_SETUP             (_IOWR(0x82, 10, __ioctl_placeholder))
+#define MALI_IOCTL_HWCNT_DUMP              (_IOWR(0x82, 11, __ioctl_placeholder))
+#define MALI_IOCTL_HWCNT_CLEAR             (_IOWR(0x82, 12, __ioctl_placeholder))
+#define MALI_IOCTL_GPU_PROPS_REG_DUMP      (_IOWR(0x82, 14, struct mali_ioctl_gpu_props_reg_dump))
+#define MALI_IOCTL_FIND_CPU_OFFSET         (_IOWR(0x82, 15, __ioctl_placeholder))
+#define MALI_IOCTL_GET_VERSION_NEW         (_IOWR(0x82, 16, struct mali_ioctl_get_version))
+#define MALI_IOCTL_SET_FLAGS               (_IOWR(0x82, 18, struct mali_ioctl_set_flags))
+#define MALI_IOCTL_SET_TEST_DATA           (_IOWR(0x82, 19, __ioctl_placeholder))
+#define MALI_IOCTL_INJECT_ERROR            (_IOWR(0x82, 20, __ioctl_placeholder))
+#define MALI_IOCTL_MODEL_CONTROL           (_IOWR(0x82, 21, __ioctl_placeholder))
+#define MALI_IOCTL_KEEP_GPU_POWERED        (_IOWR(0x82, 22, __ioctl_placeholder))
+#define MALI_IOCTL_FENCE_VALIDATE          (_IOWR(0x82, 23, __ioctl_placeholder))
+#define MALI_IOCTL_STREAM_CREATE           (_IOWR(0x82, 24, struct mali_ioctl_stream_create))
+#define MALI_IOCTL_GET_PROFILING_CONTROLS  (_IOWR(0x82, 25, __ioctl_placeholder))
+#define MALI_IOCTL_SET_PROFILING_CONTROLS  (_IOWR(0x82, 26, __ioctl_placeholder))
+#define MALI_IOCTL_DEBUGFS_MEM_PROFILE_ADD (_IOWR(0x82, 27, __ioctl_placeholder))
+#define MALI_IOCTL_JOB_SUBMIT              (_IOWR(0x82, 28, struct mali_ioctl_job_submit))
+#define MALI_IOCTL_DISJOINT_QUERY          (_IOWR(0x82, 29, __ioctl_placeholder))
+#define MALI_IOCTL_GET_CONTEXT_ID          (_IOWR(0x82, 31, struct mali_ioctl_get_context_id))
+#define MALI_IOCTL_TLSTREAM_ACQUIRE_V10_4  (_IOWR(0x82, 32, __ioctl_placeholder))
+#define MALI_IOCTL_TLSTREAM_TEST           (_IOWR(0x82, 33, __ioctl_placeholder))
+#define MALI_IOCTL_TLSTREAM_STATS          (_IOWR(0x82, 34, __ioctl_placeholder))
+#define MALI_IOCTL_TLSTREAM_FLUSH          (_IOWR(0x82, 35, __ioctl_placeholder))
+#define MALI_IOCTL_HWCNT_READER_SETUP      (_IOWR(0x82, 36, __ioctl_placeholder))
+#define MALI_IOCTL_SET_PRFCNT_VALUES       (_IOWR(0x82, 37, __ioctl_placeholder))
+#define MALI_IOCTL_SOFT_EVENT_UPDATE       (_IOWR(0x82, 38, __ioctl_placeholder))
+#define MALI_IOCTL_MEM_JIT_INIT            (_IOWR(0x82, 39, __ioctl_placeholder))
+#define MALI_IOCTL_TLSTREAM_ACQUIRE        (_IOWR(0x82, 40, __ioctl_placeholder))
+
+#endif /* __MALI_IOCTL_H__ */
diff --git a/src/gallium/drivers/panfrost/include/mali-job.h b/src/gallium/drivers/panfrost/include/mali-job.h
new file mode 100644
index 0000000..471612d
--- /dev/null
+++ b/src/gallium/drivers/panfrost/include/mali-job.h
@@ -0,0 +1,1159 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __MALI_JOB_H__
+#define __MALI_JOB_H__
+
+#include <mali-ioctl.h>
+
+#define T8XX
+
+#define MALI_SHORT_PTR_BITS (sizeof(uintptr_t)*8)
+
+#define MALI_FBD_HIERARCHY_WEIGHTS 8
+
+#define MALI_PAYLOAD_SIZE 256
+
+enum mali_job_type {
+	JOB_NOT_STARTED	= 0,
+	JOB_TYPE_NULL = 1,
+	JOB_TYPE_SET_VALUE = 2,
+	JOB_TYPE_CACHE_FLUSH = 3,
+	JOB_TYPE_COMPUTE = 4,
+	JOB_TYPE_VERTEX = 5,
+	JOB_TYPE_TILER = 7,
+	JOB_TYPE_FUSED = 8,
+	JOB_TYPE_FRAGMENT = 9,
+};
+
+enum mali_gl_mode {
+	MALI_GL_NONE           = 0x0,
+	MALI_GL_POINTS         = 0x1,
+	MALI_GL_LINES          = 0x2,
+	MALI_GL_LINE_STRIP     = 0x4,
+	MALI_GL_LINE_LOOP      = 0x6,
+	MALI_GL_TRIANGLES      = 0x8,
+	MALI_GL_TRIANGLE_STRIP = 0xA,
+	MALI_GL_TRIANGLE_FAN   = 0xC,
+};
+
+#define MALI_GL_CULL_FACE_BACK  0x80
+#define MALI_GL_CULL_FACE_FRONT 0x40
+
+#define MALI_GL_FRONT_FACE(v) (v << 5)
+#define MALI_GL_CCW (0)
+#define MALI_GL_CW  (1)
+
+/* TODO: Might this actually be a finer bitfield? */
+#define MALI_DEPTH_STENCIL_ENABLE 0x6400
+
+#define DS_ENABLE(field) \
+	(field == MALI_DEPTH_STENCIL_ENABLE) \
+	? "MALI_DEPTH_STENCIL_ENABLE" \
+	: (field == 0) ? "0" \
+	: "0 /* XXX: Unknown, check hexdump */"
+
+/* Used in stencil and depth tests */
+
+enum mali_func {
+	MALI_FUNC_NEVER    = 0,
+	MALI_FUNC_LESS     = 1,
+	MALI_FUNC_EQUAL    = 2,
+	MALI_FUNC_LEQUAL   = 3,
+	MALI_FUNC_GREATER  = 4,
+	MALI_FUNC_NOTEQUAL = 5,
+	MALI_FUNC_GEQUAL   = 6,
+	MALI_FUNC_ALWAYS   = 7
+};
+
+/* Same OpenGL, but mixed up. Why? Because forget me, that's why! */
+
+enum mali_alt_func {
+	MALI_ALT_FUNC_NEVER    = 0,
+	MALI_ALT_FUNC_GREATER  = 1,
+	MALI_ALT_FUNC_EQUAL    = 2,
+	MALI_ALT_FUNC_GEQUAL   = 3,
+	MALI_ALT_FUNC_LESS     = 4,
+	MALI_ALT_FUNC_NOTEQUAL = 5,
+	MALI_ALT_FUNC_LEQUAL   = 6,
+	MALI_ALT_FUNC_ALWAYS   = 7
+};
+
+/* Flags apply to unknown2_3? */
+
+#define MALI_HAS_MSAA		(1 << 0)
+#define MALI_CAN_DISCARD 	(1 << 5)
+
+/* Applies on T6XX, specifying that programmable blending is in use */
+#define MALI_HAS_BLEND_SHADER 	(1 << 6)
+
+/* func is mali_func */
+#define MALI_DEPTH_FUNC(func)	   (func << 8)
+#define MALI_GET_DEPTH_FUNC(flags) ((flags >> 8) & 0x7)
+#define MALI_DEPTH_FUNC_MASK	   MALI_DEPTH_FUNC(0x7)
+ 
+#define MALI_DEPTH_TEST		(1 << 11)
+
+/* Next flags to unknown2_4 */
+#define MALI_STENCIL_TEST      	(1 << 0)
+
+/* What?! */
+#define MALI_SAMPLE_ALPHA_TO_COVERAGE_NO_BLEND_SHADER (1 << 1)
+
+#define MALI_NO_DITHER		(1 << 9)
+#define MALI_DEPTH_RANGE_A	(1 << 12)
+#define MALI_DEPTH_RANGE_B	(1 << 13)
+#define MALI_NO_MSAA		(1 << 14)
+
+/* Stencil test state is all encoded in a single u32, just with a lot of
+ * enums... */
+
+enum mali_stencil_op {
+	MALI_STENCIL_KEEP 	= 0,
+	MALI_STENCIL_REPLACE 	= 1,
+	MALI_STENCIL_ZERO 	= 2,
+	MALI_STENCIL_INVERT 	= 3,
+	MALI_STENCIL_INCR_WRAP 	= 4,
+	MALI_STENCIL_DECR_WRAP 	= 5,
+	MALI_STENCIL_INCR 	= 6,
+	MALI_STENCIL_DECR 	= 7
+};
+
+struct mali_stencil_test {
+	unsigned ref  			: 8;
+	unsigned mask 			: 8;
+	enum mali_func func 		: 3;
+	enum mali_stencil_op sfail 	: 3;
+	enum mali_stencil_op dpfail 	: 3;
+	enum mali_stencil_op dppass 	: 3;
+	unsigned zero			: 4;
+} __attribute__((packed));
+
+/* Blending is a mess, since anything fancy triggers a blend shader, and
+ * -those- are not understood whatsover yet */
+
+#define MALI_MASK_R (1 << 0)
+#define MALI_MASK_G (1 << 1)
+#define MALI_MASK_B (1 << 2)
+#define MALI_MASK_A (1 << 3)
+
+enum mali_nondominant_mode {
+	MALI_BLEND_NON_MIRROR = 0,
+	MALI_BLEND_NON_ZERO = 1
+};
+
+enum mali_dominant_blend {
+	MALI_BLEND_DOM_SOURCE = 0,
+	MALI_BLEND_DOM_DESTINATION  = 1
+};
+
+enum mali_dominant_factor {
+	MALI_DOMINANT_UNK0 = 0,
+	MALI_DOMINANT_ZERO = 1,
+	MALI_DOMINANT_SRC_COLOR = 2,
+	MALI_DOMINANT_DST_COLOR = 3,
+	MALI_DOMINANT_UNK4 = 4,
+	MALI_DOMINANT_SRC_ALPHA = 5,
+	MALI_DOMINANT_DST_ALPHA = 6,
+	MALI_DOMINANT_CONSTANT = 7,
+};
+
+enum mali_blend_modifier {
+	MALI_BLEND_MOD_UNK0 = 0,
+	MALI_BLEND_MOD_NORMAL = 1,
+	MALI_BLEND_MOD_SOURCE_ONE = 2,
+	MALI_BLEND_MOD_DEST_ONE = 3,
+};
+
+struct mali_blend_mode {
+	enum mali_blend_modifier clip_modifier : 2;
+	unsigned unused_0 : 1;
+	unsigned negate_source : 1;
+
+	enum mali_dominant_blend dominant : 1;
+
+	enum mali_nondominant_mode nondominant_mode : 1;
+
+	unsigned unused_1 : 1;
+
+	unsigned negate_dest : 1;
+
+	enum mali_dominant_factor dominant_factor : 3;
+	unsigned complement_dominant : 1;
+} __attribute__((packed));
+
+struct mali_blend_equation {
+	/* Of type mali_blend_mode */
+	unsigned rgb_mode : 12;
+	unsigned alpha_mode : 12;
+
+	unsigned zero1 : 4;
+
+	/* Corresponds to MALI_MASK_* above and glColorMask arguments */
+
+	unsigned color_mask : 4;
+
+	/* Attached constant for CONSTANT_ALPHA, etc */
+
+#ifndef BIFROST
+	float constant;
+#endif
+} __attribute__((packed));
+
+/* Alpha coverage is encoded as 4-bits (from a clampf), with inversion
+ * literally performing a bitwise invert. This function produces slightly wrong
+ * results and I'm not sure why; some rounding issue I suppose... */
+
+#define MALI_ALPHA_COVERAGE(clampf) ((uint16_t) (int) (clampf * 15.0f))
+#define MALI_GET_ALPHA_COVERAGE(nibble) ((float) nibble / 15.0f)
+
+/* Applies to unknown1 */
+#define MALI_NO_ALPHA_TO_COVERAGE (1 << 10)
+
+struct mali_blend_meta {
+#ifdef T8XX
+	/* Base value of 0x200.
+	 * OR with 0x1 for blending (anything other than REPLACE).
+	 * OR with 0x2 for programmable blending
+	 */
+
+	u64 unk1;
+
+	/* For programmable blending, these turn into the blend_shader address */
+	struct mali_blend_equation blend_equation_1;
+
+	u64 zero2;
+	struct mali_blend_equation blend_equation_2;
+#else
+	u32 unk1; // = 0x200
+	struct mali_blend_equation blend_equation;
+	/*
+	 * - 0x19 normally
+	 * - 0x3 when this slot is unused (everything else is 0 except the index)
+	 * - 0x11 when this is the fourth slot (and it's used)
+	 */
+	u16 unk2;
+	/* increments from 0 to 3 */
+	u16 index;
+	u32 unk3; // = 0x10ed688
+#endif
+} __attribute__((packed));
+
+struct mali_shader_meta {
+	mali_ptr shader;
+	u16 texture_count;
+	u16 sampler_count;
+	u16 attribute_count;
+	u16 varying_count;
+
+	union {
+		struct {
+			u32 uniform_buffer_count : 4;
+			u32 unk1 : 28; // = 0x800000 for vertex, 0x958020 for tiler
+		} bifrost1;
+		struct {
+			/* 0x200 except MALI_NO_ALPHA_TO_COVERAGE. Mysterious 1
+			 * other times. Who knows really? */
+			u16 unknown1; 
+
+			 /* Whole number of uniform registers used, times two;
+			  * whole number of work registers used (no scale). 
+			  */
+			unsigned work_count : 5;
+			unsigned uniform_count : 5;
+			unsigned unknown2 : 6;
+		} midgard1;
+	};
+
+	/* On bifrost: Exactly the same as glPolygonOffset() for both.
+	 * On midgard: Depth factor is exactly as passed to glPolygonOffset.
+	 * Depth units is equal to the value passed to glDeptOhffset + 1.0f
+	 * (use MALI_NEGATIVE)
+	 */
+	float depth_units;
+	float depth_factor;
+
+	u32 unknown2_2;
+
+	u16 alpha_coverage;
+	u16 unknown2_3;
+
+	u8 stencil_mask_front;
+	u8 stencil_mask_back;
+	u16 unknown2_4;
+
+	struct mali_stencil_test stencil_front;
+	struct mali_stencil_test stencil_back;
+
+	union {
+		struct {
+			u32 unk3 : 7;
+			/* On Bifrost, some system values are preloaded in
+			 * registers R55-R62 by the thread dispatcher prior to
+			 * the start of shader execution. This is a bitfield
+			 * with one entry for each register saying which
+			 * registers need to be preloaded. Right now, the known
+			 * values are:
+			 *
+			 * Vertex/compute:
+			 * - R55 : gl_LocalInvocationID.xy
+			 * - R56 : gl_LocalInvocationID.z + unknown in high 16 bits
+			 * - R57 : gl_WorkGroupID.x
+			 * - R58 : gl_WorkGroupID.y
+			 * - R59 : gl_WorkGroupID.z
+			 * - R60 : gl_GlobalInvocationID.x
+			 * - R61 : gl_GlobalInvocationID.y/gl_VertexID (without base)
+			 * - R62 : gl_GlobalInvocationID.z/gl_InstanceID (without base)
+			 *
+			 * Fragment:
+			 * - R55 : unknown, never seen (but the bit for this is
+			 *   always set?)
+			 * - R56 : unknown (bit always unset)
+			 * - R57 : gl_PrimitiveID
+			 * - R58 : gl_FrontFacing in low bit, potentially other stuff
+			 * - R59 : u16 fragment coordinates (used to compute
+			 *   gl_FragCoord.xy, together with sample positions)
+			 * - R60 : gl_SampleMask (used in epilog, so pretty
+			 *   much always used, but the bit is always 0 -- is
+			 *   this just always pushed?)
+			 * - R61 : gl_SampleMaskIn and gl_SampleID, used by
+			 *   varying interpolation.
+			 * - R62 : unknown (bit always unset).
+			 */
+			u32 preload_regs : 8;
+			/* In units of 8 bytes or 64 bits, since the
+			 * uniform/const port loads 64 bits at a time.
+			 */
+			u32 uniform_count : 7;
+			u32 unk4 : 10; // = 2
+		} bifrost2;
+		struct {
+			u32 unknown2_7;
+		} midgard2;
+	};
+
+	/* zero on bifrost */
+	u32 unknown2_8;
+
+	/* Blending information for the older non-MRT Midgard HW. Check for
+	 * MALI_HAS_BLEND_SHADER to decide how to interpret.
+	 */
+
+	union {
+		mali_ptr blend_shader;
+		struct mali_blend_equation blend_equation;
+	};
+
+	/* There can be up to 4 blend_meta's. None of them are required for
+	 * vertex shaders or the non-MRT case for Midgard (so the blob doesn't
+	 * allocate any space).
+	 */
+	struct mali_blend_meta blend_meta[];
+
+} __attribute__((packed));
+
+/* This only concerns hardware jobs */
+
+/* Possible values for job_descriptor_size */
+
+#define MALI_JOB_32 0
+#define MALI_JOB_64 1
+
+struct mali_job_descriptor_header {
+	u32 exception_status;
+	u32 first_incomplete_task;
+	u64 fault_pointer;
+	u8 job_descriptor_size : 1;
+	enum mali_job_type job_type : 7;
+	u8 job_barrier : 1;
+	u8 unknown_flags : 7;
+	u16 job_index;
+	u16 job_dependency_index_1;
+	u16 job_dependency_index_2;
+	
+	union {
+		u64 next_job_64;
+		u32 next_job_32;
+	};
+} __attribute__((packed));
+
+struct mali_payload_set_value {
+	u64 out;
+	u64 unknown;
+} __attribute__((packed));
+
+/* Special attributes have a fixed index */
+#define MALI_SPECIAL_ATTRIBUTE_BASE 16
+#define MALI_VERTEX_ID   (MALI_SPECIAL_ATTRIBUTE_BASE + 0)
+#define MALI_INSTANCE_ID (MALI_SPECIAL_ATTRIBUTE_BASE + 1)
+
+struct mali_attr {
+	mali_ptr elements;
+	u32 stride;
+	u32 size;
+} __attribute__((packed));
+
+/* TODO: I'm pretty sure this isn't really right in the presence of more
+ * complicated metadata, like matrices or varyings */
+
+enum mali_attr_type {
+	MALI_ATYPE_PACKED = 1,
+	MALI_ATYPE_UNK1 = 1,
+	MALI_ATYPE_BYTE = 3,
+	MALI_ATYPE_SHORT = 4,
+	MALI_ATYPE_INT = 5,
+	MALI_ATYPE_GPVARYING = 6,
+	MALI_ATYPE_FLOAT = 7,
+};
+
+struct mali_attr_meta {
+	/* Vertex buffer index */
+	u8 index;
+
+	u64 unknown1 :14;
+
+	/* Part of the type specifier, anyway:
+	 * 1: packed (with other encoding weirdness)
+	 * 3: byte
+	 * 4: short
+	 * 5: int
+	 * 6: used for float gl_Position varying?
+	 * 7: half, float, packed
+	 */
+
+	unsigned type : 3;
+
+	/* After MALI_POSITIVE, 4 for vec4, 1 for scalar, etc */
+	unsigned nr_components : 2;
+
+	/* Somewhat correlated to the opposite of not_normalised, or the opposite of is_half_float? */
+	unsigned unknown2 : 1;
+
+	/* If the type is a signed integer, is_int_signed is set. If the type
+	 * is a half-float, it's also set. Otherwise, it is clear. */
+
+	unsigned is_int_signed : 1;
+
+	/* if `normalized` passed to VertexAttribPointer is clear */
+	unsigned not_normalised : 1;
+
+	/* Always observed to be zero at the moment */
+	unsigned unknown3 : 2;
+
+	/* When packing multiple attributes in a buffer, offset addresses by this value */
+	uint32_t src_offset;
+} __attribute__((packed));
+
+enum mali_fbd_type {
+	MALI_SFBD = 0,
+	MALI_MFBD = 1,
+};
+
+#define FBD_TYPE (1)
+#define FBD_MASK (~0x3f)
+
+struct mali_uniform_buffer_meta {
+	/* This is actually the size minus 1 (MALI_POSITIVE), in units of 16
+	 * bytes. This gives a maximum of 2^14 bytes, which just so happens to
+	 * be the GL minimum-maximum for GL_MAX_UNIFORM_BLOCK_SIZE.
+	 */
+	u64 size : 10;
+
+	/* This is missing the bottom 2 bits and top 8 bits. The top 8 bits
+	 * should be 0 for userspace pointers, according to
+	 * https://lwn.net/Articles/718895/. By reusing these bits, we can make
+	 * each entry in the table only 64 bits.
+	 */
+	mali_ptr ptr : 64 - 10;
+};
+
+/* On Bifrost, these fields are the same between the vertex and tiler payloads.
+ * They also seem to be the same between Bifrost and Midgard. They're shared in
+ * fused payloads.
+ */
+
+/* Applies to unknown_draw */
+#define MALI_DRAW_INDEXED_UINT8  (0x10)
+#define MALI_DRAW_INDEXED_UINT16 (0x20)
+#define MALI_DRAW_INDEXED_UINT32 (0x30)
+
+struct mali_vertex_tiler_prefix {
+	/* This is a dynamic bitfield containing the following things in this order:
+	 *
+	 * - gl_WorkGroupSize.x
+	 * - gl_WorkGroupSize.y
+	 * - gl_WorkGroupSize.z
+	 * - gl_NumWorkGroups.x
+	 * - gl_NumWorkGroups.y
+	 * - gl_NumWorkGroups.z
+	 *
+	 * The number of bits allocated for each number is based on the *_shift
+	 * fields below. For example, workgroups_y_shift gives the bit that
+	 * gl_NumWorkGroups.y starts at, and workgroups_z_shift gives the bit
+	 * that gl_NumWorkGroups.z starts at (and therefore one after the bit
+	 * that gl_NumWorkGroups.y ends at). The actual value for each gl_*
+	 * value is one more than the stored value, since if any of the values
+	 * are zero, then there would be no invocations (and hence no job). If
+	 * there were 0 bits allocated to a given field, then it must be zero,
+	 * and hence the real value is one.
+	 *
+	 * Vertex jobs reuse the same job dispatch mechanism as compute jobs,
+	 * effectively doing glDispatchCompute(1, vertex_count, instance_count)
+	 * where vertex count is the number of vertices.
+	 */
+	u32 invocation_count; 
+
+	u32 size_y_shift : 5;
+	u32 size_z_shift : 5;
+	u32 workgroups_x_shift : 6;
+	u32 workgroups_y_shift : 6;
+	u32 workgroups_z_shift : 6;
+	/* This is max(workgroups_x_shift, 2) in all the cases I've seen. */
+	u32 workgroups_x_shift_2 : 4;
+
+	u32 draw_mode : 4;
+	u32 unknown_draw : 22; 
+
+	/* This is the the same as workgroups_x_shift_2 in compute shaders, but
+	 * always 5 for vertex jobs and 6 for tiler jobs. I suspect this has
+	 * something to do with how many quads get put in the same execution
+	 * engine, which is a balance (you don't want to starve the engine, but
+	 * you also want to distribute work evenly).
+	 */
+	u32 workgroups_x_shift_3 : 6;
+
+
+	/* Negative of draw_start for TILER jobs from what I've seen */
+	int32_t negative_start;
+	u32 zero1;
+
+	/* Like many other strictly nonzero quantities, index_count is
+	 * subtracted by one. For an indexed cube, this is equal to 35 = 6
+	 * faces * 2 triangles/per face * 3 vertices/per triangle - 1. For
+	 * non-indexed draws, equal to vertex_count. */
+
+	u32 index_count;
+
+	/* No hidden structure; literally just a pointer to an array of
+	 * uint32_t indices. Thanks, guys, for not making my life insane for
+	 * once! NULL for non-indexed draws. */
+
+	uintptr_t indices;
+} __attribute__((packed));
+
+struct bifrost_vertex_only {
+	u32 unk2; /* =0x2 */
+
+	u32 zero0;
+
+	u64 zero1;
+} __attribute__((packed));
+
+struct bifrost_tiler_heap_meta {
+	u32 zero;
+	u32 heap_size;
+	/* note: these are just guesses! */
+	mali_ptr tiler_heap_start;
+	mali_ptr tiler_heap_free;
+	mali_ptr tiler_heap_end;
+
+	/* hierarchy weights? but they're still 0 after the job has run... */
+	u32 zeros[12];
+} __attribute__((packed));
+
+struct bifrost_tiler_meta {
+	u64 zero0;
+	u32 unk; // = 0xf0
+	u16 width;
+	u16 height;
+	u64 zero1;
+	mali_ptr tiler_heap_meta;
+	/* TODO what is this used for? */
+	u64 zeros[20];
+} __attribute__((packed));
+
+struct bifrost_tiler_only {
+	/* 0x20 */
+	float line_width;
+	u32 zero0;
+
+	mali_ptr tiler_meta;
+
+	u64 zero1, zero2, zero3, zero4, zero5, zero6;
+
+	u32 gl_enables;
+	u32 zero7;
+	u64 zero8;
+} __attribute__((packed));
+
+struct bifrost_scratchpad {
+	u32 zero;
+	u32 flags; // = 0x1f
+	/* This is a pointer to a CPU-inaccessible buffer, 16 pages, allocated
+	 * during startup. It seems to serve the same purpose as the
+	 * gpu_scratchpad in the SFBD for Midgard, although it's slightly
+	 * larger.
+	 */
+	mali_ptr gpu_scratchpad;
+} __attribute__((packed));
+
+struct mali_vertex_tiler_postfix {
+	/* Zero for vertex jobs. Pointer to the position (gl_Position) varying
+	 * output from the vertex shader for tiler jobs.
+	 */
+
+	uintptr_t position_varying;
+
+	/* An array of mali_uniform_buffer_meta's. The size is given by the
+	 * shader_meta.
+	 */
+	uintptr_t uniform_buffers;
+
+	/* For reasons I don't quite understand this is a pointer to a pointer.
+	 * That second pointer points to the actual texture descriptor. */
+	uintptr_t texture_trampoline;
+
+	/* For OpenGL, from what I've seen, this is intimately connected to
+	 * texture_meta. cwabbott says this is not the case under Vulkan, hence
+	 * why this field is seperate (Midgard is Vulkan capable) */
+	uintptr_t sampler_descriptor;
+
+	uintptr_t uniforms;
+	u8 flags : 4;
+	uintptr_t _shader_upper : MALI_SHORT_PTR_BITS - 4; /* struct shader_meta */
+	uintptr_t attributes; /* struct attribute_buffer[] */
+	uintptr_t attribute_meta; /* attribute_meta[] */
+	uintptr_t varyings; /* struct attr */
+	uintptr_t varying_meta; /* pointer */
+	uintptr_t viewport;
+	uintptr_t zero6;
+
+	/* Note: on Bifrost, this isn't actually the FBD. It points to
+	 * bifrost_scratchpad instead. However, it does point to the same thing
+	 * in vertex and tiler jobs.
+	 */
+	mali_ptr framebuffer;
+
+#if UINTPTR_MAX == 0xffffffffffffffff /* 64-bit */
+#ifndef T8XX
+	/* most likely padding to make this a multiple of 64 bytes */
+	u64 zero7;
+#endif
+#endif
+} __attribute__((packed));
+
+struct midgard_payload_vertex_tiler {
+#ifdef T6XX
+	float line_width;
+#endif
+
+	struct mali_vertex_tiler_prefix prefix;
+
+#ifdef T6XX
+	u32 zero3;
+#endif
+	u32 gl_enables; // 0x5
+
+	/* Offset for first vertex in buffer */
+	u32 draw_start;
+
+#ifdef T6XX
+	u32 zero5;
+#else
+	u64 zero5;
+#endif
+
+	struct mali_vertex_tiler_postfix postfix;
+
+#ifdef T8XX
+	float line_width;
+#endif
+} __attribute__((packed));
+
+struct bifrost_payload_vertex {
+	struct mali_vertex_tiler_prefix prefix;
+	struct bifrost_vertex_only vertex;
+	struct mali_vertex_tiler_postfix postfix;
+} __attribute__((packed));
+
+struct bifrost_payload_tiler {
+	struct mali_vertex_tiler_prefix prefix;
+	struct bifrost_tiler_only tiler;
+	struct mali_vertex_tiler_postfix postfix;
+} __attribute__((packed));
+
+struct bifrost_payload_fused {
+	struct mali_vertex_tiler_prefix prefix;
+	struct bifrost_tiler_only tiler;
+	struct mali_vertex_tiler_postfix tiler_postfix;
+	struct bifrost_vertex_only vertex;
+	struct mali_vertex_tiler_postfix vertex_postfix;
+} __attribute__((packed));
+
+/* Pointed to from texture_trampoline, mostly unknown still, haven't
+ * managed to replay successfully */
+
+/* Purposeful off-by-one in width, height fields. For example, a (64, 64)
+ * texture is stored as (63, 63) in these fields. This adjusts for that.
+ * There's an identical pattern in the framebuffer descriptor. Even vertex
+ * count fields work this way, hence the generic name -- integral fields that
+ * are strictly positive generally need this adjustment. */
+
+#define MALI_POSITIVE(dim) (dim - 1)
+
+/* Opposite of MALI_POSITIVE, found in the depth_units field */
+
+#define MALI_NEGATIVE(dim) (dim + 1)
+
+/* Used with channel swizzling */
+enum mali_channel {
+	MALI_CHANNEL_RED = 0,
+	MALI_CHANNEL_GREEN = 1,
+	MALI_CHANNEL_BLUE = 2,
+	MALI_CHANNEL_ALPHA = 3,
+	MALI_CHANNEL_ZERO = 4,
+	MALI_CHANNEL_ONE = 5,
+	MALI_CHANNEL_RESERVED_0 = 6,
+	MALI_CHANNEL_RESERVED_1 = 7,
+};
+
+/* Used with wrapping. Incomplete (this is a 4-bit field...) */
+
+enum mali_wrap_mode {
+	MALI_WRAP_REPEAT = 0x8,
+	MALI_WRAP_CLAMP_TO_EDGE = 0x9,
+	MALI_WRAP_CLAMP_TO_BORDER = 0xB,
+	MALI_WRAP_MIRRORED_REPEAT = 0xC
+};
+
+/* 8192x8192 */
+#define MAX_MIP_LEVELS (13)
+
+/* Cubemap bloats everything up */
+#define MAX_FACES (6)
+
+/* Corresponds to the type passed to glTexImage2D and so forth */
+
+struct mali_texture_format {
+	unsigned bottom : 8;
+	unsigned unk1 : 4;
+
+	/*
+	 * 0: ushort_5_6_5
+	 * 2: ushort_4_4_4_4
+	 * 3: u8
+	 * 4: u16
+	 * 5: u32
+	 * 7: float
+	 */
+
+	unsigned component_size : 3;
+
+	unsigned nr_channels : 2;
+
+	/*
+	 * 2: ushort_5_5_5_1, ushort_5_6_5
+	 * 3: snorm
+	 * 4: unsigned int
+	 * 5: (unsigned) int / full-float
+	 * 6: signed int / half-float
+	 * 7: maybe also snorm related
+	 */
+
+	unsigned typeA : 3;
+
+	unsigned usage1 : 3;
+	unsigned is_not_cubemap : 1;
+	unsigned usage2 : 8;
+} __attribute__((packed));
+
+struct mali_texture_descriptor {
+	uint16_t width;
+	uint16_t height;
+	uint16_t depth;
+
+	uint16_t unknown1;
+
+	struct mali_texture_format format; 
+
+	uint16_t unknown3;
+
+	/* One for non-mipmapped, zero for mipmapped */
+	uint8_t unknown3A;
+
+	/* Zero for non-mipmapped, (number of levels - 1) for mipmapped */
+	uint8_t nr_mipmap_levels;
+
+	/* Swizzling is a single 32-bit word, broken up here for convenience.
+	 * Here, swizzling refers to the ES 3.0 texture parameters for channel
+	 * level swizzling, not the internal pixel-level swizzling which is
+	 * below OpenGL's reach */
+
+	enum mali_channel swizzle_r : 3;
+	enum mali_channel swizzle_g : 3;
+	enum mali_channel swizzle_b : 3;
+	enum mali_channel swizzle_a : 3;
+	unsigned swizzle_zero       : 20;
+
+	uint32_t unknown5;
+	uint32_t unknown6;
+	uint32_t unknown7;
+
+	mali_ptr swizzled_bitmaps[MAX_MIP_LEVELS * MAX_FACES];
+} __attribute__((packed));
+
+/* Used as part of filter_mode */
+
+#define MALI_GL_LINEAR 0
+#define MALI_GL_NEAREST 1
+#define MALI_GL_MIP_LINEAR (0x18)
+
+/* Used to construct low bits of filter_mode */
+
+#define MALI_GL_TEX_MAG(mode) (((mode) & 1) << 0)
+#define MALI_GL_TEX_MIN(mode) (((mode) & 1) << 1)
+
+#define MALI_GL_TEX_MAG_MASK (1)
+#define MALI_GL_TEX_MIN_MASK (2)
+
+#define MALI_FILTER_NAME(filter) (filter ? "MALI_GL_NEAREST" : "MALI_GL_LINEAR")
+
+/* Used for lod encoding. Thanks @urjaman for pointing out these routines can
+ * be cleaned up a lot. */
+
+#define DECODE_FIXED_16(x) ((float) (x / 256.0))
+
+static inline uint16_t
+FIXED_16(float x) {
+	/* Clamp inputs, accounting for float error */
+	float max_lod = (32.0 - (1.0/512.0));
+
+	x = ((x > max_lod) ? max_lod : ((x < 0.0) ? 0.0 : x));
+
+	return (int) (x * 256.0);
+}
+
+struct mali_sampler_descriptor {
+	uint32_t filter_mode;
+	
+	/* Fixed point. Upper 8-bits is before the decimal point, although it
+	 * caps [0-31]. Lower 8-bits is after the decimal point: int(round(x *
+	 * 256)) */
+
+	uint16_t min_lod;
+	uint16_t max_lod;
+
+	/* All one word in reality, but packed a bit */
+
+	enum mali_wrap_mode wrap_s : 4;
+	enum mali_wrap_mode wrap_t : 4;
+	enum mali_wrap_mode wrap_r : 4;
+	enum mali_alt_func compare_func : 3;
+
+	/* A single set bit of unknown, ha! */
+	unsigned unknown2 : 1;
+
+	unsigned zero : 16;
+
+	uint32_t zero2;
+	float border_color[4];
+} __attribute__((packed));
+
+/* TODO: What are the floats? Apparently always { -inf, -inf, inf, inf },
+ * unless the scissor test is enabled.
+ *
+ * viewport0/viewport1 form the arguments to glViewport. viewport1 is modified
+ * by MALI_POSITIVE; viewport0 is as-is.
+ */
+
+struct mali_viewport {
+	float floats[4];
+
+	float depth_range_n;
+	float depth_range_f;
+
+	u16 viewport0[2];
+	u16 viewport1[2];
+} __attribute__((packed));
+
+/* TODO: Varying meta is symmetrical with attr_meta, but there is some
+ * weirdness associated. Figure it out. */
+
+struct mali_unknown6 {
+	u64 unknown0;
+	u64 unknown1;
+};
+
+/* From presentations, 16x16 tiles externally. Use shift for fast computation
+ * of tile numbers. */
+
+#define MALI_TILE_SHIFT 4
+#define MALI_TILE_LENGTH (1 << MALI_TILE_SHIFT)
+
+/* Tile coordinates are stored as a compact u32, as only 12 bits are needed to
+ * each component. Notice that this provides a theoretical upper bound of (1 <<
+ * 12) = 4096 tiles in each direction, addressing a maximum framebuffer of size
+ * 65536x65536. Multiplying that together, times another four given that Mali
+ * framebuffers are 32-bit ARGB8888, means that this upper bound would take 16
+ * gigabytes of RAM just to store the uncompressed framebuffer itself, let
+ * alone rendering in real-time to such a buffer.
+ *
+ * Nice job, guys.*/
+
+/* From mali_kbase_10969_workaround.c */
+#define MALI_X_COORD_MASK 0x00000FFF
+#define MALI_Y_COORD_MASK 0x0FFF0000
+
+/* Extract parts of a tile coordinate */
+
+#define MALI_TILE_COORD_X(coord) ((coord) & MALI_X_COORD_MASK)
+#define MALI_TILE_COORD_Y(coord) (((coord) & MALI_Y_COORD_MASK) >> 16)
+#define MALI_TILE_COORD_FLAGS(coord) ((coord) & ~(MALI_X_COORD_MASK | MALI_Y_COORD_MASK))
+
+/* No known flags yet, but just in case...? */
+
+#define MALI_TILE_NO_FLAG (0)
+
+/* Helpers to generate tile coordinates based on the boundary coordinates in
+ * screen space. So, with the bounds (0, 0) to (128, 128) for the screen, these
+ * functions would convert it to the bounding tiles (0, 0) to (7, 7).
+ * Intentional "off-by-one"; finding the tile number is a form of fencepost
+ * problem. */
+
+#define MALI_MAKE_TILE_COORDS(X, Y) ((X) | ((Y) << 16))
+#define MALI_BOUND_TO_TILE(B, bias) ((B - bias) >> MALI_TILE_SHIFT)
+#define MALI_COORDINATE_TO_TILE(W, H, bias) MALI_MAKE_TILE_COORDS(MALI_BOUND_TO_TILE(W, bias), MALI_BOUND_TO_TILE(H, bias))
+#define MALI_COORDINATE_TO_TILE_MIN(W, H) MALI_COORDINATE_TO_TILE(W, H, 0) 
+#define MALI_COORDINATE_TO_TILE_MAX(W, H) MALI_COORDINATE_TO_TILE(W, H, 1)
+
+struct mali_payload_fragment {
+	u32 min_tile_coord;
+	u32 max_tile_coord;
+	mali_ptr framebuffer;
+} __attribute__((packed));
+
+/* (Single?) Framebuffer Descriptor */
+
+/* Flags apply to format. With just MSAA_A and MSAA_B, the framebuffer is
+ * configured for 4x. With MSAA_8, it is configured for 8x. */
+
+#define MALI_FRAMEBUFFER_MSAA_8 (1 << 3)
+#define MALI_FRAMEBUFFER_MSAA_A (1 << 4)
+#define MALI_FRAMEBUFFER_MSAA_B (1 << 23)
+
+/* Fast/slow based on whether all three buffers are cleared at once */
+
+#define MALI_CLEAR_FAST         (1 << 18)
+#define MALI_CLEAR_SLOW         (1 << 28)
+#define MALI_CLEAR_SLOW_STENCIL (1 << 31)
+
+struct mali_single_framebuffer {
+	u32 unknown1;
+	u32 unknown2;
+	u64 unknown_address_0;
+	u64 zero1;
+	u64 zero0;
+
+	/* Exact format is ironically not known, since EGL is finnicky with the
+	 * blob. MSAA, colourspace, etc are configured here. */
+
+	u32 format; 
+
+	u32 clear_flags;
+	u32 zero2;
+
+	/* Purposeful off-by-one in these fields should be accounted for by the
+	 * MALI_DIMENSION macro */
+
+	u16 width;
+	u16 height;
+
+	u32 zero3[8];
+
+	/* By default, the framebuffer is upside down from OpenGL's
+	 * perspective. Set framebuffer to the end and negate the stride to
+	 * flip in the Y direction */
+
+	mali_ptr framebuffer;
+	int32_t stride;
+
+	u32 zero4;
+
+	/* Depth and stencil buffers are interleaved, it appears, as they are
+	 * set to the same address in captures. Both fields set to zero if the
+	 * buffer is not being cleared. Depending on GL_ENABLE magic, you might
+	 * get a zero enable despite the buffer being present; that still is
+	 * disabled. */
+
+	mali_ptr depth_buffer; // not SAME_VA
+	u64 depth_buffer_enable; 
+
+	mali_ptr stencil_buffer; // not SAME_VA
+	u64 stencil_buffer_enable; 
+
+	u32 clear_color_1; // RGBA8888 from glClear, actually used by hardware
+	u32 clear_color_2; // always equal, but unclear function?
+	u32 clear_color_3; // always equal, but unclear function?
+	u32 clear_color_4; // always equal, but unclear function?
+
+	/* Set to zero if not cleared */
+
+	float clear_depth_1; // float32, ditto
+	float clear_depth_2; // float32, ditto
+	float clear_depth_3; // float32, ditto
+	float clear_depth_4; // float32, ditto
+
+	u32 clear_stencil; // Exactly as it appears in OpenGL
+
+	u32 zero6[7];
+
+	/* Very weird format, see generation code in trans_builder.c */
+	u32 resolution_check;
+
+	u32 tiler_flags;
+
+	u64 unknown_address_1; /* Pointing towards... a zero buffer? */
+	u64 unknown_address_2;
+
+	/* See mali_kbase_replay.c */
+	u64 tiler_heap_free;
+	u64 tiler_heap_end;
+
+	/* More below this, maybe */
+} __attribute__((packed));
+
+/* Format bits for the render target */
+
+#define MALI_MFBD_FORMAT_AFBC 	  (1 << 10)
+#define MALI_MFBD_FORMAT_MSAA 	  (1 << 12)
+#define MALI_MFBD_FORMAT_NO_ALPHA (1 << 25)
+
+struct bifrost_render_target {
+	u32 unk1; // = 0x4000000
+	u32 format;
+
+	u64 zero1;
+
+	union {
+		struct {
+			/* Stuff related to ARM Framebuffer Compression. When AFBC is enabled,
+			 * there is an extra metadata buffer that contains 16 bytes per tile.
+			 * The framebuffer needs to be the same size as before, since we don't
+			 * know ahead of time how much space it will take up. The
+			 * framebuffer_stride is set to 0, since the data isn't stored linearly
+			 * anymore.
+			 */
+
+			mali_ptr metadata;
+			u32 stride; // stride in units of tiles
+			u32 unk; // = 0x20000
+		} afbc;
+
+		struct {
+			/* Heck if I know */
+			u64 unk;
+			mali_ptr pointer;
+		} chunknown;
+	};
+
+	mali_ptr framebuffer;
+
+	u32 zero2 : 4;
+	u32 framebuffer_stride : 28; // in units of bytes
+	u32 zero3;
+
+	u32 clear_color_1; // RGBA8888 from glClear, actually used by hardware
+	u32 clear_color_2; // always equal, but unclear function?
+	u32 clear_color_3; // always equal, but unclear function?
+	u32 clear_color_4; // always equal, but unclear function?
+} __attribute__((packed));
+
+/* An optional part of bifrost_framebuffer. It comes between the main structure
+ * and the array of render targets. It must be included if any of these are
+ * enabled:
+ *
+ * - Transaction Elimination
+ * - Depth/stencil
+ * - TODO: Anything else?
+ */
+
+struct bifrost_fb_extra {
+	mali_ptr checksum;
+	/* Each tile has an 8 byte checksum, so the stride is "width in tiles * 8" */
+	u32 checksum_stride;
+
+	u32 unk;
+
+	union {
+		/* Note: AFBC is only allowed for 24/8 combined depth/stencil. */
+		struct {
+			mali_ptr depth_stencil_afbc_metadata;
+			u32 depth_stencil_afbc_stride; // in units of tiles
+			u32 zero1;
+
+			mali_ptr depth_stencil;
+
+			u64 padding;
+		} ds_afbc;
+
+		struct {
+			/* Depth becomes depth/stencil in case of combined D/S */
+			mali_ptr depth;
+			u32 depth_stride_zero : 4;
+			u32 depth_stride : 28;
+			u32 zero1;
+
+			mali_ptr stencil;
+			u32 stencil_stride_zero : 4;
+			u32 stencil_stride : 28;
+			u32 zero2;
+		} ds_linear;
+	};
+
+
+	u64 zero3, zero4;
+} __attribute__((packed));
+
+/* flags for unk3 */
+#define MALI_MFBD_EXTRA (1 << 13)
+
+struct bifrost_framebuffer {
+	u32 unk0; // = 0x10
+	u32 zero1;
+	u64 zero2;
+	/* 0x10 */
+	mali_ptr sample_locations;
+	mali_ptr unknown1;
+	/* 0x20 */
+	u16 width1, height1;
+	u32 zero3;
+	u16 width2, height2;
+	u32 unk1 : 19; // = 0x01000
+	u32 rt_count_1 : 2; // off-by-one (use MALI_POSITIVE)
+	u32 unk2 : 3; // = 0
+	u32 rt_count_2 : 3; // no off-by-one
+	u32 zero4 : 5;
+	/* 0x30 */
+	u32 clear_stencil : 8;
+	u32 unk3 : 24; // = 0x100
+	float clear_depth;
+	mali_ptr tiler_meta;
+	/* 0x40 */
+	u64 zero5, zero6, zero7, zero8, zero9, zero10, zero11, zero12;
+
+	/* optional: struct bifrost_fb_extra extra */
+	/* struct bifrost_render_target rts[] */
+} __attribute__((packed));
+
+#endif /* __MALI_JOB_H__ */
diff --git a/src/gallium/drivers/panfrost/meson.build b/src/gallium/drivers/panfrost/meson.build
new file mode 100644
index 0000000..fb32ef8
--- /dev/null
+++ b/src/gallium/drivers/panfrost/meson.build
@@ -0,0 +1,92 @@
+# Copyright © 2017 Intel Corporation
+# Copyright © 2018 Alyssa Rosenzweig
+
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+files_panfrost = files(
+  'pan_public.h',
+  'pan_screen.c',
+  'pan_screen.h',
+  'pan_texture.c',
+  'pan_texture.h',
+
+  'midgard/midgard_compile.c',
+  'midgard/cppwrap.cpp',
+  'midgard/disassemble.c',
+
+  'pan_context.c',
+
+  'pan_nondrm.c',
+  'pan_allocate.c',
+  'pan_assemble.c',
+  'pan_slowfb.c',
+  'pan_swizzle.c',
+  'pan_blending.c',
+  'pan_blend_shaders.c',
+)
+
+midgard_nir_algebraic_c = custom_target(
+  'midgard_nir_algebraic.c',
+  input : 'midgard/midgard_nir_algebraic.py',
+  output : 'midgard_nir_algebraic.c',
+  command : [
+    prog_python, '@INPUT@',
+    '-p', join_paths(meson.source_root(), 'src/compiler/nir/'),
+  ],
+  capture : true,
+  depend_files : nir_algebraic_py,
+)
+
+libpanfrost = static_library(
+  'panfrost',
+  [files_panfrost, midgard_nir_algebraic_c],
+  dependencies: [cc.find_library('X11', required: true), cc.find_library('Xext', required: true), dep_thread, idep_nir],
+  include_directories : [inc_common, inc_gallium_aux, inc_gallium, inc_include, inc_src, include_directories('midgard'), include_directories('include')],
+  c_args : [c_vis_args, c_msvc_compat_args],
+)
+
+driver_panfrost = declare_dependency(
+  compile_args : ['-DGALLIUM_PANFROST', '-Wno-pointer-arith'],
+  link_with : libpanfrost
+)
+
+files_midgard = files(
+  'midgard/midgard_compile.c',
+  'midgard/cppwrap.cpp',
+  'midgard/disassemble.c',
+  'midgard/cmdline.c',
+)
+
+midgard_compiler = executable(
+  'midgard_compiler',
+  [files_midgard, midgard_nir_algebraic_c],
+  include_directories : [inc_common, inc_src, inc_include, inc_gallium, inc_gallium_aux, include_directories('midgard')],
+  dependencies : [
+    dep_thread,
+    idep_nir
+  ],
+  link_with : [
+    libgallium,
+    libglsl_standalone,
+    libmesa_util
+  ],
+  build_by_default : true
+)
+
+subdir('panwrap')
diff --git a/src/gallium/drivers/panfrost/midgard/assemble.py b/src/gallium/drivers/panfrost/midgard/assemble.py
new file mode 100644
index 0000000..8088934
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/assemble.py
@@ -0,0 +1,643 @@
+"""
+Copyright (C) 2018 Alyssa Rosenzweig
+Copyright (c) 2013 Connor Abbott (connor@abbott.cx)
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
+"""
+
+import sys
+import pprint
+import struct
+
+program = []
+
+# Definitions from cwabbott's tools
+
+t6xx_alu_ops = {
+    "fadd":  0x10,
+    "fmul":  0x14,
+    "fmin":  0x28,
+    "fmax":  0x2C,
+    "fmov":  0x30,
+    "ffloor":  0x36,
+    "fceil":  0x37,
+    "fdot3":  0x3C,
+    "fdot3r":  0x3D,
+    "fdot4":  0x3E,
+    "freduce":  0x3F,
+    "iadd":  0x40,
+    "isub":  0x46,
+    "imul":  0x58,
+    "imov":  0x7B,
+    "feq":  0x80,
+    "fne":  0x81,
+    "flt":  0x82,
+    "fle":  0x83,
+    "f2i":  0x99,
+    "f2u8":  0x9C,
+    "u2f": 0xBC,
+    "ieq":  0xA0,
+    "ine":  0xA1,
+    "ilt":  0xA4,
+    "ile":  0xA5,
+    "iand": 0x70,
+    "ior": 0x71,
+    "inot": 0x72,
+    "iandnot": 0x74,
+    "ixor": 0x76,
+    "ball":  0xA9,
+    "bany":  0xB1,
+    "i2f":  0xB8,
+    "csel":  0xC5,
+    "fatan_pt2":  0xE8,
+    "frcp":  0xF0,
+    "frsqrt":  0xF2,
+    "fsqrt":  0xF3,
+    "fexp2":  0xF4,
+    "flog2":  0xF5,
+    "fsin":  0xF6,
+    "fcos":  0xF7,
+    "fatan2_pt1":  0xF9,
+}
+
+t6xx_alu_bits = {
+        "vmul": 17,
+        "sadd": 19,
+        "vadd": 21,
+        "smul": 23,
+        "lut": 25,
+        "br": 26,
+        "branch": 27,
+        "constants": 32
+}
+
+t6xx_alu_size_bits = {
+        "vmul": 48,
+        "sadd": 32,
+        "vadd": 48,
+        "smul": 32,
+        "lut": 48,
+        "br": 16,
+        "branch": 48
+}
+
+t6xx_outmod = {
+        "none": 0,
+        "pos": 1,
+        "int": 2,
+        "sat": 3
+}
+
+t6xx_reg_mode = {
+    "quarter": 0,
+    "half": 1,
+    "full": 2,
+    "double": 3
+}
+
+t6xx_dest_override = {
+    "lower": 0,
+    "upper": 1,
+    "none": 2
+}
+
+t6xx_load_store_ops = {
+    "ld_st_noop":  0x03,
+    "ld_attr_16":  0x95,
+    "ld_attr_32":  0x94,
+    "ld_vary_16":  0x99,
+    "ld_vary_32":  0x98,
+    "ld_uniform_16":  0xAC,
+    "ld_uniform_32":  0xB0,
+    "st_vary_16":  0xD5,
+    "st_vary_32":  0xD4,
+    "ld_color_buffer_8": 0xBA
+}
+
+t6xx_tag = {
+        "texture": 0x3,
+        "load_store": 0x5,
+        "alu4": 0x8,
+        "alu8": 0x9,
+        "alu12": 0xA,
+        "alu16": 0xB,
+}
+
+def is_tag_alu(tag):
+    return (tag >= t6xx_tag["alu4"]) and (tag <= t6xx_tag["alu16"])
+
+# Just an enum
+
+ALU = 0
+LDST = 1
+TEXTURE = 2
+
+# Constant types supported, mapping the constant prefix to the Python format
+# string and the coercion function
+
+constant_types = {
+        "f": ("f", float),
+        "h": ("e", float),
+        "i": ("i", int),
+        "s": ("h", int)
+}
+
+compact_branch_op = {
+        "jump": 1,
+        "branch": 2,
+        "discard": 4,
+        "write": 7
+}
+
+branch_condition = {
+        "false": 1,
+        "true": 2,
+        "always": 3,
+}
+
+# TODO: What else?
+
+texture_op = {
+        "normal": 0x11,
+        "texelfetch": 0x14
+}
+
+texture_fmt = {
+        "2d": 0x02,
+        "3d": 0x03
+}
+	
+with open(sys.argv[1], "r") as f:
+    for ln in f:
+        space = ln.strip().split(" ")
+
+        instruction = space[0]
+        rest = " ".join(space[1:])
+
+        arguments = [s.strip() for s in rest.split(",")]
+        program += [(instruction, arguments)]
+
+swizzle_component = {
+        "x": 0,
+        "y": 1,
+        "z": 2,
+        "w": 3
+}
+
+def decode_reg_name(reg_name):
+    ireg = 0
+    upper = False
+    half = False
+
+    if reg_name[0] == 'r':
+        ireg = int(reg_name[1:])
+    elif reg_name[0] == 'h':
+        rreg = int(reg_name[2:])
+
+        # Decode half-register into its full register's half
+        ireg = rreg >> 1
+        upper = rreg & 1
+        half = True
+    else:
+        # Special case for load/store addresses
+        ireg = int(reg_name)
+
+    return (ireg, half, upper)
+
+def standard_swizzle_from_parts(swizzle_parts):
+    swizzle_s = swizzle_parts[1] if len(swizzle_parts) > 1 else "xyzw"
+
+    swizzle = 0
+    for (i, c) in enumerate(swizzle_s):
+        swizzle |= swizzle_component[c] << (2 * i)
+
+    return swizzle
+
+def mask_from_parts(mask_parts, large_mask):
+    mask_s = mask_parts[1] if len(mask_parts) > 1 else "xyzw"
+
+    if large_mask:
+        mask = sum([(3 << (2*swizzle_component[c]) if c in mask_s else 0) for c in "xyzw"])
+    else:
+        mask = sum([(1 << swizzle_component[c] if c in mask_s else 0) for c in "xyzw"])
+
+    return (mask, mask_s)
+
+def decode_reg(reg):
+    if reg[0] == "#":
+        # Not actually a register, instead an immediate float
+        return (True, struct.unpack("H", struct.pack("e", float(reg[1:])))[0], 0, 0, 0, 0)
+
+    # Function call syntax used in abs() modifier
+    if reg[-1] == ')':
+        reg = reg[:-1]
+
+    swizzle_parts = reg.split(".")
+
+    reg_name = swizzle_parts[0]
+
+    modifiers = 0
+
+    if reg_name[0] == '-':
+        modifiers |= 2
+        reg_name = reg_name[1:]
+
+    if reg_name[0] == 'a':
+        modifiers |= 1
+        reg_name = reg_name[len("abs("):]
+    
+    (ireg, half, upper) = decode_reg_name(reg_name)
+
+    return (False, ireg, standard_swizzle_from_parts(swizzle_parts), half, upper, modifiers)
+
+def decode_masked_reg(reg, large_mask):
+    mask_parts = reg.split(".")
+
+    reg_name = mask_parts[0]
+    (ireg, half, upper) = decode_reg_name(reg_name)
+    (mask, mask_s) = mask_from_parts(mask_parts, large_mask)
+
+    component = max([0] + [swizzle_component[c] for c in "xyzw" if c in mask_s])
+
+    return (ireg, mask, component, half, upper)
+
+# TODO: Fill these in XXX
+
+# Texture pipeline registers in r28-r29
+TEXTURE_BASE = 28
+
+def decode_texture_reg_number(reg):
+    r = reg.split(".")[0]
+
+    if r[0] == "r":
+        return (True, int(r[1:]) - TEXTURE_BASE, 0)
+    else:
+        no = int(r[2:])
+        return (False, (no >> 1) - TEXTURE_BASE, no & 1)
+
+def decode_texture_reg(reg):
+    (full, select, upper) = decode_texture_reg_number(reg)
+
+    # Swizzle mandatory for texture registers, afaict
+    swizzle = reg.split(".")[1]
+    swizzleL = swizzle_component[swizzle[0]]
+    swizzleR = swizzle_component[swizzle[1]]
+
+    return (full, select, upper, swizzleR, swizzleL)
+
+def decode_texture_out_reg(reg):
+    (full, select, upper) = decode_texture_reg_number(reg)
+    (mask, _) = mask_from_parts(reg.split("."), False)
+
+    return (full, select, upper, mask)
+
+instruction_stream = []
+
+for p in program:
+    ins = p[0]
+    arguments = p[1]
+
+    family = ins_mod = ins.split(".")[0]
+    ins_op = (ins + ".").split(".")[1]
+
+    ins_outmod = (ins + "." + ".").split(".")[2]
+    
+    try:
+        out_mod = t6xx_outmod[ins_outmod]
+    except:
+        out_mod = 0
+
+    if ins in t6xx_load_store_ops:
+        op = t6xx_load_store_ops[ins]
+        (reg, mask, component, half, upper) = decode_masked_reg(p[1][0], False)
+        (immediate, address, swizzle, half, upper, modifiers) = decode_reg(p[1][1])
+        unknown = int(p[1][2], 16)
+        b = (op << 0) | (reg << 8) | (mask << 13) | (swizzle << 17) | (unknown << 25) | (address << 51)
+        instruction_stream += [(LDST, b)]
+    elif ins_op in t6xx_alu_ops:
+        op = t6xx_alu_ops[ins_op]
+
+        (reg_out, mask, out_component, half0, upper0) = decode_masked_reg(p[1][0], True)
+        (_, reg_in1, swizzle1, half1, upper1, mod1) = decode_reg(p[1][1])
+        (immediate, reg_in2, swizzle2, half2, upper2, mod2) = decode_reg(p[1][2])
+
+        if immediate:
+            register_word = (reg_in1 << 0) | ((reg_in2 >> 11) << 5) | (reg_out << 10) | (1 << 15)
+        else:
+            register_word = (reg_in1 << 0) | (reg_in2 << 5) | (reg_out << 10) 
+
+        if ins_mod in ["vadd", "vmul", "lut"]:
+            io_mode = t6xx_reg_mode["half" if half0 else "full"]
+            repsel = 0
+            i1half = half1
+            i2block = 0
+            output_override = 2 # NORMAL, TODO
+            wr_mask = 0
+
+            if (ins_outmod == "quarter"):
+                io_mode = t6xx_reg_mode["quarter"]
+
+            if half0:
+                # TODO: half actually
+                repsel = 2 * upper1 
+            else:
+                repsel = upper1
+
+            if half0:
+                # Rare case...
+
+                (_, halfmask, _, _, _) = decode_masked_reg(p[1][0], False)
+                wr_mask = halfmask
+            else:
+                wr_mask = mask
+
+
+            if immediate:
+                # Inline constant: lower 11 bits
+
+                i2block = ((reg_in2 & 0xFF) << 3) | ((reg_in2 >> 8) & 0x7)
+            else:
+                if half0:
+                    # TODO: replicate input 2 if half
+                    pass
+                else:
+                    # TODO: half selection
+                    i2block = upper2 | (half2 << 2)
+
+                i2block |= swizzle2 << 3
+
+            # Extra modifier for some special cased stuff
+            try:
+                special = ins.split(".")[3]
+
+                if special == "low":
+                    output_override = 0 # low
+                elif special == "fulllow":
+                    # TODO: Not really a special case, just a bug?
+                    io_mode = t6xx_reg_mode["full"]
+                    output_override = 0 #low
+                    wr_mask = 0xFF
+            except:
+                pass
+
+            instruction_word = (op << 0) | (io_mode << 8) | (mod1 << 10) | (repsel << 12) | (i1half << 14) | (swizzle1 << 15) | (mod2 << 23) | (i2block << 25) | (output_override << 36) | (out_mod << 38) | (wr_mask << 40)
+        elif ins_mod in ["sadd", "smul"]:
+            # TODO: What are these?
+            unknown2 = 0
+            unknown3 = 0
+
+            i1comp_block = 0
+
+            if half1:
+                i1comp_block = swizzle1 | (upper1 << 2)
+            else:
+                i1comp_block = swizzle1 << 1
+
+            i2block = 0
+
+            if immediate:
+                # Inline constant is splattered in a... bizarre way
+
+                i2block = (((reg_in2 >> 9) & 3) << 0) | (((reg_in2 >> 8) & 1) << 2) | (((reg_in2 >> 5) & 7) << 3) | (((reg_in2 >> 0) & 15) << 6)
+            else:
+                # TODO: half register
+                swizzle2 = (swizzle2 << 1) & 0x1F
+                i2block = (mod2 << 0) | ((not half2) << 2) | (swizzle2 << 3) | (unknown2 << 5)
+
+            outcomp_block = 0
+            
+            if True:
+                outcomp_block = out_component << 1
+            else:
+                # TODO: half register
+                pass
+
+            instruction_word = (op << 0) | (mod1 << 8) | ((not half1) << 10) | (i1comp_block << 11) | (i2block << 14) | (unknown3 << 25) | (out_mod << 26) | ((not half0) << 28) | (outcomp_block) << 29
+
+        else:
+            instruction_word = op
+
+        instruction_stream += [(ALU, ins_mod, register_word, instruction_word)]
+    elif family == "texture":
+        # Texture ops use long series of modifiers to describe their needed
+        # capabilities, seperated by dots. Decode them here
+        parts = ins.split(".")
+
+        # First few modifiers are fixed, like an instruction name
+        tex_op = parts[1]
+        tex_fmt = parts[2]
+
+        # The remaining are variable, but strictly ordered
+        parts = parts[3:]
+
+        op = texture_op[tex_op]
+
+        # Some bits are defined directly in the modifier list
+        shadow = "shadow" in parts
+        cont = "cont" in parts
+        last = "last" in parts
+        has_filter = "raw" not in parts
+
+        # The remaining need order preserved since they have their own arguments
+        argument_parts = [part for part in parts if part not in ["shadow", "cont", "last", "raw"]]
+
+        bias_lod = 0
+
+        for argument, part in zip(argument_parts, arguments[4:]):
+            if argument == "bias":
+                bias_lod = int(float(part) * 256)
+            else:
+                print("Unknown argument: " + str(argument))
+
+        fmt = texture_fmt[tex_fmt]
+        has_offset = 0
+
+        magic1 = 1 # IDEK
+        magic2 = 2 # Where did this even come from?!
+
+        texture_handle = int(arguments[1][len("texture"):])
+        
+        sampler_parts = arguments[2].split(".")
+        sampler_handle = int(sampler_parts[0][len("sampler"):])
+        swizzle0 = standard_swizzle_from_parts(sampler_parts)
+
+        (full0, select0, upper0, mask0) = decode_texture_out_reg(arguments[0])
+        (full1, select1, upper1, swizzleR1, swizzleL1) = decode_texture_reg(arguments[3])
+
+        tex = (op << 0) | (shadow << 6) | (cont << 8) | (last << 9) | (fmt << 10) | (has_offset << 15) | (has_filter << 16) | (select1 << 17) | (upper1 << 18) | (swizzleL1 << 19) | (swizzleR1 << 21) | (0 << 23) | (magic2 << 25) | (full0 << 29) | (magic1 << 30) | (select0 << 32) | (upper0 << 33) | (mask0 << 34) | (swizzle0 << 40) | (bias_lod << 72) | (texture_handle << 88) | (sampler_handle << 104)
+
+        instruction_stream += [(TEXTURE, tex)]
+    elif family == "br":
+        cond = ins.split(".")[2]
+        condition = branch_condition[cond]
+        bop = compact_branch_op[ins_op]
+
+        offset = int(arguments[0].split("->")[0])
+
+        # 2's complement and chill
+        if offset < 0:
+            offset = (1 << 7) - abs(offset)
+
+        # Find where we're going
+        dest_tag = int(arguments[0].split("->")[1])
+
+        br = (bop << 0) | (dest_tag << 3) | (offset << 7) | (condition << 14)
+
+        # TODO: Unconditional branch encoding
+
+        instruction_stream += [(ALU, "br", None, br)]
+    elif ins[1:] == "constants":
+        if ins[0] not in constant_types:
+            print("Unknown constant type " + str(constant_type))
+            break
+
+        (fmt, cast) = constant_types[ins[0]]
+
+        encoded = [struct.pack(fmt, cast(f)) for f in p[1]]
+
+        consts = bytearray()
+        for c in encoded:
+            consts += c
+
+        # consts must be exactly 4 quadwords, so pad with zeroes if necessary
+        consts += bytes(4*4 - len(consts))
+
+        instruction_stream += [(ALU, "constants", consts)]
+
+# Emit from instruction stream
+instructions = []
+index = 0
+while index < len(instruction_stream):
+    output_stream = bytearray()
+    ins = instruction_stream[index]
+    tag = ins[0]
+
+    can_prefetch = index + 1 < len(instruction_stream)
+    succeeding = None
+
+    if tag == LDST:
+        succeeding = instruction_stream[index + 1] if can_prefetch else None
+        parta = ins[1]
+        partb = None
+
+        if succeeding and succeeding[0] == LDST:
+            partb = succeeding[1]
+            index += 1
+        else:
+            partb = parta
+            parta = t6xx_load_store_ops["ld_st_noop"]
+
+        tag8 = t6xx_tag["load_store"]
+
+        ins = (partb << 68) | (parta << 8) | tag8
+        output_stream += (ins.to_bytes(16, "little"))
+    elif tag == TEXTURE:
+        tag8 = t6xx_tag["texture"] 
+        ins = (ins[1] << 8) | tag8
+
+        output_stream += (ins.to_bytes(16, "little"))
+    elif tag == ALU:
+        # TODO: Combining ALU ops
+
+        emit_size = 4 # 32-bit tag always emitted
+
+        tag = 0
+        register_words = bytearray()
+        body_words = bytearray()
+        constant_words = None
+
+        last_alu_bit = 0
+
+        # Iterate through while there are ALU tags in strictly ascending order
+        while index < len(instruction_stream) and instruction_stream[index][0] == ALU and t6xx_alu_bits[instruction_stream[index][1]] > last_alu_bit:
+            ins = instruction_stream[index]
+
+            bit = t6xx_alu_bits[ins[1]]
+            last_alu_bit = bit
+
+            if ins[1] == "constants":
+                constant_words = ins[2]
+            else:
+                # Flag for the used part of the GPU
+                tag |= 1 << bit
+
+                # 16-bit register word, if present
+                if ins[2] is not None:
+                    register_words += (ins[2].to_bytes(2, "little"))
+                    emit_size += 2
+
+                size = int(t6xx_alu_size_bits[ins[1]] / 8)
+                body_words += (ins[3].to_bytes(size, "little"))
+                emit_size += size
+
+            index += 1
+
+        index -= 1 # fix off by one, from later loop increment
+
+        # Pad to nearest multiple of 4 words
+        padding = (16 - (emit_size & 15)) if (emit_size & 15) else 0
+        emit_size += padding
+
+        # emit_size includes constants
+        if constant_words:
+            emit_size += len(constant_words)
+
+        # Calculate tag given size
+        words = emit_size >> 2
+        tag |= t6xx_tag["alu" + str(words)]
+
+        # Actually emit, now that we can
+        output_stream += tag.to_bytes(4, "little")
+        output_stream += register_words
+        output_stream += body_words
+        output_stream += bytes(padding)
+
+        if constant_words:
+            output_stream += constant_words
+
+    instructions += [output_stream]
+    index += 1
+
+# Assmebly over; just emit tags at this point
+binary = bytearray()
+
+for (idx, ins) in enumerate(instructions):
+    # Instruction prefetch
+    tag = 0
+
+    if idx + 1 < len(instructions):
+        tag = instructions[idx + 1][0] & 0xF
+
+        # Check for ALU special case
+
+        if is_tag_alu(tag) and idx + 2 == len(instructions):
+            tag = 1
+    else:
+        # Instruction stream over
+        
+        tag = 1
+
+    ins[0] |= tag << 4
+
+    binary += ins
+
+pprint.pprint(program)
+
+with open(sys.argv[2], "wb") as f:
+    f.write(binary)
diff --git a/src/gallium/drivers/panfrost/midgard/blend-constcolor-zero.asm b/src/gallium/drivers/panfrost/midgard/blend-constcolor-zero.asm
new file mode 100644
index 0000000..1aae836
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/blend-constcolor-zero.asm
@@ -0,0 +1,17 @@
+ld_color_buffer_8 r1.x, 0.xxxx, 0x0
+
+ld_color_buffer_8 r1.y, 0.xxxx, 0x1
+
+ld_color_buffer_8 r1.z, 0.xxxx, 0x2
+
+ld_color_buffer_8 r1.w, 0.xxxx, 0x3
+
+vadd.u2f hr2.xyzw, abs(hr2), #0
+
+vadd.f2u8.pos.low hr0, hr2, #0
+
+vmul.imov.quarter r0, r0, r0
+br.write.always +0 -> 8
+
+vmul.imov.quarter r0, r0, r0
+br.write.always -1 -> 8
diff --git a/src/gallium/drivers/panfrost/midgard/blend.frag b/src/gallium/drivers/panfrost/midgard/blend.frag
new file mode 100644
index 0000000..3bdec0b
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/blend.frag
@@ -0,0 +1,11 @@
+/* Input source color in gl_Color; backbuffer color in gl_SecondaryColor; constant in uniform */
+
+uniform vec4 constant;
+
+void main() {
+	vec4 multiply = gl_Color * gl_SecondaryColor;
+	vec4 screen = 1.0 - (1.0 - gl_Color) * (1.0 - gl_SecondaryColor);
+	gl_FragColor = screen;
+
+	gl_FragColor = constant.a > 0.5 ? screen : multiply;
+}
diff --git a/src/gallium/drivers/panfrost/midgard/blendify.sh b/src/gallium/drivers/panfrost/midgard/blendify.sh
new file mode 100755
index 0000000..1c0f51e
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/blendify.sh
@@ -0,0 +1 @@
+python3 assemble.py blend-constcolor-zero.asm blend-constcolor-zero.bin
diff --git a/src/gallium/drivers/panfrost/midgard/cmdline.c b/src/gallium/drivers/panfrost/midgard/cmdline.c
new file mode 100644
index 0000000..965c8d8
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/cmdline.c
@@ -0,0 +1,140 @@
+/*
+ * Copyright (C) 2018 Alyssa Rosenzweig <alyssa@rosenzweig.io>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "compiler/glsl/standalone.h"
+#include "compiler/glsl/glsl_to_nir.h"
+#include "compiler/nir_types.h"
+#include "midgard_compile.h"
+#include "disassemble.h"
+#include "util/u_dynarray.h"
+#include "main/mtypes.h"
+
+bool c_do_mat_op_to_vec(struct exec_list *instructions);
+
+static void
+finalise_to_disk(const char *filename, struct util_dynarray *data)
+{
+	FILE *fp;
+	printf("%s, %d\n", filename, data->size);
+	fp = fopen(filename, "wb");
+	fwrite(data->data, 1, data->size, fp);
+	fclose(fp);
+
+	util_dynarray_fini(data);
+}
+
+static void
+compile_shader(char **argv)
+{
+	struct gl_shader_program *prog;
+	nir_shader *nir;
+
+	struct standalone_options options = {
+		.glsl_version = 140,
+		.do_link = true,
+	};
+
+	prog = standalone_compile_shader(&options, 2, argv);
+	prog->_LinkedShaders[MESA_SHADER_FRAGMENT]->Program->info.stage = MESA_SHADER_FRAGMENT;
+
+	for (unsigned i = 0; i < MESA_SHADER_STAGES; ++i) {
+		if (prog->_LinkedShaders[i] == NULL)
+			continue;
+
+		c_do_mat_op_to_vec(prog->_LinkedShaders[i]->ir);
+	}
+
+	midgard_program compiled;
+	nir = glsl_to_nir(prog, MESA_SHADER_VERTEX, &midgard_nir_options);
+	midgard_compile_shader_nir(nir, &compiled, false);
+	//finalise_to_disk("/dev/shm/vertex.bin", &compiled);
+
+	nir = glsl_to_nir(prog, MESA_SHADER_FRAGMENT, &midgard_nir_options);
+	midgard_compile_shader_nir(nir, &compiled, false);
+	//finalise_to_disk("/dev/shm/fragment.bin", &compiled);
+}
+
+static void
+compile_blend(char **argv)
+{
+	struct gl_shader_program *prog;
+	nir_shader *nir;
+
+	struct standalone_options options = {
+		.glsl_version = 140,
+	};
+
+	prog = standalone_compile_shader(&options, 1, argv);
+	prog->_LinkedShaders[MESA_SHADER_FRAGMENT]->Program->info.stage = MESA_SHADER_FRAGMENT;
+
+#if 0
+	for (unsigned i = 0; i < MESA_SHADER_STAGES; ++i) {
+		if (prog->_LinkedShaders[i] == NULL)
+			continue;
+
+		c_do_mat_op_to_vec(prog->_LinkedShaders[i]->ir);
+	}
+#endif
+
+	midgard_program program;
+	nir = glsl_to_nir(prog, MESA_SHADER_FRAGMENT, &midgard_nir_options);
+	midgard_compile_shader_nir(nir, &program, true);
+	finalise_to_disk("blend.bin", &program.compiled);
+}
+
+static void
+disassemble(const char *filename)
+{
+	FILE *fp = fopen(filename, "rb");
+	assert(fp);
+
+	fseek(fp, 0, SEEK_END);
+	int filesize = ftell(fp);
+	rewind(fp);
+
+	unsigned char *code = malloc(filesize);
+	fread(code, 1, filesize, fp);
+	fclose(fp);
+
+	disassemble_midgard(code, filesize);
+	free(code);
+}
+
+int main(int argc, char **argv)
+{
+	if (argc < 2) {
+		printf("Pass a command\n");
+		exit(1);
+	}
+
+	if (strcmp(argv[1], "compile") == 0) {
+		compile_shader(&argv[2]);
+	} else if (strcmp(argv[1], "blend") == 0) {
+		compile_blend(&argv[2]);
+	} else if (strcmp(argv[1], "disasm") == 0) {
+		disassemble(argv[2]);
+	} else {
+		printf("Unknown commandn");
+		exit(1);
+	}
+}
diff --git a/src/gallium/drivers/panfrost/midgard/cppwrap.cpp b/src/gallium/drivers/panfrost/midgard/cppwrap.cpp
new file mode 100644
index 0000000..cf2ca3b
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/cppwrap.cpp
@@ -0,0 +1,9 @@
+struct exec_list;
+
+bool do_mat_op_to_vec(struct exec_list *instructions);
+
+extern "C" {
+	bool c_do_mat_op_to_vec(struct exec_list *instructions) {
+		return do_mat_op_to_vec(instructions);
+	}
+};
diff --git a/src/gallium/drivers/panfrost/midgard/disassemble.c b/src/gallium/drivers/panfrost/midgard/disassemble.c
new file mode 100644
index 0000000..afa28fd
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/disassemble.c
@@ -0,0 +1,952 @@
+/* Author(s):
+ *   Connor Abbott
+ *   Alyssa Rosenzweig
+ *
+ * Copyright (c) 2013 Connor Abbott (connor@abbott.cx)
+ * Copyright (c) 2018 Alyssa Rosenzweig (alyssa@rosenzweig.io)
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#include <stdio.h>
+#include <stdint.h>
+#include <assert.h>
+#include <inttypes.h>
+#include <string.h>
+#include "midgard.h"
+#include "midgard-parse.h"
+#include "disassemble.h"
+#include "util/half_float.h"
+
+#define DEFINE_CASE(define, str) case define: { printf(str); break; }
+
+static bool is_instruction_int = false;
+
+static void print_alu_opcode(midgard_alu_op op)
+{
+	bool int_op = false;
+
+	if (alu_opcode_names[op]) {
+		printf("%s", alu_opcode_names[op]);
+
+		int_op = alu_opcode_names[op][0] == 'i';
+	} else
+		printf("alu_op_%02X", op);
+
+	/* For constant analysis */
+	is_instruction_int = int_op;
+}
+
+static void print_ld_st_opcode(midgard_load_store_op op)
+{
+	if (load_store_opcode_names[op])
+		printf("%s", load_store_opcode_names[op]);
+	else
+		printf("ldst_op_%02X", op);
+}
+
+static bool is_embedded_constant_half = false;
+static bool is_embedded_constant_int = false;
+
+static void print_reg(unsigned reg, bool half)
+{
+	/* Perform basic static analysis for expanding constants correctly */
+
+	if (half && (reg >> 1) == 26) {
+		is_embedded_constant_half = true;
+		is_embedded_constant_int = is_instruction_int;
+	} else if (!half && reg == 26) {
+		is_embedded_constant_int = is_instruction_int;
+	}
+
+	if (half)
+		printf("h");
+	printf("r%u", reg);
+}
+
+static char* outmod_names[4] = {
+	"",
+	".pos",
+	"",
+	".sat"
+};
+
+static void print_outmod(midgard_outmod outmod)
+{
+	printf("%s", outmod_names[outmod]);
+}
+
+static void print_quad_word(uint32_t* words, unsigned tabs)
+{
+	unsigned i;
+	for (i = 0; i < 4; i++)
+		printf("0x%08X%s ", words[i], i == 3 ? "" : ",");
+	printf("\n");
+}
+
+static void print_vector_src(unsigned src_binary, bool out_high,
+							 bool out_half, unsigned reg)
+{
+	midgard_vector_alu_src* src = (midgard_vector_alu_src*)&src_binary;
+
+	if (src->negate)
+		printf("-");
+	if (src->abs)
+		printf("abs(");
+
+	//register
+
+	if (out_half)
+	{
+		if (src->half)
+			printf(" /* half */ ");
+
+		unsigned half_reg;
+		if (out_high)
+		{
+			if (src->rep_low)
+				half_reg = reg * 2;
+			else
+				half_reg = reg * 2 + 1;
+
+			if (src->rep_high)
+				printf(" /* rep_high */ ");
+		}
+		else
+		{
+			if (src->rep_high)
+				half_reg = reg * 2 + 1;
+			else
+				half_reg = reg * 2;
+
+			if (src->rep_low)
+				printf(" /* rep_low */ ");
+		}
+		print_reg(half_reg, true);
+	}
+	else
+	{
+		if (src->rep_high)
+			printf(" /* rep_high */ ");
+
+		if (src->half)
+			print_reg(reg * 2 + src->rep_low, true);
+		else {
+			if (src->rep_low)
+				printf(" /* rep_low */ ");
+			
+			print_reg(reg, false);
+		}
+	}
+
+	//swizzle
+
+	if (src->swizzle != 0xE4) //default swizzle
+	{
+		unsigned i;
+		static const char c[4] = "xyzw";
+
+		printf(".");
+
+		for (i = 0; i < 4; i++)
+			printf("%c", c[(src->swizzle >> (i * 2)) & 3]);
+	}
+
+	if (src->abs)
+		printf(")");
+}
+
+static uint16_t decode_vector_imm(unsigned src2_reg, unsigned imm)
+{
+	uint16_t ret;
+	ret = src2_reg << 11;
+	ret |= (imm & 0x7) << 8;
+	ret |= (imm >> 3) & 0xFF;
+	return ret;
+}
+
+static void print_immediate(uint16_t imm)
+{
+	if (is_instruction_int)
+		printf("#%d", imm);
+	else
+		printf("#%g", _mesa_half_to_float(imm));
+}
+
+static void print_vector_field(const char *name, uint16_t* words, uint16_t reg_word,
+							   unsigned tabs)
+{
+	midgard_reg_info* reg_info = (midgard_reg_info*)&reg_word;
+	midgard_vector_alu* alu_field = (midgard_vector_alu*) words;
+
+	if (alu_field->reg_mode != midgard_reg_mode_half &&
+		alu_field->reg_mode != midgard_reg_mode_full)
+	{
+		printf("unknown reg mode %u\n", alu_field->reg_mode);
+	}
+
+	/* For now, prefix instruction names with their unit, until we
+	 * understand how this works on a deeper level */
+	printf("%s.", name);
+
+	print_alu_opcode(alu_field->op);
+	print_outmod(alu_field->outmod);
+	printf(" ");
+
+	bool half, out_half, out_high = false;
+	unsigned mask;
+
+	half = (alu_field->reg_mode == midgard_reg_mode_half);
+
+	if (half)
+	{
+		if (alu_field->mask & 0xF)
+		{
+			out_high = false;
+
+			if ((alu_field->mask & 0xF0))
+				printf("/* %X */ ", alu_field->mask);
+
+			mask = alu_field->mask;
+		}
+		else
+		{
+			out_high = true;
+			mask = alu_field->mask >> 4;
+		}
+	}
+	else
+	{
+		mask = alu_field->mask & 1;
+		mask |= (alu_field->mask & 4) >> 1;
+		mask |= (alu_field->mask & 16) >> 2;
+		mask |= (alu_field->mask & 64) >> 3;
+	}
+
+	out_half = half;
+
+	if (alu_field->dest_override != midgard_dest_override_none)
+	{
+		if (out_half)
+			printf("/* half */ ");
+
+		out_half = true;
+		if (alu_field->dest_override == midgard_dest_override_lower)
+			out_high = false;
+		else if (alu_field->dest_override == midgard_dest_override_upper)
+			out_high = true;
+		else
+			assert(0);
+	}
+
+	if (out_half)
+	{
+		if (out_high)
+			print_reg(2 * reg_info->out_reg + 1, true);
+		else
+			print_reg(2 * reg_info->out_reg, true);
+	}
+	else
+		print_reg(reg_info->out_reg, false);
+
+	if (mask != 0xF)
+	{
+		unsigned i;
+		static const char c[4] = "xyzw";
+
+		printf(".");
+		for (i = 0; i < 4; i++)
+			if (mask & (1 << i))
+				printf("%c", c[i]);
+	}
+
+	printf(", ");
+
+	print_vector_src(alu_field->src1, out_high, half, reg_info->src1_reg);
+
+	printf(", ");
+
+	if (reg_info->src2_imm)
+	{
+		uint16_t imm = decode_vector_imm(reg_info->src2_reg, alu_field->src2 >> 2);
+		print_immediate(imm);
+	}
+	else
+	{
+		print_vector_src(alu_field->src2, out_high, half,
+						 reg_info->src2_reg);
+	}
+
+	printf("\n");
+}
+
+static void print_scalar_src(unsigned src_binary, unsigned reg)
+{
+	midgard_scalar_alu_src* src = (midgard_scalar_alu_src*)&src_binary;
+
+	if (src->negate)
+		printf("-");
+	if (src->abs)
+		printf("abs(");
+
+	if (src->full)
+		print_reg(reg, false);
+	else
+		print_reg(reg * 2 + (src->component >> 2), true);
+
+	static const char c[4] = "xyzw";\
+	printf(".%c", c[src->full ? src->component >> 1 : src->component & 3]);
+
+	if (src->abs)
+		printf(")");
+
+}
+
+static uint16_t decode_scalar_imm(unsigned src2_reg, unsigned imm)
+{
+	uint16_t ret;
+	ret = src2_reg << 11;
+	ret |= (imm & 3) << 9;
+	ret |= (imm & 4) << 6;
+	ret |= (imm & 0x38) << 2;
+	ret |= imm >> 6;
+	return ret;
+}
+
+static void print_scalar_field(const char *name, uint16_t* words, uint16_t reg_word,
+							   unsigned tabs)
+{
+	midgard_reg_info* reg_info = (midgard_reg_info*)&reg_word;
+	midgard_scalar_alu* alu_field = (midgard_scalar_alu*) words;
+
+	if (alu_field->unknown)
+		printf("scalar ALU unknown bit set\n");
+
+	printf("%s.", name);
+	print_alu_opcode(alu_field->op);
+	print_outmod(alu_field->outmod);
+	printf(" ");
+
+	if (alu_field->output_full)
+		print_reg(reg_info->out_reg, false);
+	else
+		print_reg(reg_info->out_reg * 2 + (alu_field->output_component >> 2),
+				  true);
+
+	static const char c[4] = "xyzw";
+	printf(".%c, ",
+		   c[alu_field->output_full ? alu_field->output_component >> 1 :
+			 alu_field->output_component & 3]);
+
+	print_scalar_src(alu_field->src1, reg_info->src1_reg);
+
+	printf(", ");
+
+	if (reg_info->src2_imm)
+	{
+		uint16_t imm = decode_scalar_imm(reg_info->src2_reg,
+										 alu_field->src2);
+		print_immediate(imm);
+	}
+	else
+		print_scalar_src(alu_field->src2, reg_info->src2_reg);
+
+	printf("\n");
+}
+
+static void print_branch_op(int op)
+{
+       switch (op)
+       {
+	       case midgard_jmp_writeout_op_branch_cond:	printf("cond."); break;
+	       case midgard_jmp_writeout_op_writeout:		printf("write."); break;
+	       case midgard_jmp_writeout_op_discard:		printf("discard."); break;
+	       default: 					printf("unk%d.", op); break;
+       }
+}
+
+static void print_branch_cond(int cond)
+{
+       switch (cond)
+       {
+	       case midgard_condition_write0: printf("write0"); break;
+	       case midgard_condition_false:  printf("false"); break;
+	       case midgard_condition_true:   printf("true"); break;
+	       case midgard_condition_always: printf("always"); break;
+	       default: break;
+       }
+}
+
+static void print_compact_branch_writeout_field(uint16_t word)
+{
+       midgard_jmp_writeout_op op = word & 0x7;
+
+       switch (op)
+       {
+               case midgard_jmp_writeout_op_branch_uncond:
+               {
+                       midgard_branch_uncond br_uncond;
+                       memcpy((char*) &br_uncond, (char*) &word, sizeof(br_uncond));
+                       printf("br.uncond ");
+                       if (br_uncond.unknown != 1)
+                               printf("unknown:%d, ", br_uncond.unknown);
+                       if (br_uncond.offset >= 0)
+                               printf("+");
+                       printf("%d", br_uncond.offset);
+
+		       printf(" -> %X\n", br_uncond.dest_tag);
+                       break;
+               }
+               case midgard_jmp_writeout_op_branch_cond:
+               case midgard_jmp_writeout_op_writeout:
+               case midgard_jmp_writeout_op_discard:
+	       default:
+               {
+                       midgard_branch_cond br_cond;
+                       memcpy((char*) &br_cond, (char*) &word, sizeof(br_cond));
+
+                       printf("br.");
+
+		       print_branch_op(br_cond.op);
+		       print_branch_cond(br_cond.cond);
+
+                       printf(" ");
+                       if (br_cond.offset >= 0)
+                               printf("+");
+                       printf("%d", br_cond.offset);
+
+		       printf(" -> %X\n", br_cond.dest_tag);
+                       break;
+               }
+       }
+}
+
+static void print_extended_branch_writeout_field(uint8_t *words)
+{
+       midgard_branch_extended br;
+       memcpy((char*) &br, (char*) words, sizeof(br));
+
+       printf("br.");
+
+       print_branch_op(br.op);
+       print_branch_cond(br.cond);
+
+       /* XXX: This can't be right */
+       if (br.unknown)
+	       printf(".unknown%d\n", br.unknown);
+
+       if (br.zero)
+	       printf(".zero%d\n", br.zero);
+
+       printf(" ");
+       if (br.offset >= 0)
+	       printf("+");
+       printf("%d", br.offset);
+
+       printf(" -> %X\n", br.dest_tag);
+}
+
+static unsigned num_alu_fields_enabled(uint32_t control_word)
+{
+	unsigned ret = 0;
+
+	if ((control_word >> 17) & 1)
+		ret++;
+
+	if ((control_word >> 19) & 1)
+		ret++;
+
+	if ((control_word >> 21) & 1)
+		ret++;
+
+	if ((control_word >> 23) & 1)
+		ret++;
+
+	if ((control_word >> 25) & 1)
+		ret++;
+
+	return ret;
+}
+
+static float float_bitcast(uint32_t integer)
+{
+	union {
+		uint32_t i;
+		float f;
+	} v;
+
+	v.i = integer;
+	return v.f;
+}
+
+static void print_alu_word(uint32_t* words, unsigned num_quad_words,
+						   unsigned tabs)
+{
+	uint32_t control_word = words[0];
+	uint16_t* beginning_ptr = (uint16_t*)(words + 1);
+	unsigned num_fields = num_alu_fields_enabled(control_word);
+	uint16_t* word_ptr = beginning_ptr + num_fields;
+	unsigned num_words = 2 + num_fields;
+
+	if ((control_word >> 16) & 1)
+		printf("unknown bit 16 enabled\n");
+
+	if ((control_word >> 17) & 1)
+	{
+		print_vector_field("vmul", word_ptr, *beginning_ptr, tabs);
+		beginning_ptr += 1;
+		word_ptr += 3;
+		num_words += 3;
+	}
+
+	if ((control_word >> 18) & 1)
+		printf("unknown bit 18 enabled\n");
+
+	if ((control_word >> 19) & 1)
+	{
+		print_scalar_field("sadd", word_ptr, *beginning_ptr, tabs);
+		beginning_ptr += 1;
+		word_ptr += 2;
+		num_words += 2;
+	}
+
+	if ((control_word >> 20) & 1)
+		printf("unknown bit 20 enabled\n");
+
+	if ((control_word >> 21) & 1)
+	{
+		print_vector_field("vadd", word_ptr, *beginning_ptr, tabs);
+		beginning_ptr += 1;
+		word_ptr += 3;
+		num_words += 3;
+	}
+
+	if ((control_word >> 22) & 1)
+		printf("unknown bit 22 enabled\n");
+
+	if ((control_word >> 23) & 1)
+	{
+		print_scalar_field("smul", word_ptr, *beginning_ptr, tabs);
+		beginning_ptr += 1;
+		word_ptr += 2;
+		num_words += 2;
+	}
+
+	if ((control_word >> 24) & 1)
+		printf("unknown bit 24 enabled\n");
+
+	if ((control_word >> 25) & 1)
+	{
+		print_vector_field("lut", word_ptr, *beginning_ptr, tabs);
+		beginning_ptr += 1;
+		word_ptr += 3;
+		num_words += 3;
+	}
+
+	if ((control_word >> 26) & 1)
+	{
+		print_compact_branch_writeout_field(*word_ptr);
+		word_ptr += 1;
+		num_words += 1;
+	}
+
+	if ((control_word >> 27) & 1)
+	{
+		print_extended_branch_writeout_field((uint8_t *) word_ptr);
+		word_ptr += 3;
+		num_words += 3;
+	}
+
+	if (num_quad_words > (num_words + 7) / 8)
+	{
+		assert(num_quad_words == (num_words + 15) / 8);
+		//Assume that the extra quadword is constants
+		void* consts = words + (4 * num_quad_words - 4);
+
+		if (is_embedded_constant_int) {
+			if (is_embedded_constant_half) {
+				int16_t* sconsts = (int16_t*) consts;
+				printf("sconstants %d, %d, %d, %d\n",
+					   sconsts[0],
+					   sconsts[1],
+					   sconsts[2],
+					   sconsts[3]);
+			} else {
+				int32_t* iconsts = (int32_t*) consts;
+				printf("iconstants %d, %d, %d, %d\n",
+					   iconsts[0],
+					   iconsts[1],
+					   iconsts[2],
+					   iconsts[3]);
+			}
+		} else {
+			if (is_embedded_constant_half) {
+				uint16_t* hconsts = (uint16_t*) consts;
+				printf("hconstants %g, %g, %g, %g\n",
+						_mesa_half_to_float(hconsts[0]),
+						_mesa_half_to_float(hconsts[1]),
+						_mesa_half_to_float(hconsts[2]),
+						_mesa_half_to_float(hconsts[3]));
+			} else {
+				uint32_t* fconsts = (uint32_t*) consts;
+				printf("fconstants %g, %g, %g, %g\n",
+					   float_bitcast(fconsts[0]),
+					   float_bitcast(fconsts[1]),
+					   float_bitcast(fconsts[2]),
+					   float_bitcast(fconsts[3]));
+			}
+
+		}
+	}
+}
+
+/* Swizzle/mask formats are common between load/store ops and texture ops, it
+ * looks like... */
+
+static void print_swizzle(uint32_t swizzle)
+{
+	unsigned i;
+
+	if (swizzle != 0xE4)
+	{
+		printf(".");
+		for (i = 0; i < 4; i++)
+			printf("%c", "xyzw"[(swizzle >> (2 * i)) & 3]);
+	}
+}
+
+static void print_mask(uint32_t mask)
+{
+	unsigned i;
+
+	if (mask != 0xF)
+	{
+		printf(".");
+
+		for (i = 0; i < 4; i++)
+			if (mask & (1 << i))
+				printf("%c", "xyzw"[i]);
+
+		/* Handle degenerate case */
+		if (mask == 0)
+			printf("0");
+	}
+}
+
+static void print_varying_parameters(midgard_load_store_word* word)
+{
+	midgard_varying_parameter param;
+	unsigned v = word->varying_parameters;
+	memcpy(&param, &v, sizeof(param));
+
+	if (param.is_varying) {
+		/* If a varying, there are qualifiers */
+		if (param.flat)
+			printf(".flat");
+
+		if (param.interpolation != midgard_interp_default) {
+			if (param.interpolation == midgard_interp_centroid)
+				printf(".centroid");
+			else
+				printf(".interp%d", param.interpolation);
+		}
+	} else if (param.flat || param.interpolation) {
+		printf(" /* is_varying not set but varying metadata attached */");
+	}
+
+	if (param.zero1 || param.zero2)
+		printf(" /* zero tripped, %d %d */ ", param.zero1, param.zero2);
+}
+
+static bool is_op_varying(unsigned op)
+{
+	switch (op) {
+		case midgard_op_store_vary_16:
+		case midgard_op_store_vary_32:
+		case midgard_op_load_vary_16:
+		case midgard_op_load_vary_32:
+			return true;
+	}
+
+	return false;
+}
+
+static void print_load_store_instr(uint64_t data,
+								   unsigned tabs)
+{
+	midgard_load_store_word* word = (midgard_load_store_word *) &data;
+
+	print_ld_st_opcode(word->op);
+
+	if (is_op_varying(word->op))
+		print_varying_parameters(word);
+
+	printf(" r%d", word->reg);
+	print_mask(word->mask);
+	
+	int address = word->address;
+
+	if (word->op == midgard_op_load_uniform_32) {
+		/* Uniforms use their own addressing scheme */
+
+		int lo = word->varying_parameters >> 7;
+		int hi = word->address;
+
+		/* TODO: Combine fields logically */
+		address = (hi << 3) | lo;
+	}
+
+	printf(", %d", address);
+
+	print_swizzle(word->swizzle);
+
+	printf(", 0x%X\n", word->unknown);
+}
+
+static void print_load_store_word(uint32_t* word, unsigned tabs)
+{
+	midgard_load_store* load_store = (midgard_load_store *) word;
+
+	if (load_store->word1 != 3)
+	{
+		print_load_store_instr(load_store->word1, tabs);
+	}
+	if (load_store->word2 != 3)
+	{
+		print_load_store_instr(load_store->word2, tabs);
+	}
+}
+
+static void print_texture_reg(bool full, bool select, bool upper)
+{
+	if (full)
+		printf("r%d", REG_TEX_BASE + select);
+	else
+		printf("hr%d", (REG_TEX_BASE + select)*2 + upper);
+
+	if (full && upper)
+		printf("// error: out full / upper mutually exclusive\n");
+
+}
+
+static void print_texture_format(int format)
+{
+	/* Act like a modifier */
+	printf(".");
+
+	switch(format) {
+		DEFINE_CASE(TEXTURE_2D, "2d");
+		DEFINE_CASE(TEXTURE_3D, "3d");
+
+		default:
+			printf("fmt_%d", format);
+			break;
+	}
+}
+
+static void print_texture_op(int format)
+{
+	/* Act like a modifier */
+	printf(".");
+
+	switch(format) {
+		DEFINE_CASE(TEXTURE_OP_NORMAL, "normal");
+		DEFINE_CASE(TEXTURE_OP_TEXEL_FETCH, "texelfetch");
+
+		default:
+			printf("op_%d", format);
+			break;
+	}
+}
+
+#undef DEFINE_CASE
+
+static void print_texture_word(uint32_t* word, unsigned tabs)
+{
+	midgard_texture_word* texture = (midgard_texture_word *) word;
+
+	/* Instruction family, like ALU words have theirs */
+	printf("texture");
+
+	/* Broad category of texture operation in question */
+	print_texture_op(texture->op);
+
+	/* Specific format in question */
+	print_texture_format(texture->format);
+
+	/* Instruction "modifiers" parallel the ALU instructions. First group
+	 * are modifiers that act alone */
+
+	if (!texture->filter)
+		printf(".raw");
+
+	if (texture->shadow)
+		printf(".shadow");
+
+	if (texture->cont)
+		printf(".cont");
+
+	if (texture->last)
+		printf(".last");
+
+	/* Second set are modifiers which take an extra argument each */
+
+	if (texture->has_offset)
+		printf(".offset");
+
+	if (texture->bias)
+		printf(".bias");
+
+	printf(" ");
+
+	print_texture_reg(texture->out_full, texture->out_reg_select, texture->out_upper);
+	print_mask(texture->mask);
+	printf(", ");
+
+	printf("texture%d, ", texture->texture_handle);
+
+	printf("sampler%d", texture->sampler_handle);
+	print_swizzle(texture->swizzle);
+	printf(", ");
+
+	print_texture_reg(/*texture->in_reg_full*/true, texture->in_reg_select, texture->in_reg_upper);
+	printf(".%c%c, ", "xyzw"[texture->in_reg_swizzle_left],
+			  "xyzw"[texture->in_reg_swizzle_right]);
+
+	/* TODO: can offsets be full words? */
+	if (texture->has_offset) {
+		print_texture_reg(false, texture->offset_reg_select, texture->offset_reg_upper);
+		printf(", ");
+	}
+
+	if (texture->bias)
+		printf("%f, ", texture->bias / 256.0f);
+
+	printf("\n");
+
+	/* While not zero in general, for these simple instructions the
+	 * following unknowns are zero, so we don't include them */
+
+	if (texture->unknown1 ||
+	    texture->unknown2 ||
+	    texture->unknown3 ||
+	    texture->unknown4 ||
+	    texture->unknownA ||
+	    texture->unknownB ||
+	    texture->unknown8 ||
+	    texture->unknown9) {
+		printf("// unknown1 = 0x%x\n", texture->unknown1);
+		printf("// unknown2 = 0x%x\n", texture->unknown2);
+		printf("// unknown3 = 0x%x\n", texture->unknown3);
+		printf("// unknown4 = 0x%x\n", texture->unknown4);
+		printf("// unknownA = 0x%x\n", texture->unknownA);
+		printf("// unknownB = 0x%x\n", texture->unknownB);
+		printf("// unknown8 = 0x%x\n", texture->unknown8);
+		printf("// unknown9 = 0x%x\n", texture->unknown9);
+	}
+
+	/* Similarly, if no offset is applied, these are zero. If an offset
+	 * -is- applied, or gradients are used, etc, these are nonzero but
+	 *  largely unknown still. */
+
+	if (texture->offset_unknown1 ||
+	    texture->offset_reg_select ||
+	    texture->offset_reg_upper ||
+	    texture->offset_unknown4 ||
+	    texture->offset_unknown5 ||
+	    texture->offset_unknown6 ||
+	    texture->offset_unknown7 ||
+	    texture->offset_unknown8 ||
+	    texture->offset_unknown9) {
+		printf("// offset_unknown1 = 0x%x\n", texture->offset_unknown1);
+		printf("// offset_reg_select = 0x%x\n", texture->offset_reg_select);
+		printf("// offset_reg_upper = 0x%x\n", texture->offset_reg_upper);
+		printf("// offset_unknown4 = 0x%x\n", texture->offset_unknown4);
+		printf("// offset_unknown5 = 0x%x\n", texture->offset_unknown5);
+		printf("// offset_unknown6 = 0x%x\n", texture->offset_unknown6);
+		printf("// offset_unknown7 = 0x%x\n", texture->offset_unknown7);
+		printf("// offset_unknown8 = 0x%x\n", texture->offset_unknown8);
+		printf("// offset_unknown9 = 0x%x\n", texture->offset_unknown9);
+	}
+
+	/* Don't blow up */
+	if (texture->unknown7 != 0x1)
+		printf("// (!) unknown7 = %d\n", texture->unknown7);
+}
+
+void disassemble_midgard(uint8_t* code, size_t size)
+{
+	uint32_t* words = (uint32_t *) code;
+	unsigned num_words = size / 4;
+	int tabs = 0;
+
+	bool prefetch_flag = false;
+
+	unsigned i = 0;
+	while (i < num_words)
+	{
+		unsigned num_quad_words = midgard_word_size[words[i] & 0xF];
+		switch (midgard_word_types[words[i] & 0xF])
+		{
+			case midgard_word_type_texture:
+				print_texture_word(&words[i], tabs);
+				break;
+
+			case midgard_word_type_load_store:
+				print_load_store_word(&words[i], tabs);
+				break;
+
+			case midgard_word_type_alu:
+				print_alu_word(&words[i], num_quad_words, tabs);
+
+				if (prefetch_flag)
+					return;
+
+				/* Reset word static analysis state */
+				is_embedded_constant_half = false;
+				is_embedded_constant_int = false;
+
+				break;
+
+			default:
+				printf("Unknown word type %u:\n", words[i] & 0xF);
+				num_quad_words = 1;
+				print_quad_word(&words[i], tabs);
+				printf("\n");
+				break;
+		}
+
+		printf("\n");
+
+		unsigned next = (words[i] & 0xF0) >> 4;
+
+		i += 4 * num_quad_words;
+
+		/* Break based on instruction prefetch flag */
+
+		if (i < num_words && next == 1) {
+			prefetch_flag = true;
+
+			if (midgard_word_types[words[i] & 0xF] != midgard_word_type_alu)
+				return;
+		}
+	}
+
+	return;
+}
diff --git a/src/gallium/drivers/panfrost/midgard/disassemble.h b/src/gallium/drivers/panfrost/midgard/disassemble.h
new file mode 100644
index 0000000..3919695
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/disassemble.h
@@ -0,0 +1,2 @@
+#include <stddef.h>
+void disassemble_midgard(uint8_t* code, size_t size);
diff --git a/src/gallium/drivers/panfrost/midgard/helpers.h b/src/gallium/drivers/panfrost/midgard/helpers.h
new file mode 100644
index 0000000..0954577
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/helpers.h
@@ -0,0 +1,220 @@
+/* Author(s):
+ *  Alyssa Rosenzweig
+ *
+ * Copyright (c) 2018 Alyssa Rosenzweig (alyssa@rosenzweig.io)
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+/* Some constants and macros not found in the disassembler */
+
+#define OP_IS_STORE(op) (\
+		op == midgard_op_store_vary_16 || \
+		op == midgard_op_store_vary_32 \
+	)
+
+/* ALU control words are single bit fields with a lot of space */
+
+#define ALU_ENAB_VEC_MUL  (1 << 17)
+#define ALU_ENAB_SCAL_ADD  (1 << 19)
+#define ALU_ENAB_VEC_ADD  (1 << 21)
+#define ALU_ENAB_SCAL_MUL  (1 << 23)
+#define ALU_ENAB_VEC_LUT  (1 << 25)
+#define ALU_ENAB_BR_COMPACT (1 << 26)
+#define ALU_ENAB_BRANCH   (1 << 27)
+
+/* Vector-independant shorthands for the above; these numbers are arbitrary and
+ * not from the ISA. Convert to the above with unit_enum_to_midgard */
+
+#define UNIT_MUL 0
+#define UNIT_ADD 1
+#define UNIT_LUT 2
+
+/* 4-bit type tags */
+
+#define TAG_TEXTURE_4 0x3
+#define TAG_LOAD_STORE_4 0x5
+#define TAG_ALU_4 0x8
+#define TAG_ALU_8 0x9
+#define TAG_ALU_12 0xA
+#define TAG_ALU_16 0xB
+
+/* Special register aliases */
+
+#define MAX_WORK_REGISTERS 16
+
+/* Uniforms are begin at (REGISTER_UNIFORMS - uniform_count) */
+#define REGISTER_UNIFORMS 24
+
+#define REGISTER_UNUSED 24
+#define REGISTER_CONSTANT 26
+#define REGISTER_VARYING_BASE 26
+#define REGISTER_OFFSET 27
+#define REGISTER_TEXTURE_BASE 28
+#define REGISTER_SELECT 31
+
+/* Special uniforms used for e.g. vertex epilogues */
+#define SPECIAL_UNIFORM_BASE (1 << 24)
+#define UNIFORM_VIEWPORT (SPECIAL_UNIFORM_BASE + 0)
+
+/* SSA helper aliases to mimic the registers. UNUSED_0 encoded as an inline
+ * constant. UNUSED_1 encoded as REGISTER_UNUSED */
+
+#define SSA_UNUSED_0 0
+#define SSA_UNUSED_1 -2
+
+#define SSA_FIXED_SHIFT 24
+#define SSA_FIXED_REGISTER(reg) ((1 + reg) << SSA_FIXED_SHIFT)
+#define SSA_REG_FROM_FIXED(reg) ((reg >> SSA_FIXED_SHIFT) - 1)
+#define SSA_FIXED_MINIMUM SSA_FIXED_REGISTER(0)
+
+/* Swizzle support */
+
+#define SWIZZLE(A, B, C, D) ((D << 6) | (C << 4) | (B << 2) | (A << 0))
+#define SWIZZLE_FROM_ARRAY(r) SWIZZLE(r[0], r[1], r[2], r[3])
+#define COMPONENT_X 0x0
+#define COMPONENT_Y 0x1
+#define COMPONENT_Z 0x2
+#define COMPONENT_W 0x3
+
+/* See ISA notes */
+
+#define LDST_NOP (3)
+
+/* Is this opcode that of an integer? */
+static bool
+midgard_is_integer_op(int op)
+{
+	switch (op) {
+		case midgard_alu_op_iadd: 
+		case midgard_alu_op_ishladd: 
+		case midgard_alu_op_isub: 
+		case midgard_alu_op_imul: 
+		case midgard_alu_op_imin: 
+		case midgard_alu_op_imax: 
+		case midgard_alu_op_iasr: 
+		case midgard_alu_op_ilsr: 
+		case midgard_alu_op_ishl: 
+		case midgard_alu_op_iand: 
+		case midgard_alu_op_ior: 
+		case midgard_alu_op_inot: 
+		case midgard_alu_op_iandnot: 
+		case midgard_alu_op_ixor: 
+		case midgard_alu_op_imov: 
+		//case midgard_alu_op_f2i: 
+		//case midgard_alu_op_f2u: 
+		case midgard_alu_op_ieq: 
+		case midgard_alu_op_ine: 
+		case midgard_alu_op_ilt: 
+		case midgard_alu_op_ile: 
+		case midgard_alu_op_iball_eq: 
+		case midgard_alu_op_ibany_neq: 
+		//case midgard_alu_op_i2f: 
+		//case midgard_alu_op_u2f: 
+		case midgard_alu_op_icsel: 
+			return true;
+		default:
+			return false;
+	}
+}
+
+/* There are five ALU units: VMUL, VADD, SMUL, SADD, LUT. A given opcode is
+ * implemented on some subset of these units (or occassionally all of them).
+ * This table encodes a bit mask of valid units for each opcode, so the
+ * scheduler can figure where to plonk the instruction. */
+
+/* Shorthands for each unit */
+#define UNIT_VMUL ALU_ENAB_VEC_MUL
+#define UNIT_SADD ALU_ENAB_SCAL_ADD
+#define UNIT_VADD ALU_ENAB_VEC_ADD
+#define UNIT_SMUL ALU_ENAB_SCAL_MUL
+#define UNIT_VLUT ALU_ENAB_VEC_LUT
+
+/* Shorthands for usual combinations of units. LUT is intentionally excluded
+ * since it's nutty. */
+
+#define UNITS_MUL (UNIT_VMUL | UNIT_SMUL)
+#define UNITS_ADD (UNIT_VADD | UNIT_SADD)
+#define UNITS_ALL (UNITS_MUL | UNITS_ADD)
+#define UNITS_SCALAR (UNIT_SADD | UNIT_SMUL)
+#define UNITS_VECTOR (UNIT_VMUL | UNIT_VADD)
+#define UNITS_ANY_VECTOR (UNITS_VECTOR | UNIT_VLUT)
+
+static int alu_opcode_unit[256] = {
+	[midgard_alu_op_fadd]		 = UNITS_ADD,
+	[midgard_alu_op_fmul]		 = UNITS_MUL | UNIT_VLUT,
+	[midgard_alu_op_fmin]		 = UNITS_MUL,
+	[midgard_alu_op_fmax]		 = UNITS_MUL,
+	[midgard_alu_op_imin]		 = UNITS_ALL,
+	[midgard_alu_op_imax]		 = UNITS_ALL,
+	[midgard_alu_op_fmov]		 = UNITS_ALL | UNIT_VLUT,
+	[midgard_alu_op_ffloor]		 = UNITS_ADD,
+	[midgard_alu_op_fceil]		 = UNITS_ADD,
+
+	/* Though they output a scalar, they need to run on a vector unit
+	 * since they process vectors */
+	[midgard_alu_op_fdot3]		 = UNIT_VMUL,
+	[midgard_alu_op_fdot4]		 = UNIT_VMUL,
+
+	[midgard_alu_op_iadd]		 = UNITS_ADD,
+	[midgard_alu_op_isub]		 = UNITS_ADD,
+	[midgard_alu_op_imul]		 = UNITS_ALL,
+	[midgard_alu_op_imov]		 = UNITS_ALL,
+
+	/* For vector comparisons, use ball etc */
+	[midgard_alu_op_feq]		 = UNITS_ALL,
+	[midgard_alu_op_fne]		 = UNITS_ALL,
+	[midgard_alu_op_flt]		 = UNIT_SADD,
+	[midgard_alu_op_ieq]		 = UNITS_ALL,
+	[midgard_alu_op_ine]		 = UNITS_ALL,
+	[midgard_alu_op_ilt]		 = UNITS_ALL,
+	[midgard_alu_op_ile]		 = UNITS_ALL,
+
+	[midgard_alu_op_icsel]		 = UNITS_ADD,
+	[midgard_alu_op_fcsel]		 = UNITS_ADD | UNIT_SMUL,
+
+	[midgard_alu_op_frcp]		 = UNIT_VLUT,
+	[midgard_alu_op_frsqrt]		 = UNIT_VLUT,
+	[midgard_alu_op_fsqrt]		 = UNIT_VLUT,
+	[midgard_alu_op_fexp2]		 = UNIT_VLUT,
+	[midgard_alu_op_flog2]		 = UNIT_VLUT,
+
+	[midgard_alu_op_f2i]		 = UNITS_ADD,
+	[midgard_alu_op_f2u]		 = UNITS_ADD,
+	[midgard_alu_op_f2u8]		 = UNITS_ADD,
+	[midgard_alu_op_i2f]		 = UNITS_ADD,
+	[midgard_alu_op_u2f]		 = UNITS_ADD,
+
+	[midgard_alu_op_fsin]		 = UNIT_VLUT,
+	[midgard_alu_op_fcos]		 = UNIT_VLUT,
+
+	[midgard_alu_op_iand]		 = UNITS_ADD, /* XXX: Test case where it's right on smul but not sadd */
+	[midgard_alu_op_ior]		 = UNITS_ADD,
+	[midgard_alu_op_ixor]		 = UNITS_ADD,
+	[midgard_alu_op_inot]		 = UNITS_ALL,
+	[midgard_alu_op_ishl]		 = UNITS_ADD,
+	[midgard_alu_op_iasr]		 = UNITS_ADD,
+	[midgard_alu_op_ilsr]		 = UNITS_ADD,
+	[midgard_alu_op_ilsr]		 = UNITS_ADD,
+
+	[midgard_alu_op_fball_eq]	 = UNITS_ALL,
+	[midgard_alu_op_fbany_neq]	 = UNITS_ALL,
+	[midgard_alu_op_iball_eq]	 = UNITS_ALL,
+	[midgard_alu_op_ibany_neq]	 = UNITS_ALL
+};
diff --git a/src/gallium/drivers/panfrost/midgard/midgard-parse.h b/src/gallium/drivers/panfrost/midgard/midgard-parse.h
new file mode 100644
index 0000000..2708eaf
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/midgard-parse.h
@@ -0,0 +1,70 @@
+/* Author(s):
+ *   Connor Abbott
+ *   Alyssa Rosenzweig
+ *
+ * Copyright (c) 2013 Connor Abbott (connor@abbott.cx)
+ * Copyright (c) 2018 Alyssa Rosenzweig (alyssa@rosenzweig.io)
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#ifndef __midgard_parse_h__
+#define __midgard_parse_h__
+
+/* Additional metadata for parsing Midgard binaries, not needed for compilation */
+
+static midgard_word_type midgard_word_types[16] = {
+	midgard_word_type_unknown,    /* 0x0 */
+	midgard_word_type_unknown,    /* 0x1 */
+	midgard_word_type_texture,    /* 0x2 */
+	midgard_word_type_texture,    /* 0x3 */
+	midgard_word_type_unknown,    /* 0x4 */
+	midgard_word_type_load_store, /* 0x5 */
+	midgard_word_type_unknown,    /* 0x6 */
+	midgard_word_type_unknown,    /* 0x7 */
+	midgard_word_type_alu,        /* 0x8 */
+	midgard_word_type_alu,        /* 0x9 */
+	midgard_word_type_alu,        /* 0xA */
+	midgard_word_type_alu,        /* 0xB */
+	midgard_word_type_alu,        /* 0xC */
+	midgard_word_type_alu,        /* 0xD */
+	midgard_word_type_alu,        /* 0xE */
+	midgard_word_type_alu,        /* 0xF */
+};
+
+static unsigned midgard_word_size[16] = {
+	0, /* 0x0 */
+	0, /* 0x1 */
+	1, /* 0x2 */
+	1, /* 0x3 */
+	0, /* 0x4 */
+	1, /* 0x5 */
+	0, /* 0x6 */
+	0, /* 0x7 */
+	1, /* 0x8 */
+	2, /* 0x9 */
+	3, /* 0xA */
+	4, /* 0xB */
+	1, /* 0xC */
+	2, /* 0xD */
+	3, /* 0xE */
+	4, /* 0xF */
+};
+
+#endif
diff --git a/src/gallium/drivers/panfrost/midgard/midgard.h b/src/gallium/drivers/panfrost/midgard/midgard.h
new file mode 100644
index 0000000..e0ac466
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/midgard.h
@@ -0,0 +1,470 @@
+/* Author(s):
+ *   Connor Abbott
+ *   Alyssa Rosenzweig
+ *
+ * Copyright (c) 2013 Connor Abbott (connor@abbott.cx)
+ * Copyright (c) 2018 Alyssa Rosenzweig (alyssa@rosenzweig.io)
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#ifndef __midgard_h__
+#define __midgard_h__
+
+#include <stdint.h>
+#include <stdbool.h>
+
+typedef enum
+{
+	midgard_word_type_alu,
+	midgard_word_type_load_store,
+	midgard_word_type_texture,
+	midgard_word_type_unknown
+} midgard_word_type;
+
+typedef enum
+{
+	midgard_alu_vmul,
+	midgard_alu_sadd,
+	midgard_alu_smul,
+	midgard_alu_vadd,
+	midgard_alu_lut
+} midgard_alu;
+
+/*
+ * ALU words
+ */
+
+typedef enum
+{
+	midgard_alu_op_fadd       = 0x10,
+	midgard_alu_op_fmul       = 0x14,
+	midgard_alu_op_fmin       = 0x28,
+	midgard_alu_op_fmax       = 0x2C,
+	midgard_alu_op_fmov       = 0x30,
+	midgard_alu_op_ffloor     = 0x36,
+	midgard_alu_op_fceil      = 0x37,
+	midgard_alu_op_fdot3      = 0x3C,
+	midgard_alu_op_fdot3r     = 0x3D,
+	midgard_alu_op_fdot4      = 0x3E,
+	midgard_alu_op_freduce    = 0x3F,
+	midgard_alu_op_iadd       = 0x40,
+	midgard_alu_op_ishladd    = 0x41,
+	midgard_alu_op_isub       = 0x46,
+	midgard_alu_op_imul       = 0x58,
+	midgard_alu_op_imin       = 0x60,
+	midgard_alu_op_imax       = 0x62,
+	midgard_alu_op_iasr       = 0x68,
+	midgard_alu_op_ilsr       = 0x69,
+	midgard_alu_op_ishl       = 0x6E,
+	midgard_alu_op_iand       = 0x70,
+	midgard_alu_op_ior        = 0x71,
+	midgard_alu_op_inot       = 0x72,
+	midgard_alu_op_iandnot    = 0x74, /* (a, b) -> a & ~b, used for not/b2f */
+	midgard_alu_op_ixor       = 0x76,
+	midgard_alu_op_imov       = 0x7B,
+	midgard_alu_op_feq        = 0x80,
+	midgard_alu_op_fne        = 0x81,
+	midgard_alu_op_flt        = 0x82,
+	midgard_alu_op_fle        = 0x83,
+	midgard_alu_op_fball_eq   = 0x88,
+	midgard_alu_op_bball_eq   = 0x89,
+	midgard_alu_op_bbany_neq  = 0x90, /* used for bvec4(1) */
+	midgard_alu_op_fbany_neq  = 0x91, /* bvec4(0) also */
+	midgard_alu_op_f2i        = 0x99,
+	midgard_alu_op_f2u8       = 0x9C,
+	midgard_alu_op_f2u        = 0x9D,
+	midgard_alu_op_ieq        = 0xA0,
+	midgard_alu_op_ine        = 0xA1,
+	midgard_alu_op_ilt        = 0xA4,
+	midgard_alu_op_ile        = 0xA5,
+	midgard_alu_op_iball_eq   = 0xA8,
+	midgard_alu_op_ball       = 0xA9,
+	midgard_alu_op_ibany_neq  = 0xB1, 
+	midgard_alu_op_i2f        = 0xB8,
+	midgard_alu_op_u2f        = 0xBC,
+	midgard_alu_op_icsel      = 0xC1,
+	midgard_alu_op_fcsel      = 0xC5,
+	midgard_alu_op_fatan_pt2  = 0xE8,
+	midgard_alu_op_frcp       = 0xF0,
+	midgard_alu_op_frsqrt     = 0xF2,
+	midgard_alu_op_fsqrt      = 0xF3,
+	midgard_alu_op_fexp2      = 0xF4,
+	midgard_alu_op_flog2      = 0xF5,
+	midgard_alu_op_fsin       = 0xF6,
+	midgard_alu_op_fcos       = 0xF7,
+	midgard_alu_op_fatan2_pt1 = 0xF9,
+} midgard_alu_op;
+
+typedef enum
+{
+	midgard_outmod_none = 0,
+	midgard_outmod_pos  = 1,
+	midgard_outmod_int  = 2,
+	midgard_outmod_sat  = 3
+} midgard_outmod;
+
+typedef enum
+{
+	midgard_reg_mode_quarter = 0,
+	midgard_reg_mode_half = 1,
+	midgard_reg_mode_full = 2,
+	midgard_reg_mode_double = 3 /* TODO: verify */
+} midgard_reg_mode;
+
+typedef enum
+{
+	midgard_dest_override_lower = 0,
+	midgard_dest_override_upper = 1,
+	midgard_dest_override_none = 2
+} midgard_dest_override;
+
+typedef struct
+__attribute__((__packed__))
+{
+	bool abs         : 1;
+	bool negate      : 1;
+
+	/* replicate lower half if dest = half, or low/high half selection if
+	 * dest = full
+	 */
+	bool rep_low     : 1;
+	bool rep_high    : 1; /* unused if dest = full */
+	bool half        : 1; /* only matters if dest = full */
+	unsigned swizzle : 8;
+} midgard_vector_alu_src;
+
+typedef struct
+__attribute__((__packed__))
+{
+	midgard_alu_op op               :  8;
+	midgard_reg_mode reg_mode   :  2;
+	unsigned src1 : 13;
+	unsigned src2 : 13;
+	midgard_dest_override dest_override : 2;
+	midgard_outmod outmod               : 2;
+	unsigned mask                           : 8;
+} midgard_vector_alu;
+
+typedef struct
+__attribute__((__packed__))
+{
+	bool abs           : 1;
+	bool negate        : 1;
+	bool full          : 1; /* 0 = half, 1 = full */
+	unsigned component : 3;
+} midgard_scalar_alu_src;
+
+typedef struct
+__attribute__((__packed__))
+{
+	midgard_alu_op op         :  8;
+	unsigned src1             :  6;
+	unsigned src2             : 11;
+	unsigned unknown          :  1;
+	midgard_outmod outmod :  2;
+	bool output_full          :  1;
+	unsigned output_component :  3;
+} midgard_scalar_alu;
+
+typedef struct
+__attribute__((__packed__))
+{
+	unsigned src1_reg : 5;
+	unsigned src2_reg : 5;
+	unsigned out_reg  : 5;
+	bool src2_imm     : 1;
+} midgard_reg_info;
+
+typedef enum
+{
+	midgard_jmp_writeout_op_branch_uncond = 1,
+	midgard_jmp_writeout_op_branch_cond = 2,
+	midgard_jmp_writeout_op_discard = 4,
+	midgard_jmp_writeout_op_writeout = 7,
+} midgard_jmp_writeout_op;
+
+typedef enum
+{
+	midgard_condition_write0 = 0,
+	midgard_condition_false = 1,
+	midgard_condition_true = 2,
+	midgard_condition_always = 3, /* Special for writeout/uncond discard */
+} midgard_condition;
+
+typedef struct
+__attribute__((__packed__))
+{
+	midgard_jmp_writeout_op op : 3; /* == branch_uncond */
+	unsigned dest_tag : 4; /* tag of branch destination */
+	unsigned unknown : 2;
+	int offset : 7;
+} midgard_branch_uncond;
+
+typedef struct
+__attribute__((__packed__))
+{
+	midgard_jmp_writeout_op op : 3; /* == branch_cond */
+	unsigned dest_tag : 4; /* tag of branch destination */
+	int offset : 7;
+	midgard_condition cond : 2;
+} midgard_branch_cond;
+
+typedef struct
+__attribute__((__packed__))
+{
+	midgard_jmp_writeout_op op : 3; /* == branch_cond */
+	unsigned dest_tag : 4; /* tag of branch destination */
+	unsigned unknown : 2;
+	signed offset : 7;
+	unsigned zero : 16;
+	unsigned cond : 16;
+} midgard_branch_extended;
+
+typedef struct
+__attribute__((__packed__))
+{
+	midgard_jmp_writeout_op op : 3; /* == writeout */
+	unsigned unknown : 13;
+} midgard_writeout;
+
+/*
+ * Load/store words
+ */
+
+typedef enum
+{
+	midgard_op_ld_st_noop   = 0x03,
+	midgard_op_load_attr_16 = 0x95,
+	midgard_op_load_attr_32 = 0x94,
+	midgard_op_load_vary_16 = 0x99,
+	midgard_op_load_vary_32 = 0x98,
+	midgard_op_load_color_buffer_16 = 0x9D,
+	midgard_op_load_color_buffer_8 = 0xBA,
+	midgard_op_load_uniform_16 = 0xAC,
+	midgard_op_load_uniform_32 = 0xB0,
+	midgard_op_store_vary_16 = 0xD5,
+	midgard_op_store_vary_32 = 0xD4
+} midgard_load_store_op;
+
+typedef enum
+{
+	midgard_interp_centroid = 1,
+	midgard_interp_default = 2
+} midgard_interpolation;
+
+typedef struct
+__attribute__((__packed__))
+{
+	unsigned zero1 : 4; /* Always zero */
+
+	/* Varying qualifiers, zero if not a varying */
+	unsigned flat    : 1;
+	unsigned is_varying : 1; /* Always one for varying, but maybe something else? */
+	midgard_interpolation interpolation : 2;
+
+	unsigned zero2 : 2; /* Always zero */
+} midgard_varying_parameter;
+
+typedef struct
+__attribute__((__packed__))
+{
+	midgard_load_store_op op : 8;
+	unsigned reg     : 5;
+	unsigned mask    : 4;
+	unsigned swizzle : 8;
+	unsigned unknown : 16;
+
+	unsigned varying_parameters : 10;
+
+	unsigned address : 9;
+} midgard_load_store_word;
+
+typedef struct
+__attribute__((__packed__))
+{
+	unsigned type      : 4;
+	unsigned next_type : 4;
+	uint64_t word1     : 60;
+	uint64_t word2     : 60;
+} midgard_load_store;
+
+/* Texture pipeline results are in r28-r29 */
+#define REG_TEX_BASE 28
+
+/* Texture opcodes... maybe? */
+#define TEXTURE_OP_NORMAL 0x11
+#define TEXTURE_OP_TEXEL_FETCH 0x14
+
+/* Texture format types, found in format */
+#define TEXTURE_CUBE 0x00
+#define TEXTURE_2D 0x02
+#define TEXTURE_3D 0x03
+
+typedef struct
+__attribute__((__packed__))
+{
+	unsigned type      : 4;
+	unsigned next_type : 4;
+
+	unsigned op  : 6;
+	unsigned shadow    : 1;
+	unsigned unknown3  : 1;
+
+	/* A little obscure, but last is set for the last texture operation in
+	 * a shader. cont appears to just be last's opposite (?). Yeah, I know,
+	 * kind of funky.. BiOpen thinks it could do with memory hinting, or
+	 * tile locking? */
+
+	unsigned cont  : 1;
+	unsigned last  : 1;
+
+	unsigned format    : 5;
+	unsigned has_offset : 1;
+
+	/* Like in Bifrost */
+	unsigned filter  : 1;
+
+	unsigned in_reg_select : 1;
+	unsigned in_reg_upper  : 1;
+
+	unsigned in_reg_swizzle_left : 2;
+	unsigned in_reg_swizzle_right : 2;
+
+	unsigned unknown1 : 2;
+
+	unsigned unknown8  : 4;
+
+	unsigned out_full  : 1;
+
+	/* Always 1 afaict... */
+	unsigned unknown7  : 2;
+
+	unsigned out_reg_select : 1;
+	unsigned out_upper : 1;
+
+	unsigned mask : 4;
+
+	unsigned unknown2  : 2;
+
+	unsigned swizzle  : 8;
+	unsigned unknown4  : 8;
+
+	unsigned unknownA  : 4;
+
+	unsigned offset_unknown1  : 1;
+	unsigned offset_reg_select : 1;
+	unsigned offset_reg_upper : 1;
+	unsigned offset_unknown4  : 1;
+	unsigned offset_unknown5  : 1;
+	unsigned offset_unknown6  : 1;
+	unsigned offset_unknown7  : 1;
+	unsigned offset_unknown8  : 1;
+	unsigned offset_unknown9  : 1;
+
+	unsigned unknownB  : 3;
+
+	/* Texture bias or LOD, depending on whether it is executed in a
+	 * fragment/vertex shader respectively. Compute as int(2^8 * biasf).
+	 *
+	 * For texel fetch, this is the LOD as is. */
+	unsigned bias  : 8;
+
+	unsigned unknown9  : 8;
+
+	unsigned texture_handle : 16;
+	unsigned sampler_handle : 16;
+} midgard_texture_word;
+
+/* Opcode name table */
+
+static char* alu_opcode_names[256] = {
+	[midgard_alu_op_fadd]       = "fadd",
+	[midgard_alu_op_fmul]       = "fmul",
+	[midgard_alu_op_fmin]       = "fmin",
+	[midgard_alu_op_fmax]       = "fmax",
+	[midgard_alu_op_fmov]       = "fmov",
+	[midgard_alu_op_ffloor]     = "ffloor",
+	[midgard_alu_op_fceil]      = "fceil",
+	[midgard_alu_op_fdot3]      = "fdot3",
+	[midgard_alu_op_fdot3r]     = "fdot3r",
+	[midgard_alu_op_fdot4]      = "fdot4",
+	[midgard_alu_op_freduce]    = "freduce",
+	[midgard_alu_op_imin]       = "imin",
+	[midgard_alu_op_imax]       = "imax",
+	[midgard_alu_op_ishl]       = "ishl",
+	[midgard_alu_op_iasr]       = "iasr",
+	[midgard_alu_op_ilsr]       = "ilsr",
+	[midgard_alu_op_iadd]       = "iadd",
+	[midgard_alu_op_ishladd]    = "ishladd",
+	[midgard_alu_op_isub]       = "isub",
+	[midgard_alu_op_imul]       = "imul",
+	[midgard_alu_op_imov]       = "imov",
+	[midgard_alu_op_iand]       = "iand",
+	[midgard_alu_op_ior]        = "ior",
+	[midgard_alu_op_inot]       = "inot",
+	[midgard_alu_op_iandnot]    = "iandnot",
+	[midgard_alu_op_ixor]       = "ixor",
+	[midgard_alu_op_feq]        = "feq",
+	[midgard_alu_op_fne]        = "fne",
+	[midgard_alu_op_flt]        = "flt",
+	[midgard_alu_op_fle]        = "fle",
+	[midgard_alu_op_fball_eq]   = "fball_eq",
+	[midgard_alu_op_fbany_neq]  = "fbany_neq",
+	[midgard_alu_op_bball_eq]   = "bball_eq",
+	[midgard_alu_op_bbany_neq]  = "bbany_neq",
+	[midgard_alu_op_f2i]        = "f2i",
+	[midgard_alu_op_f2u]        = "f2u",
+	[midgard_alu_op_f2u8]       = "f2u8",
+	[midgard_alu_op_ieq]        = "ieq",
+	[midgard_alu_op_ine]        = "ine",
+	[midgard_alu_op_ilt]        = "ilt",
+	[midgard_alu_op_ile]        = "ile",
+	[midgard_alu_op_iball_eq]   = "iball_eq",
+	[midgard_alu_op_ball]       = "ball",
+	[midgard_alu_op_ibany_neq]  = "ibany_neq",
+	[midgard_alu_op_i2f]        = "i2f",
+	[midgard_alu_op_u2f]        = "u2f",
+	[midgard_alu_op_icsel]      = "icsel",
+	[midgard_alu_op_fcsel]      = "fcsel",
+	[midgard_alu_op_fatan_pt2]  = "fatan_pt2",
+	[midgard_alu_op_frcp]       = "frcp",
+	[midgard_alu_op_frsqrt]     = "frsqrt",
+	[midgard_alu_op_fsqrt]      = "fsqrt",
+	[midgard_alu_op_fexp2]      = "fexp2",
+	[midgard_alu_op_flog2]      = "flog2",
+	[midgard_alu_op_fsin]       = "fsin",
+	[midgard_alu_op_fcos]       = "fcos",
+	[midgard_alu_op_fatan2_pt1] = "fatan2_pt1"
+};
+
+static char* load_store_opcode_names[256] = {
+	[midgard_op_load_attr_16] = "ld_attr_16",
+	[midgard_op_load_attr_32] = "ld_attr_32",
+	[midgard_op_load_vary_16] = "ld_vary_16",
+	[midgard_op_load_vary_32] = "ld_vary_32",
+	[midgard_op_load_uniform_16] = "ld_uniform_16",
+	[midgard_op_load_uniform_32] = "ld_uniform_32",
+	[midgard_op_load_color_buffer_8] = "ld_color_buffer_8",
+	[midgard_op_load_color_buffer_16] = "ld_color_buffer_16",
+	[midgard_op_store_vary_16] = "st_vary_16",
+	[midgard_op_store_vary_32] = "st_vary_32"
+};
+
+#endif
diff --git a/src/gallium/drivers/panfrost/midgard/midgard_compile.c b/src/gallium/drivers/panfrost/midgard/midgard_compile.c
new file mode 100644
index 0000000..2bf2e30
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/midgard_compile.c
@@ -0,0 +1,3296 @@
+/*
+ * Copyright (C) 2018 Alyssa Rosenzweig <alyssa@rosenzweig.io>
+ *
+ * Copyright (C) 2014 Rob Clark <robclark@freedesktop.org>
+ * Copyright (c) 2014 Scott Mansell
+ * Copyright © 2014 Broadcom
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <sys/mman.h>
+#include <fcntl.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <err.h>
+
+#include "compiler/glsl/glsl_to_nir.h"
+#include "compiler/nir_types.h"
+#include "main/imports.h"
+#include "compiler/nir/nir_builder.h"
+#include "util/half_float.h"
+#include "util/register_allocate.h"
+#include "util/u_dynarray.h"
+#include "main/mtypes.h"
+
+#include "midgard.h"
+#include "midgard_nir.h"
+#include "midgard_compile.h"
+#include "helpers.h"
+
+#include "disassemble.h"
+
+#define IN_ARRAY(n, arr) ((uintptr_t) (n) < (((uintptr_t) ((arr)->data)) + ((arr)->size)))
+
+#define NIR_DEBUG
+//#define NIR_DEBUG_FINE
+//#define MIR_DEBUG
+#define MDG_DEBUG
+
+/* Instruction arguments represented as block-local SSA indices, rather than
+ * registers. Negative values mean unused. */
+
+typedef struct {
+	int src0;
+	int src1;
+	int dest;
+
+	/* The output is -not- SSA -- it's a direct register from I/O -- and
+	 * must not be culled/renamed */
+	bool literal_out;
+
+	/* src1 is -not- SSA but instead a 16-bit inline constant to be smudged
+	 * in. Only valid for ALU ops. */
+	bool inline_constant;
+} ssa_args;
+
+/* Forward declare so midgard_branch can reference */
+struct midgard_block;
+
+typedef struct midgard_branch {
+	/* If conditional, the condition is specified in r31.w */
+	bool conditional;
+	
+	/* For conditionals, if this is true, we branch on FALSE. If false, we  branch on TRUE. */
+	bool invert_conditional;
+
+	/* We can either branch to the start or the end of the block */
+	int target_start;
+	int target_after;
+} midgard_branch;
+
+/* Generic in-memory data type repesenting a single logical instruction, rather
+ * than a single instruction group. This is the preferred form for code gen.
+ * Multiple midgard_insturctions will later be combined during scheduling,
+ * though this is not represented in this structure.  Its format bridges
+ * the low-level binary representation with the higher level semantic meaning.
+ *
+ * Notably, it allows registers to be specified as block local SSA, for code
+ * emitted before the register allocation pass.
+ */
+
+typedef struct midgard_instruction {
+	unsigned type; /* ALU, load/store, texture */
+
+	/* If the register allocator has not run yet... */
+	bool uses_ssa;
+	ssa_args ssa_args;
+
+	/* Special fields for an ALU instruction */
+	bool vector; 
+	midgard_reg_info registers;
+
+	/* I.e. (1 << alu_bit) */
+	int unit;
+
+	bool has_constants;
+	float constants[4];
+	bool has_blend_constant;
+
+	bool compact_branch;
+	bool writeout;
+	bool prepacked_branch;
+
+	/* dynarray's are O(n) to delete from, which makes peephole
+	 * optimisations a little awkward. Instead, just have an unused flag
+	 * which the code gen will skip over */
+
+	bool unused;
+
+	union {
+		midgard_load_store_word load_store;
+		midgard_vector_alu alu;
+		midgard_texture_word texture;
+		uint16_t br_compact;
+
+		/* General branch, rather than packed br_compact. Higher level
+		 * than the other components */
+		midgard_branch branch;
+	};
+} midgard_instruction;
+
+typedef struct midgard_block {
+	/* List of midgard_instructions emitted for the current block */
+	struct util_dynarray instructions;
+
+	bool is_scheduled;
+
+	/* List of midgard_bundles emitted (after the scheduler has run) */
+	struct util_dynarray bundles;
+
+	/* Number of quadwords _actually_ emitted, as determined after scheduling */
+	unsigned quadword_count;
+
+	struct midgard_block *next_fallthrough;
+} midgard_block;
+
+/* Pretty printer for internal Midgard IR */
+
+static void
+print_mir_source(int source)
+{
+	if (source >= SSA_FIXED_MINIMUM) {
+		/* Specific register */
+		int reg = SSA_REG_FROM_FIXED(source);
+
+		/* TODO: Moving threshold */
+		if (reg > 16 && reg < 24)
+			printf("u%d", 23 - reg);
+		else
+			printf("r%d", reg);
+	} else {
+		printf("%d", source);
+	}
+}
+
+static void
+print_mir_instruction(midgard_instruction *ins)
+{
+	if (ins->unused)
+		return;
+
+	printf("\t");
+
+	switch (ins->type) {
+		case TAG_ALU_4: {
+			midgard_alu_op op = ins->alu.op;
+			const char *name = alu_opcode_names[op];
+
+			if (ins->unit)
+				printf("%d.", ins->unit);
+			
+			printf("%s", name ? name : "??");
+			break;
+		}
+
+		case TAG_LOAD_STORE_4: {
+			midgard_load_store_op op = ins->load_store.op;
+			const char *name = load_store_opcode_names[op];
+			
+			assert(name);
+			printf("%s", name);
+			break;
+	        }
+
+		case TAG_TEXTURE_4: {
+			printf("texture");
+		       break;
+	        }
+
+		default:
+		    assert(0);
+	}
+
+	if (ins->uses_ssa) {
+		ssa_args *args = &ins->ssa_args;
+
+		printf(" %s%d, ", args->literal_out ? "r" : "", args->dest);
+
+		print_mir_source(args->src0);
+		printf(", ");
+
+		if (args->inline_constant)
+			printf("#%d", args->src1);
+		else
+			print_mir_source(args->src1);
+	} else {
+		printf(" nonssa");
+	}
+
+	if (ins->has_constants)
+		printf(" <%f, %f, %f, %f>", ins->constants[0], ins->constants[1], ins->constants[2], ins->constants[3]);
+
+	printf("\n");
+}
+
+static void
+print_mir_block(midgard_block *block)
+{
+	printf("{\n");
+
+	util_dynarray_foreach(&block->instructions, midgard_instruction, ins) {
+		print_mir_instruction(ins);
+	}
+
+	printf("}\n");
+}
+
+/* Helpers to generate midgard_instruction's using macro magic, since every
+ * driver seems to do it that way */
+
+#define EMIT(op, ...) util_dynarray_append((ctx->current_block), midgard_instruction, v_##op(__VA_ARGS__));
+
+#define M_LOAD_STORE(name, rname, uname) \
+	static midgard_instruction m_##name(unsigned ssa, unsigned address) { \
+		midgard_instruction i = { \
+			.type = TAG_LOAD_STORE_4, \
+			.uses_ssa = true, \
+			.ssa_args = { \
+				.rname = ssa, \
+				.uname = -1, \
+				.src1 = -1 \
+			}, \
+			.unused = false, \
+			.load_store = { \
+				.op = midgard_op_##name, \
+				.mask = 0xF, \
+				.swizzle = SWIZZLE(COMPONENT_X, COMPONENT_Y, COMPONENT_Z, COMPONENT_W), \
+				.address = address \
+			} \
+		}; \
+		\
+		return i; \
+	}
+
+#define M_LOAD(name) M_LOAD_STORE(name, dest, src0)
+#define M_STORE(name) M_LOAD_STORE(name, src0, dest)
+
+const midgard_vector_alu_src blank_alu_src = {
+	.swizzle = SWIZZLE(COMPONENT_X, COMPONENT_Y, COMPONENT_Z, COMPONENT_W),
+};
+
+const midgard_scalar_alu_src blank_scalar_alu_src = {
+	.full = true
+};
+
+/* Used for encoding the unused source of 1-op instructions */
+const midgard_vector_alu_src zero_alu_src = { 0 };
+
+/* Coerce structs to integer */
+
+static unsigned
+vector_alu_srco_unsigned(midgard_vector_alu_src src)
+{
+	unsigned u;
+	memcpy(&u, &src, sizeof(src));
+	return u;
+}
+
+/* Inputs a NIR ALU source, with modifiers attached if necessary, and outputs
+ * the corresponding Midgard source */
+
+static midgard_vector_alu_src
+vector_alu_modifiers(nir_alu_src *src)
+{
+	if (!src) return blank_alu_src;
+
+	midgard_vector_alu_src alu_src = {
+		.abs = src->abs,
+		.negate = src->negate,
+		.rep_low = 0,
+		.rep_high = 0,
+		.half = 0, /* TODO */
+		.swizzle = SWIZZLE_FROM_ARRAY(src->swizzle)
+	};
+
+	return alu_src;
+}
+
+static midgard_instruction
+m_alu_vector(midgard_alu_op op, int unit, unsigned src0, midgard_vector_alu_src mod1, unsigned src1, midgard_vector_alu_src mod2, unsigned dest, bool literal_out, midgard_outmod outmod)
+{
+	midgard_instruction ins = {
+		.type = TAG_ALU_4,
+		.unused = false,
+		.uses_ssa = true,
+		.ssa_args = {
+			.src0 = src0,
+			.src1 = src1,
+			.dest = dest,
+			.literal_out = literal_out
+		},
+		.vector = true,
+		.alu = {
+			.op = op,
+			.reg_mode = midgard_reg_mode_full,
+			.dest_override = midgard_dest_override_none,
+			.outmod = outmod,
+			.mask = 0xFF,
+			.src1 = vector_alu_srco_unsigned(mod1),
+			.src2 = vector_alu_srco_unsigned(mod2)
+		},
+	};
+
+	return ins;
+}
+
+#define M_ALU_VECTOR_1(unit, name) \
+	static midgard_instruction v_##name(unsigned src, midgard_vector_alu_src mod1, unsigned dest, bool literal, midgard_outmod outmod) { \
+		return m_alu_vector(midgard_alu_op_##name, ALU_ENAB_VEC_##unit, SSA_UNUSED_1, zero_alu_src, src, mod1, dest, literal, outmod); \
+	}
+
+/* load/store instructions have both 32-bit and 16-bit variants, depending on
+ * whether we are using vectors composed of highp or mediump. At the moment, we
+ * don't support half-floats -- this requires changes in other parts of the
+ * compiler -- therefore the 16-bit versions are commented out. */
+
+//M_LOAD(load_attr_16);
+M_LOAD(load_attr_32);
+//M_LOAD(load_vary_16);
+M_LOAD(load_vary_32);
+//M_LOAD(load_uniform_16);
+M_LOAD(load_uniform_32);
+M_LOAD(load_color_buffer_8);
+//M_STORE(store_vary_16);
+M_STORE(store_vary_32);
+
+/* Used as a sort of intrinsic outside of the ALU code */
+M_ALU_VECTOR_1(MUL, fmov);
+
+static midgard_instruction
+v_alu_br_compact_cond(midgard_jmp_writeout_op op, unsigned tag, signed offset, unsigned cond)
+{
+	midgard_branch_cond branch = {
+		.op = op,
+		.dest_tag = tag,
+		.offset = offset,
+		.cond = cond
+	};
+
+	uint16_t compact;
+	memcpy(&compact, &branch, sizeof(branch));
+
+	midgard_instruction ins = {
+		.type = TAG_ALU_4,
+		.unit = ALU_ENAB_BR_COMPACT ,
+		.unused = false,
+		.uses_ssa = false,
+
+		.prepacked_branch = true,
+		.compact_branch = true, 
+		.br_compact = compact
+	};
+
+	if (op == midgard_jmp_writeout_op_writeout)
+		ins.writeout = true;
+
+	return ins;
+}
+
+static midgard_instruction
+v_branch(bool conditional, bool invert)
+{
+	midgard_instruction ins = {
+		.type = TAG_ALU_4,
+		.unit = ALU_ENAB_BR_COMPACT,
+		.compact_branch = true,
+		.branch = {
+			.conditional = conditional,
+			.invert_conditional = invert
+		}
+	};
+
+	return ins;
+}
+
+typedef struct midgard_bundle {
+	/* Tag for the overall bundle */
+	int tag;
+
+	/* Instructions contained by the bundle */
+	int instruction_count;
+	midgard_instruction instructions[5];
+
+	/* Bundle-wide ALU configuration */
+	int padding;
+	int control;
+	bool has_embedded_constants;
+	float constants[4];
+	bool has_blend_constant;
+
+	uint16_t register_words[8];
+	int register_words_count;
+
+	uint64_t body_words[8];
+	size_t body_size[8];
+	int body_words_count;
+} midgard_bundle;
+
+typedef struct compiler_context {
+	nir_shader *nir;
+	gl_shader_stage stage;
+
+	/* Is internally a blend shader? Depends on stage == FRAGMENT */
+	bool is_blend;
+
+	/* Tracking for blend constant patching */
+	int blend_constant_number;
+	int blend_constant_offset;
+
+	/* Current NIR function */
+	nir_function *func;
+
+	/* Unordered array of midgard_block */
+	int block_count;
+	struct util_dynarray blocks;
+
+	midgard_block *initial_block;
+	midgard_block *previous_source_block;
+	midgard_block *final_block;
+
+	/* List of midgard_instructions emitted for the current block */
+	struct util_dynarray *current_block;
+
+	/* Constants which have been loaded, for later inlining */
+	struct hash_table_u64 *ssa_constants;
+
+	/* SSA values / registers which have been aliased. Naively, these
+	 * demand a fmov output; instead, we alias them in a later pass to
+	 * avoid the wasted op.
+	 *
+	 * A note on encoding: to avoid dynamic memory management here, rather
+	 * than ampping to a pointer, we map to the source index; the key
+	 * itself is just the destination index. */
+
+	struct hash_table_u64 *ssa_to_alias;
+	struct set *leftover_ssa_to_alias;
+	
+	/* Actual SSA-to-register for RA */
+	struct hash_table_u64 *ssa_to_register;
+
+	/* Mapping of hashes computed from NIR indices to the sequential temp indices ultimately used in MIR */
+	struct hash_table_u64 *hash_to_temp;
+	int temp_count;
+	int max_hash;
+
+	/* Uniform IDs for mdg */
+	struct hash_table_u64 *uniform_nir_to_mdg;
+	int uniform_count;
+
+	struct hash_table_u64 *varying_nir_to_mdg;
+	int varying_count;
+
+	/* Just the count of the max register used. Higher count => higher
+	 * register pressure */
+	int work_registers;
+
+	/* Used for cont/last hinting. Increase when a tex op is added.
+	 * Decrease when a tex op is removed. */
+	int texture_op_count;
+
+	/* Mapping of texture register -> SSA index for unaliasing */
+	int texture_index[2];
+
+	/* Count of special uniforms (viewport, etc) in vec4 units */
+	int special_uniforms;
+
+	/* If any path hits a discard instruction */
+	bool can_discard;
+
+	/* The number of uniforms allowable for the fast path */
+	int uniform_cutoff;
+
+	/* Count of instructions emitted from NIR overall, across all blocks */
+	int instruction_count;
+} compiler_context;
+
+static void
+attach_constants(compiler_context *ctx, midgard_instruction *ins, void *constants, int name)
+{
+	ins->has_constants = true;
+	memcpy(&ins->constants, constants, 16);
+
+	/* If this is the special blend constant, mark this instruction */
+
+	if (ctx->is_blend && ctx->blend_constant_number == name)
+		ins->has_blend_constant = true;
+}
+
+static int
+glsl_type_size(const struct glsl_type *type)
+{
+	return glsl_count_attribute_slots(type, false);
+}
+
+static void
+optimise_nir(nir_shader *nir)
+{
+	bool progress;
+
+	NIR_PASS(progress, nir, nir_lower_regs_to_ssa);
+
+	do {
+		progress = false;
+
+		NIR_PASS(progress, nir, midgard_nir_lower_algebraic);
+		NIR_PASS(progress, nir, nir_lower_io, nir_var_all, glsl_type_size, 0);
+		NIR_PASS(progress, nir, nir_lower_var_copies);
+		NIR_PASS(progress, nir, nir_lower_vars_to_ssa);
+
+		NIR_PASS(progress, nir, nir_copy_prop);
+		NIR_PASS(progress, nir, nir_opt_dce);
+		NIR_PASS(progress, nir, nir_opt_dead_cf);
+		NIR_PASS(progress, nir, nir_opt_cse);
+		NIR_PASS(progress, nir, nir_opt_peephole_select, 64);
+		NIR_PASS(progress, nir, nir_opt_algebraic);
+		NIR_PASS(progress, nir, nir_opt_constant_folding);
+		NIR_PASS(progress, nir, nir_opt_undef);
+		NIR_PASS(progress, nir, nir_opt_loop_unroll, 
+				nir_var_shader_in |
+				nir_var_shader_out |
+				nir_var_local);
+	} while(progress);
+
+	/* Must be run at the end to prevent creation of fsin/fcos ops */
+	NIR_PASS(progress, nir, midgard_nir_scale_trig);
+
+	do {
+		progress = false;
+
+		NIR_PASS(progress, nir, nir_opt_dce);
+		NIR_PASS(progress, nir, nir_opt_algebraic);
+		NIR_PASS(progress, nir, nir_opt_constant_folding);
+		NIR_PASS(progress, nir, nir_copy_prop);
+	} while(progress);
+
+	/* Lower mods */
+	NIR_PASS(progress, nir, nir_lower_to_source_mods);
+	NIR_PASS(progress, nir, nir_copy_prop);
+	NIR_PASS(progress, nir, nir_opt_dce);
+
+	/* Take us out of SSA */
+	NIR_PASS(progress, nir, nir_lower_locals_to_regs);
+	NIR_PASS(progress, nir, nir_convert_from_ssa, true);
+
+	/* We are a vector architecture; write combine where possible */
+	NIR_PASS(progress, nir, nir_move_vec_src_uses_to_dest);
+	NIR_PASS(progress, nir, nir_lower_vec_to_movs);
+
+	NIR_PASS(progress, nir, nir_opt_dce);
+}
+
+/* Front-half of aliasing the SSA slots, merely by inserting the flag in the
+ * appropriate hash table. Intentional off-by-one to avoid confusing NULL with
+ * r0. See the comments in compiler_context */
+
+static void
+alias_ssa(compiler_context *ctx, int dest, int src)
+{
+	_mesa_hash_table_u64_insert(ctx->ssa_to_alias, dest + 1, (void *) ((uintptr_t) src + 1));
+	_mesa_set_add(ctx->leftover_ssa_to_alias, (void *) (uintptr_t) (dest + 1));
+}
+
+/* ...or undo it, after which the original index will be used (dummy move should be emitted alongside this) */
+
+static void
+unalias_ssa(compiler_context *ctx, int dest)
+{
+	_mesa_hash_table_u64_remove(ctx->ssa_to_alias, dest + 1);
+	/* TODO: Remove from leftover or no? */
+}
+
+static void
+midgard_pin_output(compiler_context *ctx, int index, int reg)
+{
+	_mesa_hash_table_u64_insert(ctx->ssa_to_register, index + 1, (void *) ((uintptr_t) reg + 1));
+}
+
+static bool
+midgard_is_pinned(compiler_context *ctx, int index)
+{
+	return _mesa_hash_table_u64_search(ctx->ssa_to_register, index + 1) != NULL;
+}
+
+/* Do not actually emit a load; instead, cache the constant for inlining */
+
+static void
+emit_load_const(compiler_context *ctx, nir_load_const_instr *instr)
+{
+	nir_ssa_def def = instr->def;
+
+	float *v = ralloc_array(NULL, float, 4);
+	memcpy(v, &instr->value.f32, 4 * sizeof(float));
+	_mesa_hash_table_u64_insert(ctx->ssa_constants, def.index + 1, v);
+}
+
+/* Duplicate bits to convert sane 4-bit writemask to obscure 8-bit format (or
+ * do the inverse) */
+
+static unsigned
+expand_writemask(unsigned mask)
+{
+	unsigned o = 0;
+
+	for (int i = 0; i < 4; ++i)
+		if (mask & (1 << i))
+		       o |= (3 << (2*i));
+	
+	return o;
+}
+
+static unsigned
+squeeze_writemask(unsigned mask)
+{
+	unsigned o = 0;
+
+	for (int i = 0; i < 4; ++i)
+		if (mask & (3 << (2*i)))
+		       o |= (1 << i);
+	
+	return o;
+
+}
+
+/* Generate write mask when there are a specific number of components, e.g.
+ * xyz -> 3 -> 0x7 */
+
+static unsigned
+writemask_for_nr_components(int nr_components)
+{
+	unsigned mask = 0;
+	
+	while(nr_components--)
+		mask = (mask << 1) | 1;
+
+	return mask;
+}
+
+static unsigned
+find_or_allocate_temp(compiler_context *ctx, unsigned hash)
+{
+	if ((hash < 0) || (hash >= SSA_FIXED_MINIMUM))
+		return hash;
+
+	unsigned temp = (uintptr_t) _mesa_hash_table_u64_search(ctx->hash_to_temp, hash + 1);
+
+	if (temp)
+		return temp - 1;
+
+	/* If no temp is find, allocate one */
+	temp = ctx->temp_count++;
+	ctx->max_hash = MAX2(ctx->max_hash, hash);
+
+	_mesa_hash_table_u64_insert(ctx->hash_to_temp, hash + 1, (void *) ((uintptr_t) temp + 1));
+
+	return temp;
+}
+
+static unsigned
+nir_src_index(nir_src *src)
+{
+	if (src->is_ssa)
+		return src->ssa->index;
+	else
+		return 4096 + src->reg.reg->index;
+}
+
+static unsigned
+nir_dest_index(nir_dest *dst)
+{
+	if (dst->is_ssa)
+		return dst->ssa.index;
+	else
+		return 4096 + dst->reg.reg->index;
+}
+
+static unsigned
+nir_alu_src_index(nir_alu_src *src)
+{
+	return nir_src_index(&src->src);
+}
+
+/* Midgard puts conditionals in r31.w; move an arbitrary source (the output of
+ * a conditional test) into that register */
+
+static void
+emit_condition(compiler_context *ctx, nir_src *src, bool for_branch)
+{
+	/* XXX: Force component correct */
+	int condition = nir_src_index(src);
+
+	const midgard_vector_alu_src alu_src = {
+		.swizzle = SWIZZLE(COMPONENT_X, COMPONENT_X, COMPONENT_X, COMPONENT_X),
+	};
+
+	/* There is no boolean move instruction. Instead, we simulate a move by
+	 * ANDing the condition with itself to get it into r31.w */
+
+	midgard_instruction ins = {
+		.type = TAG_ALU_4,
+		.unit = for_branch ? UNIT_SMUL : UNIT_SADD, /* TODO: DEDUCE THIS */
+		.uses_ssa = true,
+		.ssa_args = {
+			.src0 = condition,
+			.src1 = condition,
+			.dest = 31,
+			.literal_out = true
+		},
+		.vector = true,
+		.alu = {
+			.op = midgard_alu_op_iand,
+			.reg_mode = midgard_reg_mode_full,
+			.dest_override = midgard_dest_override_none,
+			.outmod = midgard_outmod_none,
+			.mask = (0x3 << 6), /* w */
+			.src1 = vector_alu_srco_unsigned(alu_src),
+			.src2 = vector_alu_srco_unsigned(alu_src)
+		},
+	};
+
+	util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+}
+
+/* Insert a dummy instruction (unused set) to make space for later movement */
+
+static void
+midgard_insert_dummy(compiler_context *ctx)
+{
+	midgard_instruction dummy = { .unused = true, .type = TAG_ALU_4 };
+	util_dynarray_append(ctx->current_block, midgard_instruction, dummy);
+}
+
+
+/* Components: Number/style of arguments:
+ * 	3: One-argument op with r24 (i2f, f2i)
+ * 	2: Standard two argument op (fadd, fmul)
+ * 	1: Flipped one-argument op (fmov, imov)
+ * 	0: Standard one-argument op (frcp)
+ * NIR: NIR instruction op.
+ * Op: Midgard instruction op.
+ */
+
+#define ALU_CASE(_components, nir, _op) \
+	case nir_op_##nir: \
+		components = _components; \
+		op = midgard_alu_op_##_op; \
+		break;
+
+static void
+emit_alu(compiler_context *ctx, nir_alu_instr *instr)
+{
+	bool is_ssa = instr->dest.dest.is_ssa;
+
+	unsigned dest = nir_dest_index(&instr->dest.dest);
+	unsigned nr_components = is_ssa ? instr->dest.dest.ssa.num_components : instr->dest.dest.reg.reg->num_components;
+
+	/* ALU ops are unified in NIR between scalar/vector, but partially
+	 * split in Midgard. Reconcile that here, to avoid diverging code paths
+	 */
+	bool is_vector = nr_components != 1;
+
+	/* Most Midgard ALU ops have a 1:1 correspondance to NIR ops; these are
+	 * supported. A few do not and are commented for now. Also, there are a
+	 * number of NIR ops which Midgard does not support and need to be
+	 * lowered, also TODO. This switch block emits the opcode and calling
+	 * convention of the Midgard instruction; actual packing is done in
+	 * emit_alu below */
+
+	unsigned op, components;
+
+	switch(instr->op) {
+		ALU_CASE(2, fadd, fadd);
+		ALU_CASE(2, fmul, fmul);
+		ALU_CASE(2, fmin, fmin);
+		ALU_CASE(2, fmax, fmax);
+		ALU_CASE(2, imin, imin);
+		ALU_CASE(2, imax, imax);
+		ALU_CASE(1, fmov, fmov);
+		ALU_CASE(0, ffloor, ffloor);
+		ALU_CASE(0, fceil, fceil);
+		ALU_CASE(2, fdot3, fdot3);
+		//ALU_CASE(2, fdot3r);
+		ALU_CASE(2, fdot4, fdot4);
+		//ALU_CASE(2, freduce);
+		ALU_CASE(2, iadd, iadd);
+		ALU_CASE(2, isub, isub);
+		ALU_CASE(2, imul, imul);
+
+		/* XXX: Use fmov, not imov, since imov was causing major
+		 * issues with texture precision? XXX research */
+		ALU_CASE(1, imov, fmov);
+
+		ALU_CASE(2, feq, feq);
+		ALU_CASE(2, fne, fne);
+		ALU_CASE(2, flt, flt);
+		ALU_CASE(2, ieq, ieq);
+		ALU_CASE(2, ine, ine);
+		ALU_CASE(2, ilt, ilt);
+		//ALU_CASE(2, icsel, icsel);
+		ALU_CASE(0, frcp, frcp);
+		ALU_CASE(0, frsq, frsqrt);
+		ALU_CASE(0, fsqrt, fsqrt);
+		ALU_CASE(0, fexp2, fexp2);
+		ALU_CASE(0, flog2, flog2);
+
+		ALU_CASE(3, f2i32, f2i);
+		ALU_CASE(3, f2u32, f2u);
+		ALU_CASE(3, i2f32, i2f);
+		ALU_CASE(3, u2f32, u2f);
+
+		ALU_CASE(0, fsin, fsin);
+		ALU_CASE(0, fcos, fcos);
+
+		ALU_CASE(2, iand, iand);
+		ALU_CASE(2, ior, ior);
+		ALU_CASE(2, ixor, ixor);
+		ALU_CASE(0, inot, inot);
+		ALU_CASE(2, ishl, ishl);
+		ALU_CASE(2, ishr, iasr);
+		ALU_CASE(2, ushr, ilsr);
+		//ALU_CASE(2, ilsr, ilsr);
+
+		ALU_CASE(2, ball_fequal4, fball_eq);
+		ALU_CASE(2, bany_fnequal4, fbany_neq);
+		ALU_CASE(2, ball_iequal4, iball_eq);
+		ALU_CASE(2, bany_inequal4, ibany_neq);
+
+		/* For greater-or-equal, we use less-or-equal and flip the
+		 * arguments */
+
+		case nir_op_ige: {
+			components = 2;
+			op = midgard_alu_op_ile;
+
+			/* Swap via temporary */
+			nir_alu_src temp = instr->src[1];
+			instr->src[1] = instr->src[0];
+			instr->src[0] = temp;
+
+			break;
+		 }
+
+		case nir_op_bcsel: {
+			components = 2;
+			op = midgard_alu_op_fcsel;
+
+			emit_condition(ctx, &instr->src[0].src, false);
+
+			/* The condition is the first argument; move the other
+			 * arguments up one to be a binary instruction for
+			 * Midgard */
+
+			memmove(instr->src, instr->src + 1, 2 * sizeof(nir_alu_src));
+			break;
+		}
+
+		case 0: {
+			printf("0 ALU op?\n");
+			return;
+		}
+
+		default:
+			printf("Unhandled ALU op %s\n", nir_op_infos[instr->op].name);
+			assert(0);
+			return;
+	}
+
+	int _unit = alu_opcode_unit[op];
+
+	/* slut doesn't exist; lower to vlut which acts as scalar
+	 * despite the name */
+
+	if (_unit == UNIT_VLUT)
+		is_vector = true;
+
+	/* Certain ops cannot run as scalars */
+	if (!(_unit & UNITS_SCALAR))
+		is_vector = true;
+
+	/* Initialise fields common between scalar/vector instructions */
+	midgard_outmod outmod = instr->dest.saturate ? midgard_outmod_sat : midgard_outmod_none;
+
+	/* src0 will always exist afaik, but src1 will not for 1-argument
+	 * instructions. The latter can only be fetched if the instruction
+	 * needs it, or else we may segfault. */
+
+	unsigned src0 = nir_alu_src_index(&instr->src[0]);
+	unsigned src1 = components == 2 ? nir_alu_src_index(&instr->src[1]) : SSA_UNUSED_0;
+
+	/* Rather than use the instruction generation helpers, we do it
+	 * ourselves here to avoid the mess */
+
+	midgard_instruction ins = {
+		.type = TAG_ALU_4,
+		.unused = false,
+		.uses_ssa = true,
+		.ssa_args = {
+			.src0 = components == 3 || components == 2 || components == 0 ? src0 : SSA_UNUSED_1,
+			.src1 = components == 2 ? src1 : components == 1 ? src0 : components == 0 ? SSA_UNUSED_0 : SSA_UNUSED_1,
+			.dest = dest,
+			.inline_constant = components == 0
+		},
+		.vector = is_vector
+	};
+
+	nir_alu_src *nirmod0 = NULL;
+	nir_alu_src *nirmod1 = NULL;
+
+	if (components == 2) {
+		nirmod0 = &instr->src[0];
+		nirmod1 = &instr->src[1];
+	} else if (components == 1) {
+		nirmod1 = &instr->src[0];
+	} else if (components == 0) {
+		nirmod0 = &instr->src[0];
+	}
+
+	midgard_vector_alu alu = {
+		.op = op,
+		.reg_mode = midgard_reg_mode_full,
+		.dest_override = midgard_dest_override_none,
+		.outmod = outmod,
+
+		/* Writemask only valid for non-SSA NIR */
+		.mask = expand_writemask(writemask_for_nr_components(nr_components)),
+
+		.src1 = vector_alu_srco_unsigned(vector_alu_modifiers(nirmod0)),
+		.src2 = vector_alu_srco_unsigned(vector_alu_modifiers(nirmod1)),
+	};
+
+	/* Apply writemask if non-SSA, keeping in mind that we can't write to components that don't exist */
+	
+	if (!is_ssa)
+		alu.mask &= expand_writemask(instr->dest.write_mask);
+
+	ins.alu = alu;
+
+	midgard_insert_dummy(ctx);
+
+	if (_unit == UNIT_VLUT) {
+		/* To avoid duplicating the LUTs (we think?), LUT instructions can only
+		 * operate as if they were scalars. Lower them here by changing the
+		 * component. */
+
+		assert(components == 0);
+
+		uint8_t original_swizzle[4];
+		memcpy(original_swizzle, nirmod0->swizzle, sizeof(nirmod0->swizzle));
+
+		for (int i = 0; i < nr_components; ++i) {
+			ins.alu.mask = (0x3) << (2 * i); /* Mask the associated component */
+
+			for (int j = 0; j < 4; ++j)
+				nirmod0->swizzle[j] = original_swizzle[i]; /* Pull from the correct component */
+
+			ins.alu.src1 = vector_alu_srco_unsigned(vector_alu_modifiers(nirmod0));
+			util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+		}
+	} else {
+		util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+	}
+}
+
+static void
+emit_intrinsic(compiler_context *ctx, nir_intrinsic_instr *instr)
+{
+        nir_const_value *const_offset;
+        unsigned offset, reg;
+
+	switch(instr->intrinsic) {
+		case nir_intrinsic_discard_if:
+			emit_condition(ctx, &instr->src[0], true);
+			/* fallthrough */
+
+		case nir_intrinsic_discard: {
+			midgard_condition cond = instr->intrinsic == nir_intrinsic_discard_if ? 
+				midgard_condition_true : midgard_condition_always;
+
+			EMIT(alu_br_compact_cond, midgard_jmp_writeout_op_discard, 0, 2, cond);
+			ctx->can_discard = true;
+			break;
+		}
+
+		case nir_intrinsic_load_uniform:
+		case nir_intrinsic_load_input:
+			const_offset = nir_src_as_const_value(instr->src[0]);
+			assert (const_offset && "no indirect inputs");
+
+			offset = nir_intrinsic_base(instr) + const_offset->u32[0];
+
+			reg = nir_dest_index(&instr->dest);
+
+			if (instr->intrinsic == nir_intrinsic_load_uniform && !ctx->is_blend) {
+				/* TODO: half-floats */
+
+				int uniform_offset = 0;
+
+				if (offset >= SPECIAL_UNIFORM_BASE) {
+					/* XXX: Resolve which uniform */
+					uniform_offset = 0;
+				} else {
+					/* Offset away from the special
+					 * uniform block */
+
+					void *entry = _mesa_hash_table_u64_search(ctx->uniform_nir_to_mdg, offset + 1);
+					assert(entry);
+
+					uniform_offset = (uintptr_t) (entry) - 1;
+					uniform_offset += ctx->special_uniforms;
+				}
+
+				if (uniform_offset < ctx->uniform_cutoff) {
+					/* Fast path: For the first 16 uniform,
+					 * accesses are 0-cycle, since they're
+					 * just a register fetch in the usual
+					 * case.  So, we alias the registers
+					 * while we're still in SSA-space */
+
+					int reg_slot = 23 - uniform_offset;
+					alias_ssa(ctx, reg, SSA_FIXED_REGISTER(reg_slot));
+				} else {
+					/* Otherwise, read from the 'special'
+					 * UBO to access higher-indexed
+					 * uniforms, at a performance cost */
+
+					midgard_instruction ins = m_load_uniform_32(reg, uniform_offset);
+
+					/* TODO: Don't split */
+					ins.load_store.varying_parameters = (uniform_offset & 7) << 7;
+					ins.load_store.address = uniform_offset >> 3;
+
+					ins.load_store.unknown = 0x1E00; /* xxx: what is this? */
+					util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+				}
+			} else if (ctx->stage == MESA_SHADER_FRAGMENT && !ctx->is_blend) {
+				/* XXX: Half-floats? */
+				/* TODO: swizzle, mask */
+
+				midgard_instruction ins = m_load_vary_32(reg, offset);
+
+				midgard_varying_parameter p = {
+					.is_varying = 1,
+					.interpolation = midgard_interp_default,
+					.flat = /*var->data.interpolation == INTERP_MODE_FLAT*/ 0
+				};
+
+				unsigned u;
+				memcpy(&u, &p, sizeof(p));
+				ins.load_store.varying_parameters = u;
+
+				ins.load_store.unknown = 0x1e9e; /* xxx: what is this? */
+				util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+			} else if (ctx->is_blend && instr->intrinsic == nir_intrinsic_load_uniform) {
+				/* Constant encoded as a pinned constant */	
+
+				midgard_instruction ins = v_fmov(SSA_FIXED_REGISTER(REGISTER_CONSTANT), blank_alu_src, reg, false, midgard_outmod_none);
+				ins.has_constants = true;
+				ins.has_blend_constant = true;
+				util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+			} else if (ctx->is_blend) {
+				/* For blend shaders, a load might be
+				 * translated various ways depending on what
+				 * we're loading. Figure out how this is used */
+
+				nir_variable *out = NULL;
+
+				nir_foreach_variable(var, &ctx->nir->inputs) {
+					int drvloc = var->data.driver_location;
+					if (nir_intrinsic_base(instr) == drvloc) {
+						out = var;
+						break;
+					}
+				}
+
+				assert(out);
+
+				if (out->data.location == VARYING_SLOT_COL0) {
+					/* Source color preloaded to r0 */
+
+					//alias_ssa(ctx, reg, SSA_FIXED_REGISTER(0));
+					midgard_pin_output(ctx, reg, 0);
+				} else if (out->data.location == VARYING_SLOT_COL1) {
+					/* Destination color must be read from framebuffer */
+
+					midgard_instruction ins = m_load_color_buffer_8(reg, 0);
+					ins.load_store.swizzle = 0; /* xxxx */
+
+					/* Read each component sequentially */
+
+					for (int c = 0; c < 4; ++c) {
+						ins.load_store.mask = (1 << c);
+						ins.load_store.unknown = c;
+						util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+						midgard_insert_dummy(ctx);
+					}
+
+					/* vadd.u2f hr2, abs(hr2), #0 */
+
+					midgard_vector_alu_src alu_src = blank_alu_src;
+					alu_src.abs = true;
+					alu_src.half = true;
+
+					midgard_instruction u2f = {
+						.type = TAG_ALU_4,
+						.vector = true,
+						.uses_ssa = true,
+						.ssa_args = {
+							.src0 = reg,
+							.dest = reg,
+							.inline_constant = true
+						},
+						.alu = {
+							.op = midgard_alu_op_u2f,
+							.reg_mode = midgard_reg_mode_half,
+							.dest_override = midgard_dest_override_none,
+							.outmod = midgard_outmod_none,
+							.mask = 0xF,
+							.src1 = vector_alu_srco_unsigned(alu_src),
+							.src2 = vector_alu_srco_unsigned(blank_alu_src),
+						}
+					};
+
+					util_dynarray_append(ctx->current_block, midgard_instruction, u2f);
+					midgard_insert_dummy(ctx);
+
+					/* vmul.fmul r1, hr2, #0.00392151 */
+
+					alu_src.abs = false;
+
+					midgard_instruction fmul = {
+						.type = TAG_ALU_4,
+						.vector = true,
+						.uses_ssa = true,
+						.ssa_args = {
+							.src0 = reg,
+							.dest = reg,
+							.src1 = _mesa_float_to_half(1.0 / 255.0),
+							.inline_constant = true
+						},
+						.alu = {
+							.op = midgard_alu_op_fmul,
+							.reg_mode = midgard_reg_mode_full,
+							.dest_override = midgard_dest_override_none,
+							.outmod = midgard_outmod_none,
+							.mask = 0xFF,
+							.src1 = vector_alu_srco_unsigned(alu_src),
+							.src2 = vector_alu_srco_unsigned(blank_alu_src),
+						}
+					};
+	
+					util_dynarray_append(ctx->current_block, midgard_instruction, fmul);
+					midgard_insert_dummy(ctx);
+				} else {
+					printf("Unknown input in blend shader\n");
+					assert(0);
+				}
+			} else if (ctx->stage == MESA_SHADER_VERTEX) {
+				midgard_instruction ins = m_load_attr_32(reg, offset);
+				ins.load_store.unknown = 0x1E1E; /* XXX: What is this? */
+				util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+			} else {
+				printf("Unknown load\n");
+				assert(0);
+			}
+
+			break;
+
+		case nir_intrinsic_store_output:
+			const_offset = nir_src_as_const_value(instr->src[1]);
+			assert(const_offset && "no indirect outputs");
+
+			offset = nir_intrinsic_base(instr) + const_offset->u32[0];
+
+			reg = nir_src_index(&instr->src[0]);
+
+			if (ctx->stage == MESA_SHADER_FRAGMENT) {
+				/* gl_FragColor is not emitted with load/store
+				 * instructions. Instead, it gets plonked into
+				 * r0 at the end of the shader and we do the
+				 * framebuffer writeout dance. TODO: Defer
+				 * writes */
+
+				//EMIT(fmov, reg, blank_alu_src, 0, true, midgard_outmod_none);
+				midgard_pin_output(ctx, reg, 0);
+			} else if (ctx->stage == MESA_SHADER_VERTEX) {
+				/* Varyings are written into one of two special
+				 * varying register, r26 or r27. The register itself is selected as the register 
+				 * in the st_vary instruction, minus the base of 26. E.g. write into r27 and then call st_vary(1)
+				 *
+				 * Normally emitting fmov's is frowned upon,
+				 * but due to unique constraints of
+				 * REGISTER_VARYING, fmov emission + a
+				 * dedicated cleanup pass is the only way to
+				 * guarantee correctness when considering some
+				 * (common) edge cases XXX: FIXME */
+
+				/* TODO: Integrate with special purpose RA (and scheduler?) */
+				bool high_varying_register = false;
+
+				EMIT(fmov, reg, blank_alu_src, REGISTER_VARYING_BASE + high_varying_register, true, midgard_outmod_none);
+
+				/* Look up how it was actually laid out */
+
+				printf("Trying %d\n", offset + 1);
+				void *entry = _mesa_hash_table_u64_search(ctx->varying_nir_to_mdg, offset + 1);
+
+				if (!entry) {
+					printf("Skipping varying\n");
+					break;
+				}
+
+				offset = (uintptr_t) (entry) - 1;
+
+				/* Compute offset: gl_Position is zero. The
+				 * first varying is two. The nth varying is 2+n
+				 * for zero-indexed n. Varying #1 is unused.
+				 * See the corresponding structures in the
+				 * command stream. */
+
+				if (offset > 0)
+					offset += 1;
+
+				midgard_instruction ins = m_store_vary_32(high_varying_register, offset);
+				ins.load_store.unknown = 0x1E9E; /* XXX: What is this? */
+				ins.uses_ssa = false;
+				util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+			} else {
+				printf("Unknown store\n");
+				assert(0);
+			}
+
+			break;
+
+		default:
+			printf ("Unhandled intrinsic\n");
+			assert(0);
+			break;
+	}
+
+	/* Add a dummy to allow loads to move around */
+	midgard_insert_dummy(ctx);
+}
+
+static unsigned
+midgard_tex_format(enum glsl_sampler_dim dim)
+{
+	switch (dim) {
+		case GLSL_SAMPLER_DIM_2D:
+			return TEXTURE_2D;
+
+		case GLSL_SAMPLER_DIM_3D:
+			return TEXTURE_3D;
+
+		case GLSL_SAMPLER_DIM_CUBE:
+			return TEXTURE_CUBE;
+
+		default:
+			printf("Unknown sampler dim type\n");
+			assert(0);
+	}
+}
+
+static void
+emit_tex(compiler_context *ctx, nir_tex_instr *instr)
+{
+	/* TODO */
+	//assert (!instr->sampler);
+	//assert (!instr->texture_array_size);
+	assert (instr->op == nir_texop_tex);
+
+	/* Allocate registers via a round robin scheme to alternate between the two registers */
+	int reg = ctx->texture_op_count & 1;
+	int in_reg = reg, out_reg = reg;
+
+	/* Make room for the reg */
+
+	if (ctx->texture_index[reg] > -1)
+		unalias_ssa(ctx, ctx->texture_index[reg]);
+
+	int texture_index = instr->texture_index;
+	int sampler_index = texture_index;
+
+	for (unsigned i = 0; i < instr->num_srcs; ++i) {
+		switch (instr->src[i].src_type) {
+			case nir_tex_src_coord: {
+				int index = nir_src_index(&instr->src[i].src);
+
+				midgard_vector_alu_src alu_src = blank_alu_src;
+				alu_src.swizzle = (COMPONENT_Y << 2);
+
+				midgard_instruction ins = v_fmov(index, alu_src, REGISTER_TEXTURE_BASE + in_reg, true, midgard_outmod_none);
+				util_dynarray_append((ctx->current_block), midgard_instruction, ins);
+
+				break;
+			}
+			default: {
+				printf("Unknown source type\n");
+				assert(0);
+				break;
+			 }
+		}
+	}
+	
+	/* No helper to build texture words -- we do it all here */
+	midgard_instruction ins = {
+		.type = TAG_TEXTURE_4,
+		.texture = {
+			.op = TEXTURE_OP_NORMAL,
+			.format = midgard_tex_format(instr->sampler_dim),
+			.texture_handle = texture_index,
+			.sampler_handle = sampler_index,
+
+			/* TODO: Don't force xyzw */
+			.swizzle = SWIZZLE(COMPONENT_X, COMPONENT_Y, COMPONENT_Z, COMPONENT_W), 
+			.mask = 0xF, 
+
+			/* TODO: half */
+			//.in_reg_full = 1,
+			.out_full = 1,
+
+			.filter = 1,
+			
+			/* Always 1 */
+			.unknown7 = 1,
+
+			/* Assume we can continue; hint it out later */
+			.cont = 1,
+		}
+	};
+
+	/* Set registers to read and write from the same place */
+	ins.texture.in_reg_select = in_reg;
+	ins.texture.out_reg_select = out_reg;
+	
+	/* TODO: Dynamic swizzle input selection, half-swizzles? */
+	if (instr->sampler_dim == GLSL_SAMPLER_DIM_3D) {
+		ins.texture.in_reg_swizzle_right = COMPONENT_X;
+		ins.texture.in_reg_swizzle_left = COMPONENT_Y;
+		//ins.texture.in_reg_swizzle_third = COMPONENT_Z;
+	} else {
+		ins.texture.in_reg_swizzle_left = COMPONENT_X;
+		ins.texture.in_reg_swizzle_right = COMPONENT_Y;
+		//ins.texture.in_reg_swizzle_third = COMPONENT_X;
+	}
+
+	util_dynarray_append(ctx->current_block, midgard_instruction, ins);
+
+	/* Simultaneously alias the destination and emit a move for it. The move will be eliminated if possible */
+
+	int o_reg = REGISTER_TEXTURE_BASE + out_reg, o_index = nir_dest_index(&instr->dest);
+	alias_ssa(ctx, o_index, SSA_FIXED_REGISTER(o_reg));
+	ctx->texture_index[reg] = o_index;
+
+	midgard_instruction ins2 = v_fmov(SSA_FIXED_REGISTER(o_reg), blank_alu_src, o_index, false, midgard_outmod_none);
+	util_dynarray_append(ctx->current_block, midgard_instruction, ins2);
+
+	/* Used for .cont and .last hinting */
+	ctx->texture_op_count++;
+}
+
+static void
+emit_instr(compiler_context *ctx, struct nir_instr *instr)
+{
+#ifdef NIR_DEBUG_FINE
+	nir_print_instr(instr, stdout);
+	putchar('\n');
+#endif
+
+	switch(instr->type) {
+		case nir_instr_type_load_const:
+			emit_load_const(ctx, nir_instr_as_load_const(instr));
+			break;
+
+		case nir_instr_type_intrinsic:
+			emit_intrinsic(ctx, nir_instr_as_intrinsic(instr));
+			break;
+
+		case nir_instr_type_alu:
+			emit_alu(ctx, nir_instr_as_alu(instr));
+			break;
+
+		case nir_instr_type_tex:
+			emit_tex(ctx, nir_instr_as_tex(instr));
+			break;
+
+		case nir_instr_type_ssa_undef:
+			/* Spurious */
+			break;
+
+		default:
+			printf("Unhandled instruction type\n");
+			//assert(0);
+			break;
+	}
+}
+
+/* Determine the actual hardware from the index based on the RA results or special values */
+
+static int
+dealias_register(compiler_context *ctx, struct ra_graph *g, int reg, int maxreg)
+{
+        if (reg >= SSA_FIXED_MINIMUM)
+                return SSA_REG_FROM_FIXED(reg);
+
+	if (reg >= 0) {
+		assert(reg < maxreg);
+		int r = ra_get_node_reg(g, reg);
+		ctx->work_registers = MAX2(ctx->work_registers, r);
+		return r;
+	}
+
+	switch(reg) {
+		/* fmov style unused */
+		case SSA_UNUSED_0: return REGISTER_UNUSED;
+		
+		/* lut style unused */
+		case SSA_UNUSED_1: return REGISTER_UNUSED;
+
+		default:
+		   printf("Unknown SSA register alias %d\n", reg);
+		   assert(0);
+		   return 31;
+	}
+}
+
+static unsigned int
+midgard_ra_select_callback(struct ra_graph *g, BITSET_WORD *regs, void *data)
+{
+	/* Choose the first available register to minimise reported register pressure */
+
+	for (int i = 0; i < 16; ++i) {
+		if (BITSET_TEST(regs, i)) {
+			return i;
+		}
+	}
+
+	assert(0);
+}
+
+static bool
+is_live_after(midgard_block *block, midgard_instruction *start, int src)
+{
+	/* Check the rest of the block for liveness */
+	for (midgard_instruction *ins = start + 1; IN_ARRAY(ins, &block->instructions); ++ins) {
+		if (ins->unused) continue;
+		if (!ins->uses_ssa) continue;
+
+		if (ins->ssa_args.src0 == src) return true;
+		if (!ins->ssa_args.inline_constant && ins->ssa_args.src1 == src) return true;
+	}
+
+	/* TODO: Check the succeeding blocks */
+	return false;
+}
+
+static void
+allocate_registers(compiler_context *ctx)
+{
+	/* First, initialize the RA */
+	struct ra_regs *regs = ra_alloc_reg_set(NULL, 32, true);
+
+	/* Create a primary (general purpose) class, as well as special purpose
+	 * pipeline register classes */
+
+	int primary_class = ra_alloc_reg_class(regs);
+	int varying_class  = ra_alloc_reg_class(regs);
+
+	/* Add the full set of work registers */
+	for (int i = 0; i < 16; ++i)
+		ra_class_add_reg(regs, primary_class, i);
+
+	/* Add special registers */
+	ra_class_add_reg(regs, varying_class, REGISTER_VARYING_BASE);
+	ra_class_add_reg(regs, varying_class, REGISTER_VARYING_BASE + 1);
+
+	/* We're done setting up */
+	ra_set_finalize(regs, NULL);
+
+	/* Transform the MIR into squeezed index form */
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		util_dynarray_foreach(&block->instructions, midgard_instruction, ins) {
+			if (ins->unused) continue;
+			if (!ins->uses_ssa) continue;
+
+			ins->ssa_args.src0 = find_or_allocate_temp(ctx, ins->ssa_args.src0);
+
+			if (!ins->ssa_args.inline_constant) {
+				ins->ssa_args.src1 = find_or_allocate_temp(ctx, ins->ssa_args.src1);
+			}
+
+			if (!ins->ssa_args.literal_out) {
+				ins->ssa_args.dest = find_or_allocate_temp(ctx, ins->ssa_args.dest);
+			}
+		}
+
+		print_mir_block(block);
+	}
+
+	/* Let's actually do register allocation */
+	int nodes = ctx->temp_count;
+	struct ra_graph *g = ra_alloc_interference_graph(regs, nodes);
+
+	/* Set everything to the work register class, unless it has somewhere
+	 * special to go */
+
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		util_dynarray_foreach(&block->instructions, midgard_instruction, ins) {
+			if (ins->unused) continue;
+			if (!ins->uses_ssa) continue;
+			if (ins->ssa_args.literal_out) continue;
+			if (ins->ssa_args.dest < 0) continue;
+			if (ins->ssa_args.dest >= SSA_FIXED_MINIMUM) continue;
+
+			int class = primary_class;
+
+			ra_set_node_class(g, ins->ssa_args.dest, class);
+		}
+	}
+
+	for (int index = 0; index <= ctx->max_hash; ++index) {
+		unsigned temp = (uintptr_t) _mesa_hash_table_u64_search(ctx->ssa_to_register, index + 1);
+
+		if (temp) {
+			unsigned reg = temp - 1;
+			int t = find_or_allocate_temp(ctx, index);
+			ra_set_node_reg(g, t, reg);
+		}
+	}
+
+	/* Determine liveness */
+
+	int *live_start = malloc(nodes * sizeof(int));
+	int *live_end = malloc(nodes * sizeof(int));
+
+	/* Initialize as non-existent */
+
+	for (int i = 0; i < nodes; ++i) {
+		live_start[i] = live_end[i] = -1;
+	}
+
+	int d = 0;
+
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		util_dynarray_foreach(&block->instructions, midgard_instruction, ins) {
+			if (ins->unused) continue;
+			if (!ins->uses_ssa) continue;
+
+			if (!ins->ssa_args.literal_out) {
+				/* If this destination is not yet live, it is now since we just wrote it */
+
+				int dest = ins->ssa_args.dest;
+
+				if (live_start[dest] == -1)
+					live_start[dest] = d;
+			}
+
+			/* Since we just used a source, the source might be
+			 * dead now. Scan the rest of the block for
+			 * invocations, and if there are none, the source dies
+			 * */
+
+			int src2 = ins->ssa_args.inline_constant ? -1 : ins->ssa_args.src1;
+			int sources[2] = { ins->ssa_args.src0, src2 };
+
+			for (int src = 0; src < 2; ++src) {
+				int s = sources[src];
+				
+				if (s < 0) continue;
+				if (s >= SSA_FIXED_MINIMUM) continue;
+
+				if (!is_live_after(block, ins, s)) {
+					live_end[s] = d;
+				}
+			}
+
+			++d;
+		}
+	}
+
+	/* If a node still hasn't been killed, kill it now */
+
+	for (int i = 0; i < nodes; ++i) {
+		if (live_start[i] == -1)
+			printf("-1 start for %d?\n", i);
+
+		if (live_end[i] == -1)
+			live_end[i] = d;
+	}
+
+	/* Setup interference between nodes that are live at the same time */
+
+	for (int i = 0; i < nodes; ++i) {
+		for (int j = i + 1; j < nodes; ++j) {
+			if (!(live_start[i] >= live_end[j] || live_start[j] >= live_end[i]))
+				if ((live_start[i] != -1) && (live_start[j] != -1))
+					ra_add_node_interference(g, i, j);
+		}
+	}
+
+	for (int i = 0; i < nodes; ++i) {
+		for (int j = i + 1; j < nodes; ++j) {
+			if (i == 0)
+				ra_add_node_interference(g, i, j);
+		}
+	}
+
+	ra_set_select_reg_callback(g, midgard_ra_select_callback, NULL);
+
+	if (!ra_allocate(g)) {
+		printf("Error allocating registers\n");
+		assert(0);
+	}
+
+	/* Cleanup */
+	free(live_start);
+	free(live_end);
+
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		util_dynarray_foreach(&block->instructions, midgard_instruction, ins) {
+			if (ins->unused) continue;
+			if (!ins->uses_ssa) continue;
+
+			ssa_args args = ins->ssa_args;
+
+			switch (ins->type) {
+				case TAG_ALU_4:
+					ins->registers.src1_reg = dealias_register(ctx, g, args.src0, nodes);
+
+					ins->registers.src2_imm = args.inline_constant;
+
+					if (args.inline_constant) {
+						/* Encode inline 16-bit constant as a vector by default */
+
+						ins->registers.src2_reg = args.src1 >> 11;
+
+						int lower_11 = args.src1 & ((1 << 12) - 1);
+
+						uint16_t imm = ((lower_11 >> 8) & 0x7) | ((lower_11 & 0xFF) << 3);
+						ins->alu.src2 = imm << 2;
+					} else {
+						ins->registers.src2_reg = dealias_register(ctx, g, args.src1, nodes);
+					}
+
+					ins->registers.out_reg = args.literal_out ? args.dest : dealias_register(ctx, g, args.dest, nodes);
+
+					break;
+				
+				case TAG_LOAD_STORE_4: {
+					if (OP_IS_STORE(ins->load_store.op)) {
+						/* ssa_args invalid for store_vary */
+						ins->load_store.reg = 0;
+					} else {
+						bool has_dest = args.dest >= 0;
+						int ssa_arg = has_dest ? args.dest : args.src0;
+
+						ins->load_store.reg = dealias_register(ctx, g, ssa_arg, nodes);
+					}
+
+					break;
+				}
+
+				default: 
+					break;
+			}
+		}
+	}
+}
+
+/* Midgard IR only knows vector ALU types, but we sometimes need to actually
+ * use scalar ALU instructions, for functional or performance reasons. To do
+ * this, we just demote vector ALU payloads to scalar. */
+
+static int
+component_from_mask(unsigned mask) {
+	for (int c = 0; c < 4; ++c) {
+		if (mask & (3 << (2*c)))
+			return c;
+	}
+
+	assert(0);
+}
+
+static bool
+is_single_component_mask(unsigned mask)
+{
+	int components = 0;
+
+	for (int c = 0; c < 4; ++c)
+		if (mask & (3 << (2*c)))
+			components++;
+	
+	return components == 1;
+}
+
+/* Create a mask of accessed components from a swizzle to figure out vector
+ * dependencies */
+
+static unsigned
+swizzle_to_access_mask(unsigned swizzle)
+{
+	unsigned component_mask = 0;
+
+	for (int i = 0; i < 4; ++i) {
+		unsigned c = (swizzle >> (2 * i)) & 3;
+		component_mask |= (1 << c);
+	}
+
+	return component_mask;
+}
+
+static unsigned
+vector_to_scalar_source(unsigned u)
+{
+	midgard_vector_alu_src v;
+	memcpy(&v, &u, sizeof(v));
+
+	midgard_scalar_alu_src s = {
+		.abs = v.abs,
+		.negate = v.negate,
+		.full = !v.half,
+		.component = (v.swizzle & 3) << 1
+	};
+
+	unsigned o;
+	memcpy(&o, &s, sizeof(s));
+
+	return o & ((1 << 6) - 1);
+}
+
+static midgard_scalar_alu
+vector_to_scalar_alu(midgard_vector_alu v, ssa_args *args)
+{
+	/* The output component is from the mask */
+	midgard_scalar_alu s = {
+		.op = v.op,
+		.src1 = vector_to_scalar_source(v.src1),
+		.src2 = vector_to_scalar_source(v.src2),
+		.unknown = 0,
+		.outmod = v.outmod,
+		.output_full = 1, /* TODO: Half */
+		.output_component = component_from_mask(v.mask) << 1,
+	};
+
+	/* Inline constant is passed along rather than trying to extract it
+	 * from v */
+
+	if (args->inline_constant) {
+		uint16_t imm = 0;
+		int lower_11 = args->src1 & ((1 << 12) - 1);
+		imm |= (lower_11 >> 9) & 3;
+		imm |= (lower_11 >> 6) & 4;
+		imm |= (lower_11 >> 2) & 0x38;
+		imm |= (lower_11 & 63) << 6;
+
+		s.src2 = imm;
+	}
+
+	return s;
+}
+
+/* Midgard prefetches instruction types, so during emission we need to
+ * lookahead too. Unless this is the last instruction, in which we return 1. Or
+ * if this is the second to last and the last is an ALU, then it's also 1... */
+
+#define IS_ALU(tag) (tag == TAG_ALU_4 || tag == TAG_ALU_8 ||  \
+		     tag == TAG_ALU_12 || tag == TAG_ALU_16)
+
+#define EMIT_AND_COUNT(type, val) util_dynarray_append(emission, type, val); \
+				  bytes_emitted += sizeof(type)
+
+static void
+emit_binary_vector_instruction(midgard_instruction *ains,
+		uint16_t *register_words, int *register_words_count, 
+		uint64_t *body_words, size_t *body_size, int *body_words_count, 
+		size_t *bytes_emitted)
+{
+	memcpy(&register_words[(*register_words_count)++], &ains->registers, sizeof(ains->registers));
+	*bytes_emitted += sizeof(midgard_reg_info);
+
+	body_size[*body_words_count] = sizeof(midgard_vector_alu);
+	memcpy(&body_words[(*body_words_count)++], &ains->alu, sizeof(ains->alu));
+	*bytes_emitted += sizeof(midgard_vector_alu);
+}
+
+/* Checks for an SSA data hazard between two adjacent instructions, keeping in
+ * mind that we are a vector architecture and we can write to different
+ * components simultaneously */
+
+static bool
+can_run_concurrent_ssa(midgard_instruction *first, midgard_instruction *second)
+{
+	/* If it's non-SSA... it's probably fine? */
+	if (!first->uses_ssa || !second->uses_ssa)
+		return true;
+
+	/* Each instruction reads some registers and writes to a register. See
+	 * where the first writes */
+
+	if (first->ssa_args.literal_out)
+		return false; /* Bail */
+
+	/* Figure out where exactly we wrote to */
+	int source = first->ssa_args.dest;
+	int source_mask = first->type == TAG_ALU_4 ? squeeze_writemask(first->alu.mask) : 0xF;
+
+	/* As long as the second doesn't read from the first, we're okay */
+	if (second->ssa_args.src0 == source) {
+		if (first->type == TAG_ALU_4) {
+			/* Figure out which components we just read from */
+
+			int q = second->alu.src1;
+			midgard_vector_alu_src *m = (midgard_vector_alu_src *) &q;
+
+			/* Check if there are components in common, and fail if so */
+			if (swizzle_to_access_mask(m->swizzle) & source_mask)
+				return false;
+		} else 
+			return false;
+
+	}
+
+	if (second->ssa_args.src1 == source && !second->ssa_args.inline_constant)
+		return false;
+
+	/* Otherwise, it's safe in that regard. Another data hazard is both
+	 * writing to the same place, of course */
+
+	if (!second->ssa_args.literal_out && second->ssa_args.dest == source)
+		return false;
+
+	/* ...That's it */
+	return true;
+}
+
+/* Schedules, but does not emit, a single basic block. After scheduling, the
+ * final tag and size of the block are known, which are necessary for branching
+ * */
+
+static midgard_bundle
+schedule_bundle(compiler_context *ctx, midgard_block *block, midgard_instruction *ins, int *skip)
+{
+	int instructions_emitted = 0, instructions_consumed = -1;
+	midgard_bundle bundle = { 0 };
+
+	uint8_t tag = ins->type;
+
+	/* Default to the instruction's tag */
+	bundle.tag = tag;
+
+	switch(ins->type) {
+		case TAG_ALU_4: {
+			uint32_t control = 0;
+			size_t bytes_emitted = sizeof(control);
+		
+			/* TODO: Constant combining */
+			int index = 0, last_unit = 0;
+
+			/* Previous instructions, for the purpose of parallelism */
+			midgard_instruction *segment[4] = {0};
+			int segment_size = 0;
+
+			instructions_emitted = -1;
+
+			while (IN_ARRAY(ins + index, &block->instructions)) {
+				midgard_instruction *ains = ins + index; 
+
+				/* Ensure that the chain can continue */
+				if (ains->unused) goto skip_instruction;
+				if (ains->type != TAG_ALU_4) break;
+
+				/* According to the presentation "The ARM
+				 * Mali-T880 Mobile GPU" from HotChips 27,
+				 * there are two pipeline stages. Branching
+				 * position determined experimentally. Lines
+				 * are executed in parallel: 
+				 *
+				 * [ VMUL ] [ SADD ]
+				 * [ VADD ] [ SMUL ] [ LUT ] [ BRANCH ]
+				 *
+				 * Verify that there are no ordering dependencies here.
+				 *
+				 * TODO: Allow for parallelism!!!
+				 */
+
+				/* Pick a unit for it if it doesn't force a particular unit */
+
+				int unit = ains->unit;
+				bool vectorize = false;
+
+				if (!unit) {
+					int op = ains->alu.op;
+					int units = alu_opcode_unit[op];
+
+					/* TODO: Promotion of scalars to vectors */
+					int vector = ((!is_single_component_mask(ains->alu.mask)) || ((units & UNITS_SCALAR) == 0)) && (units & UNITS_ANY_VECTOR);
+
+					if (!vector)
+						assert(units & UNITS_SCALAR);
+
+					if (vector) {
+						if (last_unit >= UNIT_VADD) {
+							if (units & UNIT_VADD)
+								unit = UNIT_VADD;
+							else if (units & UNIT_VLUT)
+								unit = UNIT_VLUT;
+							else
+								break;
+						} else {
+							if (units & UNIT_VMUL)
+								unit = UNIT_VMUL;
+							else if (units & UNIT_VADD)
+								unit = UNIT_VADD;
+							else if (units & UNIT_VLUT)
+								unit = UNIT_VLUT;
+							else
+								assert(0);
+						}
+					} else {
+						if (last_unit >= UNIT_VADD) {
+							if ((units & UNIT_SMUL) && !(control & UNIT_SMUL))
+								unit = UNIT_SMUL;
+							else if (units & UNIT_VLUT) {
+								unit = UNIT_VLUT;
+								vectorize = true;
+							} else
+								break;
+						} else {
+							if ((units & UNIT_SADD) && !(control & UNIT_SADD))
+								unit = UNIT_SADD;
+							else if (units & UNIT_SMUL)
+								unit = UNIT_SMUL;
+							else
+								break;
+						}
+					}
+
+					assert(unit & units);
+				}
+
+				/* Late unit check, this time for encoding (not parallelism) */
+				if (unit <= last_unit) break;
+
+				/* Clear the segment */
+				if (last_unit < UNIT_VADD && unit >= UNIT_VADD)
+					segment_size = 0;
+
+				/* Check for data hazards */
+				int has_hazard = false;
+
+				for (int s = 0; s < segment_size; ++s)
+					if (!can_run_concurrent_ssa(segment[s], ains))
+						has_hazard = true;
+
+				if (has_hazard)
+					break;
+
+				/* We're good to go -- emit the instruction */
+				ains->unit = unit;
+
+				/* Promote scalar to vector if needed */
+				if (vectorize)
+					ains->vector = true;
+
+				segment[segment_size++] = ains;
+
+				/* Only one set of embedded constants per
+				 * bundle possible; if we have more, we must
+				 * break the chain early, unfortunately */
+
+				if (ains->has_constants) {
+					if (bundle.has_embedded_constants) {
+						/* ...but if there are already
+						 * constants but these are the
+						 * *same* constants, we let it
+						 * through */
+
+						if (memcmp(bundle.constants, ains->constants, sizeof(bundle.constants)))
+							break;
+					} else {
+						bundle.has_embedded_constants = true;
+						memcpy(bundle.constants, ains->constants, sizeof(bundle.constants));
+
+						/* If this is a blend shader special constant, track it for patching */
+						if (ains->has_blend_constant)
+							bundle.has_blend_constant = true;
+					}
+				}
+
+				if (ains->unit & UNITS_ANY_VECTOR) {
+					emit_binary_vector_instruction(ains, bundle.register_words,
+							&bundle.register_words_count, bundle.body_words,
+							bundle.body_size, &bundle.body_words_count, &bytes_emitted);
+				} else if (ains->compact_branch) {
+					 /* All of r0 has to be written out
+					  * along with the branch writeout.
+					  * (slow!) */
+
+					if (ains->writeout) {
+						if (index == 0) {
+							midgard_instruction ins = v_fmov(0, blank_alu_src, 0, true, midgard_outmod_none);
+							ins.unit = UNIT_VMUL;
+
+							control |= ins.unit;
+
+							emit_binary_vector_instruction(&ins, bundle.register_words,
+									&bundle.register_words_count, bundle.body_words,
+									bundle.body_size, &bundle.body_words_count, &bytes_emitted);
+						} else {
+							/* Analyse the group to see if r0 is written in full */
+							bool components[4] = { 0 };
+							uint16_t register_dep_mask = 0;
+							uint16_t written_mask = 0;
+
+							for (int t = 0; t < index; ++t) {
+								midgard_instruction *qins = ins + t;
+
+								if (qins->unused) continue;
+
+								/* Mark down writes */
+
+								if (qins->registers.out_reg != 0) {
+									written_mask |= (1 << qins->registers.out_reg);
+									continue;
+								}
+
+								/* Mark down the register dependencies for errata check */
+
+								if (qins->registers.src1_reg < 16)
+									register_dep_mask |= (1 << qins->registers.src1_reg);
+
+								if (qins->registers.src2_reg < 16)
+									register_dep_mask |= (1 << qins->registers.src2_reg);
+
+								int mask = qins->alu.mask;
+
+								for (int c = 0; c < 4; ++c)
+									if (mask & (0x3 << (2 * c)))
+										components[c] = true;
+							}
+
+
+							/* ERRATA (?): In a bundle ending in a fragment writeout, the register dependencies of r0 cannot be written within this bundle (discovered in -bshading:shading=phong) */
+							if (register_dep_mask & written_mask) {
+								printf("ERRATA WORKAROUND: Breakup for writeout dependency masks %X vs %X (common %X)\n", register_dep_mask, written_mask, register_dep_mask & written_mask);
+								break;
+							}
+
+							/* If even a single component is not written, break it up (conservative check). */
+							bool breakup = false;
+
+							for (int c = 0; c < 4; ++c)
+								if (!components[c])
+									breakup = true;
+
+							if (breakup)
+								break;
+
+							/* Otherwise, we're free to proceed */
+						}
+					}
+
+					bundle.body_size[bundle.body_words_count] = sizeof(ains->br_compact);
+					memcpy(&bundle.body_words[bundle.body_words_count++], &ains->br_compact, sizeof(ains->br_compact));
+					bytes_emitted += sizeof(ains->br_compact);
+				} else {
+					memcpy(&bundle.register_words[bundle.register_words_count++], &ains->registers, sizeof(ains->registers));
+					bytes_emitted += sizeof(midgard_reg_info);
+
+					bundle.body_size[bundle.body_words_count] = sizeof(midgard_scalar_alu);
+					bundle.body_words_count++;
+					bytes_emitted += sizeof(midgard_scalar_alu);
+				}
+
+				/* Defer marking until after writing to allow for break */
+				control |= ains->unit;
+				last_unit = ains->unit;
+				++instructions_emitted;
+
+skip_instruction:
+				++index;
+			}
+
+			/* Bubble up the number of instructions for skipping */
+			instructions_consumed = index - 1;
+
+			int padding = 0;
+
+			/* Pad ALU op to nearest word */
+
+			if (bytes_emitted & 15) {
+				padding = 16 - (bytes_emitted & 15);
+				bytes_emitted += padding;
+			}
+
+			/* Constants must always be quadwords */
+			if (bundle.has_embedded_constants)
+				bytes_emitted += 16;
+
+			/* Size ALU instruction for tag */
+			bundle.tag = (TAG_ALU_4) + (bytes_emitted / 16) - 1;
+			bundle.padding = padding;
+			bundle.control = bundle.tag | control;
+
+			break;
+		 }
+
+		case TAG_LOAD_STORE_4: {
+			/* Load store instructions have two words at once. If
+			 * we only have one queued up, we need to NOP pad.
+			 * Otherwise, we store both in succession to save space
+			 * and cycles -- letting them go in parallel -- skip
+			 * the next. The usefulness of this optimisation is
+			 * greatly dependent on the quality of the instruction
+			 * scheduler.
+			 */
+
+			int idx = 1;
+
+			while(IN_ARRAY((ins + idx), &block->instructions) && (ins + idx)->unused)
+				++idx;
+
+			if (IN_ARRAY((ins + idx), &block->instructions) && ((ins + idx)->type == TAG_LOAD_STORE_4)) {
+				/* As the two operate concurrently, make sure
+				 * they are not dependent */
+
+				if (can_run_concurrent_ssa(ins, ins + idx) || true) {
+					/* Skip ahead, since it's redundant with the pair */
+					instructions_consumed = idx + (instructions_emitted++);
+				}
+			}
+
+			break;
+		}
+
+#if 0
+		case TAG_TEXTURE_4:
+			/* TODO: Schedule texture ops */
+			break;
+#endif
+
+		default:
+			/* XXX: What happens with textures? */
+			break;
+	}
+
+	/* Copy the instructions into the bundle */
+	bundle.instruction_count = instructions_emitted + 1;
+	
+	int used_idx = 0;
+	for (int i = 0; used_idx < bundle.instruction_count; ++i) {
+		if ((ins + i)->unused) continue;
+
+		bundle.instructions[used_idx++] = *(ins + i);
+	}
+
+	*skip = (instructions_consumed == -1) ? instructions_emitted : instructions_consumed;
+
+	return bundle;
+}
+
+static int
+quadword_size(int tag)
+{
+	switch (tag) {
+		case TAG_ALU_4: 	return 1;
+		case TAG_ALU_8: 	return 2;
+		case TAG_ALU_12: 	return 3;
+		case TAG_ALU_16: 	return 4;
+		case TAG_LOAD_STORE_4: 	return 1;
+		case TAG_TEXTURE_4: 	return 1;
+		default: 		assert(0);
+	}
+}
+
+/* Schedule a single block by iterating its instruction to create bundles.
+ * While we go, tally about the bundle sizes to compute the block size. */
+
+static void
+schedule_block(compiler_context *ctx, midgard_block *block)
+{
+	util_dynarray_init(&block->bundles, NULL);
+
+	block->quadword_count = 0;
+
+	util_dynarray_foreach(&block->instructions, midgard_instruction, ins) {
+		if (!ins->unused) {
+			int skip;
+			midgard_bundle bundle = schedule_bundle(ctx, block, ins, &skip);
+			util_dynarray_append(&block->bundles, midgard_bundle, bundle);
+
+			if (bundle.has_blend_constant) {
+				/* TODO: Multiblock? */
+				int quadwords_within_block = block->quadword_count + quadword_size(bundle.tag) - 1;
+				ctx->blend_constant_offset = quadwords_within_block * 0x10;
+			}
+			
+			ins += skip;
+			block->quadword_count += quadword_size(bundle.tag);
+		}
+	}
+
+	block->is_scheduled = true;
+}
+
+static void
+schedule_program(compiler_context *ctx)
+{
+	allocate_registers(ctx);
+
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		schedule_block(ctx, block);
+	}
+}
+
+/* After everything is scheduled, emit whole bundles at a time */
+
+static void
+emit_binary_bundle(compiler_context *ctx, midgard_bundle *bundle, struct util_dynarray *emission, int next_tag)
+{
+	int lookahead = next_tag << 4;
+
+	switch(bundle->tag) {
+		case TAG_ALU_4:
+		case TAG_ALU_8:
+		case TAG_ALU_12:
+		case TAG_ALU_16: {
+                       /* Actually emit each component */
+		       util_dynarray_append(emission, uint32_t, bundle->control | lookahead);
+
+                       for (int i = 0; i < bundle->register_words_count; ++i)
+                               util_dynarray_append(emission, uint16_t, bundle->register_words[i]);
+
+		       /* Emit body words based on the instructions bundled */
+		       for (int i = 0; i < bundle->instruction_count; ++i) {
+			       midgard_instruction *ins = &bundle->instructions[i];
+			       assert(!ins->unused);
+
+			       if (ins->unit & UNITS_ANY_VECTOR) {
+					memcpy(util_dynarray_grow(emission, sizeof(midgard_vector_alu)), &ins->alu, sizeof(midgard_vector_alu));
+			       } else if (ins->compact_branch) {
+				       /* Dummy move, XXX DRY */
+				       if ((i == 0) && ins->writeout) {
+						midgard_instruction ins = v_fmov(0, blank_alu_src, 0, true, midgard_outmod_none);
+						memcpy(util_dynarray_grow(emission, sizeof(midgard_vector_alu)), &ins.alu, sizeof(midgard_vector_alu));
+				       }
+
+					memcpy(util_dynarray_grow(emission, sizeof(ins->br_compact)), &ins->br_compact, sizeof(ins->br_compact));
+			       } else {
+				       /* Scalar */
+ 					midgard_scalar_alu scalarised = vector_to_scalar_alu(ins->alu, &ins->ssa_args);
+					memcpy(util_dynarray_grow(emission, sizeof(scalarised)), &scalarised, sizeof(scalarised));
+			       }
+		       }
+
+                       /* Emit padding (all zero) */
+                       memset(util_dynarray_grow(emission, bundle->padding), 0, bundle->padding);
+
+                       /* Tack on constants */
+
+                       if (bundle->has_embedded_constants) {
+			       util_dynarray_append(emission, float, bundle->constants[0]);
+			       util_dynarray_append(emission, float, bundle->constants[1]);
+			       util_dynarray_append(emission, float, bundle->constants[2]);
+			       util_dynarray_append(emission, float, bundle->constants[3]);
+                       }
+
+			break;
+		 }
+
+		case TAG_LOAD_STORE_4: {
+			/* One or two composing instructions */
+
+			uint64_t current64, next64 = LDST_NOP;
+
+			memcpy(&current64, &bundle->instructions[0].load_store, sizeof(current64));
+			
+			if (bundle->instruction_count == 2)
+				memcpy(&next64, &bundle->instructions[1].load_store, sizeof(next64));
+
+			midgard_load_store instruction = {
+				.type = bundle->tag,
+				.next_type = next_tag,
+				.word1 = current64,
+				.word2 = next64
+			};
+
+			util_dynarray_append(emission, midgard_load_store, instruction);
+
+			break;
+		}
+
+		case TAG_TEXTURE_4: {
+			/* Texture instructions are easy, since there is no
+			 * pipelining nor VLIW to worry about. We may need to set the .last flag */
+			
+			midgard_instruction *ins = &bundle->instructions[0];
+
+			ins->texture.type = TAG_TEXTURE_4;
+			ins->texture.next_type = next_tag;
+
+			ctx->texture_op_count--;
+
+			if (!ctx->texture_op_count) {
+				ins->texture.cont = 0;
+				ins->texture.last = 1;
+			}
+
+	    		util_dynarray_append(emission, midgard_texture_word, ins->texture);
+			break;
+		}		
+
+		default:
+			printf("Unknown midgard instruction type\n");
+			assert(0);
+			break;
+	}
+}
+
+
+/* ALU instructions can inline or embed constants, which decreases register
+ * pressure and saves space. */
+
+#define CONDITIONAL_ATTACH(src) { \
+	void *entry = _mesa_hash_table_u64_search(ctx->ssa_constants, alu->ssa_args.src + 1); \
+\
+	if (entry) { \
+		attach_constants(ctx, alu, entry, alu->ssa_args.src + 1); \
+		alu->ssa_args.src = SSA_FIXED_REGISTER(REGISTER_CONSTANT); \
+	} \
+}
+
+static void
+inline_alu_constants(compiler_context *ctx)
+{
+	util_dynarray_foreach(ctx->current_block, midgard_instruction, alu) {
+		/* Other instructions cannot inline constants */
+		if (alu->type != TAG_ALU_4) continue;
+
+		/* If there is already a constant here, we can do nothing */
+		if (alu->has_constants) continue;
+
+		/* Constants should always be SSA... */
+		if (!alu->uses_ssa) continue;
+
+		CONDITIONAL_ATTACH(src0);
+
+		if (!alu->has_constants) {
+			if (!alu->ssa_args.inline_constant) 
+				CONDITIONAL_ATTACH(src1)
+		} else if (!alu->ssa_args.inline_constant) {
+			/* Corner case: _two_ vec4 constants, for instance with a
+			 * csel. For this case, we can only use a constant
+			 * register for one, we'll have to emit a move for the
+			 * other. Note, if both arguments are constants, then
+			 * necessarily neither argument depends on the value of
+			 * any particular register. As the destination register
+			 * will be wiped, that means we can spill the constant
+			 * to the destination register.
+			 */
+
+			void *entry = _mesa_hash_table_u64_search(ctx->ssa_constants, alu->ssa_args.src1 + 1);
+		
+			if (entry) {
+				midgard_instruction ins = v_fmov(SSA_FIXED_REGISTER(REGISTER_CONSTANT), blank_alu_src, 4096 + alu->ssa_args.src1, false, midgard_outmod_none);
+				attach_constants(ctx, &ins, entry, alu->ssa_args.src1 + 1);
+
+				/* Force a break XXX Defer r31 writes */
+				ins.unit = UNIT_VLUT;
+
+				/* Set the source */
+				alu->ssa_args.src1 = 4096 + alu->ssa_args.src1;
+
+				/* Right before us (odd numbers) is a dummy
+				 * instruction.  Copy the new move into that.
+				 * */
+
+				memcpy(alu - 1, alu - 2, sizeof(ins));
+				memcpy(alu - 2, &ins, sizeof(ins));
+			}
+		}
+	}
+}
+
+/* Midgard supports two types of constants, embedded constants (128-bit) and
+ * inline constants (16-bit). Sometimes, especially with scalar ops, embedded
+ * constants can be demoted to inline constants, for space savings and
+ * sometimes a performance boost */
+
+static void
+embedded_to_inline_constant(compiler_context *ctx)
+{
+	util_dynarray_foreach(ctx->current_block, midgard_instruction, ins) {
+		if (!ins->has_constants) continue;
+		if (ins->ssa_args.inline_constant) continue;
+		if (ins->unused) continue;
+
+		/* Blend constants must not be inlined by definition */
+		if (ins->has_blend_constant) continue;
+
+		/* src1 cannot be an inline constant due to encoding
+		 * restrictions. So, if possible we try to flip the arguments
+		 * in that case */
+
+		int op = ins->alu.op;
+
+		if (ins->ssa_args.src0 == SSA_FIXED_REGISTER(REGISTER_CONSTANT)) {
+			/* Flip based on op. Fallthrough intentional */
+
+			switch (op) {
+				/* These ops require an operational change to flip their arguments TODO */
+				case midgard_alu_op_flt: 
+				case midgard_alu_op_fle: 
+				case midgard_alu_op_ilt: 
+				case midgard_alu_op_ile: 
+				case midgard_alu_op_fcsel: 
+				case midgard_alu_op_icsel: 
+				case midgard_alu_op_isub: 
+					printf("Missed non-commutative flip (%s)\n", alu_opcode_names[op]);
+					break;
+
+				/* These ops are commutative and Just Flip */
+				case midgard_alu_op_fne: 
+				case midgard_alu_op_fadd: 
+				case midgard_alu_op_fmul: 
+				case midgard_alu_op_fmin: 
+				case midgard_alu_op_fmax: 
+				case midgard_alu_op_iadd: 
+				case midgard_alu_op_imul: 
+				case midgard_alu_op_feq: 
+				case midgard_alu_op_ieq: 
+				case midgard_alu_op_ine: 
+				case midgard_alu_op_iand:
+				case midgard_alu_op_ior:
+				case midgard_alu_op_ixor:
+					/* Flip the SSA numbers */
+					ins->ssa_args.src0 = ins->ssa_args.src1;
+					ins->ssa_args.src1 = SSA_FIXED_REGISTER(REGISTER_CONSTANT);
+
+					/* And flip the modifiers */
+
+					unsigned src_temp;
+
+					src_temp = ins->alu.src2;
+					ins->alu.src2 = ins->alu.src1;
+					ins->alu.src1 = src_temp;
+				default:
+					break;
+			}
+		}
+
+		if (ins->ssa_args.src1 == SSA_FIXED_REGISTER(REGISTER_CONSTANT)) {
+			/* Extract the source information */
+
+			midgard_vector_alu_src *src;
+			int q = ins->alu.src2;
+			midgard_vector_alu_src *m = (midgard_vector_alu_src *) &q;
+			src = m;
+
+			/* Component is from the swizzle, e.g. r26.w -> w component */
+			int component = src->swizzle & 3;
+
+			/* Scale constant appropriately, if we can legally */
+			uint16_t scaled_constant = 0;
+
+			/* XXX: Check legality */
+			if (midgard_is_integer_op(op)) {
+				/* TODO: Inline integer */
+				continue;
+
+				unsigned int *iconstants = (unsigned int *) ins->constants;
+				scaled_constant = (uint16_t) iconstants[component];
+
+				/* Constant overflow after resize */
+				if (scaled_constant != iconstants[component])
+				       continue;
+			} else {
+				scaled_constant = _mesa_float_to_half((float) ins->constants[component]);
+			}
+
+			/* We don't know how to handle these with a constant */
+
+			if (src->abs || src->negate || src->half || src->rep_low || src->rep_high) {
+				printf("Bailing inline constant...\n");
+				continue;
+			}
+
+			/* Make sure that the constant is not itself a
+			 * vector by checking if all accessed values
+			 * (by the swizzle) are the same. */
+
+			uint32_t *cons = (uint32_t *) ins->constants;
+			uint32_t value = cons[component];
+
+			bool is_vector = false;
+
+			for (int c = 1; c < 4; ++c) {
+				uint32_t test = cons[(src->swizzle >> (2 * c)) & 3];
+				
+				if (test != value) {
+					is_vector = true;
+					break;
+				}
+			}
+
+			if (is_vector)
+				continue;
+
+			/* Get rid of the embedded constant */
+			ins->has_constants = false; 
+			ins->ssa_args.inline_constant = true;
+			ins->ssa_args.src1 = scaled_constant;
+		}
+	}
+}
+
+/* Map normal SSA sources to other SSA sources / fixed registers (like
+ * uniforms) */
+
+static void
+map_ssa_to_alias(compiler_context *ctx, int *ref)
+{
+	unsigned int alias = (uintptr_t) _mesa_hash_table_u64_search(ctx->ssa_to_alias, *ref + 1);
+	
+	if (alias) {
+		/* Remove entry in leftovers to avoid a redunant fmov */
+
+		struct set_entry *leftover = _mesa_set_search(ctx->leftover_ssa_to_alias, ((void *) (uintptr_t) (*ref + 1)));
+
+		if (leftover)
+			_mesa_set_remove(ctx->leftover_ssa_to_alias, leftover);
+
+		/* Assign the alias map */
+		*ref = alias - 1;
+		return;
+	}
+}
+
+#define AS_SRC(to, u) \
+	int q##to = ins->alu.src2; \
+	midgard_vector_alu_src *to = (midgard_vector_alu_src *) &q##to;
+
+/* Removing unused moves is necessary to clean up the texture pipeline results.
+ *
+ * To do so, we find moves in the MIR. We check if their destination is live later. If it's not, the move is redundant. */
+
+static void
+midgard_eliminate_orphan_moves(compiler_context *ctx, midgard_block *block)
+{
+	util_dynarray_foreach(&block->instructions, midgard_instruction, ins) {
+		if (ins->unused) continue;
+		if (!ins->uses_ssa) continue;
+		if (ins->type != TAG_ALU_4) continue;
+		if (ins->alu.op != midgard_alu_op_fmov) continue;
+		if (ins->ssa_args.literal_out) continue;
+		if (ins->ssa_args.dest >= SSA_FIXED_MINIMUM) continue;
+		if (midgard_is_pinned(ctx, ins->ssa_args.dest)) continue;
+		if (is_live_after(block, ins, ins->ssa_args.dest)) continue;
+
+		ins->unused = true;
+	}
+}
+
+/* The following passes reorder MIR instructions to enable better scheduling */
+
+static void
+midgard_pair_load_store(compiler_context *ctx, midgard_block *block)
+{
+	util_dynarray_foreach(&block->instructions, midgard_instruction, ins) {
+		if (ins->unused) continue;
+		if (!ins->uses_ssa) continue;
+		if (ins->type != TAG_LOAD_STORE_4) continue;
+
+		/* We've found a load/store op. Check if next is also load/store. Mind the gap */
+		if (IN_ARRAY(ins + 2, &block->instructions)) {
+			if ((ins + 2)->type == TAG_LOAD_STORE_4) {
+				/* If so, we're done since we're a pair */
+				++ins;
+				continue;
+			}
+
+			/* Maximum search distance to pair, to avoid register pressure disasters */
+			int search_distance = 8;
+
+			/* Otherwise, we have an orphaned load/store -- search for another load */
+			for (midgard_instruction *c = ins + 1; IN_ARRAY(c, &block->instructions) && search_distance; ++c, --search_distance) {
+				if (c->unused) continue;
+				if (!c->uses_ssa) continue;
+				if (c->type != TAG_LOAD_STORE_4) continue;
+				if (OP_IS_STORE(c->load_store.op)) continue;
+
+				/* We found one! Move it up to pair and remove it from the old location */
+
+				memcpy(ins + 1, c, sizeof(midgard_instruction));
+				c->unused = true;
+
+				break;
+			}
+		}
+	}
+}
+
+/* If there are leftovers after the below pass, emit actual fmov
+ * instructions for the slow-but-correct path */
+
+static void
+emit_leftover_move(compiler_context *ctx)
+{
+	set_foreach(ctx->leftover_ssa_to_alias, leftover) {
+		int base = ((uintptr_t) leftover->key) - 1;
+		int mapped = base;
+
+		map_ssa_to_alias(ctx, &mapped);
+		EMIT(fmov, mapped, blank_alu_src, base, false, midgard_outmod_none);
+	}
+}
+
+static void
+actualise_ssa_to_alias(compiler_context *ctx)
+{
+	util_dynarray_foreach(ctx->current_block, midgard_instruction, ins) {
+		if (!ins->uses_ssa) continue;
+
+		map_ssa_to_alias(ctx, &ins->ssa_args.src0);
+
+		if (!ins->ssa_args.inline_constant)
+			map_ssa_to_alias(ctx, &ins->ssa_args.src1);
+	}
+
+	emit_leftover_move(ctx);
+}
+
+/* Vertex shaders do not write gl_Position as is; instead, they write a
+ * transformed screen space position as a varying. See section 12.5 "Coordinate
+ * Transformation" of the ES 3.2 full specification for details.
+ *
+ * This transformation occurs early on, as NIR and prior to optimisation, in
+ * order to take advantage of NIR optimisation passes of the transform itself.
+ * */
+
+static void
+write_transformed_position(nir_builder *b, nir_src input_point_src, int uniform_no)
+{
+	nir_ssa_def *input_point = nir_ssa_for_src(b, input_point_src, 4);
+
+	/* Get viewport from the uniforms */
+	nir_intrinsic_instr *load;
+	load = nir_intrinsic_instr_create(b->shader, nir_intrinsic_load_uniform);
+	load->num_components = 4;
+	load->src[0] = nir_src_for_ssa(nir_imm_int(b, uniform_no));
+	nir_ssa_dest_init(&load->instr, &load->dest, 4, 32, NULL);
+	nir_builder_instr_insert(b, &load->instr);
+	
+	/* Formatted as <width, height, centerx, centery> */
+	nir_ssa_def *viewport_vec4 = &load->dest.ssa;
+	nir_ssa_def *viewport_width = nir_channel(b, viewport_vec4, 0);
+	nir_ssa_def *viewport_height = nir_channel(b, viewport_vec4, 1);
+	nir_ssa_def *viewport_offset = nir_channels(b, viewport_vec4, 0x8 | 0x4);
+
+	/* XXX: From uniforms? */
+	nir_ssa_def *depth_near = nir_imm_float(b, 0.0);
+	nir_ssa_def *depth_far = nir_imm_float(b, 1.0);
+
+	/* World space to normalised device coordinates */
+
+	nir_ssa_def *w_recip = nir_frcp(b, nir_channel(b, input_point, 3));
+	nir_ssa_def *ndc_point = nir_fmul(b, nir_channels(b, input_point, 0x7), w_recip);
+
+	/* Normalised device coordinates to screen space */
+
+	nir_ssa_def *viewport_multiplier = nir_vec2(b,
+			nir_fmul(b, viewport_width, nir_imm_float(b, 0.5f)),
+			nir_fmul(b, viewport_height, nir_imm_float(b, 0.5f)));
+
+	nir_ssa_def *viewport_xy = nir_fadd(b, nir_fmul(b, nir_channels(b, ndc_point, 0x3), viewport_multiplier), viewport_offset);
+
+	nir_ssa_def *depth_multiplier = nir_fmul(b, nir_fsub(b, depth_far, depth_near), nir_imm_float(b, 0.5f));
+	nir_ssa_def *depth_offset     = nir_fmul(b, nir_fadd(b, depth_far, depth_near), nir_imm_float(b, 0.5f));
+	nir_ssa_def *screen_depth     = nir_fadd(b, nir_fmul(b, nir_channel(b, ndc_point, 2), depth_multiplier), depth_offset);
+
+	nir_ssa_def *screen_space = nir_vec4(b, 
+			nir_channel(b, viewport_xy, 0),
+			nir_channel(b, viewport_xy, 1),
+			screen_depth,
+			nir_imm_float(b, 0.0));
+
+	/* Finally, write out the transformed values to the varying */
+
+	nir_intrinsic_instr *store;
+	store = nir_intrinsic_instr_create(b->shader, nir_intrinsic_store_output);
+	store->num_components = 4;
+	nir_intrinsic_set_base(store, 0);
+	nir_intrinsic_set_write_mask(store, 0xf);
+	store->src[0].ssa = screen_space;
+	store->src[0].is_ssa = true;
+	store->src[1] = nir_src_for_ssa(nir_imm_int(b, 0));
+	nir_builder_instr_insert(b, &store->instr);
+}
+
+static void
+transform_position_writes(nir_shader *shader)
+{
+	nir_foreach_function(func, shader) {
+		nir_foreach_block(block, func->impl) {
+			nir_foreach_instr_safe(instr, block) {
+				if (instr->type != nir_instr_type_intrinsic) continue;
+
+				nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
+				nir_variable *out = NULL;
+
+				switch (intr->intrinsic) {
+					case nir_intrinsic_store_output:
+						/* already had i/o lowered.. lookup the matching output var: */
+						nir_foreach_variable(var, &shader->outputs) {
+							int drvloc = var->data.driver_location;
+							if (nir_intrinsic_base(intr) == drvloc) {
+								out = var;
+								break;
+							}
+						}
+
+						break;
+					default:
+						break;
+				}
+
+				if (!out) continue;
+
+				if (out->data.mode != nir_var_shader_out)
+					continue;
+
+				if (out->data.location != VARYING_SLOT_POS)
+					continue;
+
+				nir_builder b;
+				nir_builder_init(&b, func->impl);
+				b.cursor = nir_before_instr(instr);
+
+				write_transformed_position(&b, intr->src[0], UNIFORM_VIEWPORT);
+				nir_instr_remove(instr);
+			}
+		}
+	}
+}
+
+static void
+emit_fragment_epilogue(compiler_context *ctx)
+{
+	/* See the docs for why this works. TODO: gl_FragDepth */
+
+	EMIT(alu_br_compact_cond, midgard_jmp_writeout_op_writeout, TAG_ALU_4, 0, midgard_condition_always);
+	EMIT(alu_br_compact_cond, midgard_jmp_writeout_op_writeout, TAG_ALU_4, -1, midgard_condition_always);
+}
+
+/* For the blend epilogue, we need to convert the blended fragment vec4 (stored
+ * in r0) to a RGBA8888 value by scaling and type converting. We then output it
+ * with the int8 analogue to the fragment epilogue */
+
+static void
+emit_blend_epilogue(compiler_context *ctx)
+{
+	/* vmul.fmul.none.fulllow hr48, r0, #255 */
+
+	midgard_instruction scale = {
+		.type = TAG_ALU_4,
+		.vector = true,
+		.uses_ssa = true,
+		.unit = UNIT_VMUL,
+		.ssa_args = {
+			.src0 = SSA_FIXED_REGISTER(0),
+			.src1 = _mesa_float_to_half(255.0),
+			.dest = 24,
+			.literal_out = true,
+			.inline_constant = true
+		},
+		.alu = {
+			.op = midgard_alu_op_fmul,
+			.reg_mode = midgard_reg_mode_full,
+			.dest_override = midgard_dest_override_lower,
+			.outmod = midgard_outmod_none,
+			.mask = 0xFF,
+			.src1 = vector_alu_srco_unsigned(blank_alu_src),
+			.src2 = vector_alu_srco_unsigned(blank_alu_src),
+		}
+	};
+
+	util_dynarray_append(ctx->current_block, midgard_instruction, scale);
+
+	/* vadd.f2u8.pos.low hr0, hr48, #0 */
+
+	midgard_vector_alu_src alu_src = blank_alu_src;
+	alu_src.half = true;
+
+	midgard_instruction f2u8 = {
+		.type = TAG_ALU_4,
+		.vector = true,
+		.uses_ssa = true,
+		.ssa_args = {
+			.src0 = SSA_FIXED_REGISTER(24),
+			.dest = 0,
+			.literal_out = true,
+			.inline_constant = true
+		},
+		.alu = {
+			.op = midgard_alu_op_f2u8,
+			.reg_mode = midgard_reg_mode_half,
+			.dest_override = midgard_dest_override_lower,
+			.outmod = midgard_outmod_pos,
+			.mask = 0xF,
+			.src1 = vector_alu_srco_unsigned(alu_src),
+			.src2 = vector_alu_srco_unsigned(blank_alu_src),
+		}
+	};
+
+	util_dynarray_append(ctx->current_block, midgard_instruction, f2u8);
+
+	/* vmul.imov.quarter r0, r0, r0 */
+
+	midgard_instruction imov_8 = {
+		.type = TAG_ALU_4,
+		.vector = true,
+		.alu = {
+			.op = midgard_alu_op_imov,
+			.reg_mode = midgard_reg_mode_quarter,
+			.dest_override = midgard_dest_override_none,
+			.outmod = midgard_outmod_none,
+			.mask = 0xFF,
+			.src1 = vector_alu_srco_unsigned(blank_alu_src),
+			.src2 = vector_alu_srco_unsigned(blank_alu_src),
+		}
+	};
+
+	/* Emit branch epilogue with the 8-bit move as the source */
+
+	util_dynarray_append(ctx->current_block, midgard_instruction, imov_8);
+	EMIT(alu_br_compact_cond, midgard_jmp_writeout_op_writeout, TAG_ALU_4, 0, midgard_condition_always);
+
+	util_dynarray_append(ctx->current_block, midgard_instruction, imov_8);
+	EMIT(alu_br_compact_cond, midgard_jmp_writeout_op_writeout, TAG_ALU_4, -1, midgard_condition_always);
+}
+
+static midgard_block *
+emit_block(compiler_context *ctx, nir_block *block)
+{
+	midgard_block this_block;
+	this_block.is_scheduled = false;
+	++ctx->block_count;
+
+	ctx->texture_index[0] = -1;
+	ctx->texture_index[1] = -1;
+
+	/* Set up current block */
+	util_dynarray_init(&this_block.instructions, NULL);
+	ctx->current_block = &this_block.instructions;
+
+	nir_foreach_instr(instr, block) {
+		emit_instr(ctx, instr);
+		++ctx->instruction_count;
+	}
+
+	inline_alu_constants(ctx);
+	embedded_to_inline_constant(ctx);
+
+	/* Perform heavylifting for aliasing */
+	actualise_ssa_to_alias(ctx);
+
+	midgard_eliminate_orphan_moves(ctx, &this_block);
+	midgard_pair_load_store(ctx, &this_block);
+
+#ifdef MIR_DEBUG
+	print_mir_block(&this_block);
+#endif
+
+	/* Append fragment shader epilogue (value writeout) */
+	if (ctx->stage == MESA_SHADER_FRAGMENT) {
+		if (block == nir_impl_last_block(ctx->func->impl)) {
+			if (ctx->is_blend)
+				emit_blend_epilogue(ctx);
+			else
+				emit_fragment_epilogue(ctx);
+		}
+	}
+
+	/* Fallthrough save */
+	this_block.next_fallthrough = ctx->previous_source_block;
+
+	/* Save the block. Initial and final may be the same. */
+	util_dynarray_append(&ctx->blocks, midgard_block, this_block);
+
+	midgard_block *block_ptr = util_dynarray_top_ptr(&ctx->blocks, midgard_block);
+
+	if (block == nir_start_block(ctx->func->impl))
+		ctx->initial_block = block_ptr;
+
+	if (block == nir_impl_last_block(ctx->func->impl))
+		ctx->final_block = block_ptr;
+
+	/* Allow the next control flow to access us retroactively, for
+	 * branching etc */
+	ctx->current_block = &block_ptr->instructions;
+	
+	/* Document the fallthrough chain */
+	ctx->previous_source_block = block_ptr;
+
+	return block_ptr;
+}
+
+static midgard_block *emit_cf_list(struct compiler_context *ctx, struct exec_list *list);
+
+static void
+emit_if(struct compiler_context *ctx, nir_if *nif)
+{
+	/* Conditional branches expect the condition in r31.w; emit a move for
+	 * that in the _previous_ block (which is the current block). */
+	emit_condition(ctx, &nif->condition, true);
+
+	/* Speculatively emit the branch, but we can't fill it in until later */
+	EMIT(branch, true, true);
+	midgard_instruction *then_branch = util_dynarray_top_ptr(ctx->current_block, midgard_instruction);
+
+	/* Emit the two subblocks */
+	midgard_block *then_block = emit_cf_list(ctx, &nif->then_list);
+
+	/* Emit a jump from the end of the then block to the end of the else */
+	EMIT(branch, false, false);
+	midgard_instruction *then_exit = util_dynarray_top_ptr(ctx->current_block, midgard_instruction);
+
+	/* Emit second block, and check if it's empty */
+
+	int else_idx = ctx->block_count;
+	int count_in = ctx->instruction_count;
+	midgard_block *else_block = emit_cf_list(ctx, &nif->else_list);
+
+	/* Now that we have the subblocks emitted, fix up the branches */
+
+	assert(then_block);
+	assert(else_block);
+
+
+	if (ctx->instruction_count == count_in) {
+		/* The else block is empty, so don't emit an exit jump */
+		then_exit->unused = true;
+		then_branch->branch.target_start = else_idx + 1;
+	} else {
+		then_branch->branch.target_start = else_idx;
+		then_exit->branch.target_start = else_idx + 1;
+	}
+}
+
+static void
+emit_loop(struct compiler_context *ctx, nir_loop *nloop)
+{
+	emit_cf_list(ctx, &nloop->body);
+}
+
+static midgard_block *
+emit_cf_list(struct compiler_context *ctx, struct exec_list *list)
+{
+	midgard_block *start_block = NULL;
+
+	foreach_list_typed(nir_cf_node, node, node, list) {
+		switch (node->type) {
+		case nir_cf_node_block: {
+			midgard_block *block = emit_block(ctx, nir_cf_node_as_block(node));
+
+			if (!start_block)
+				start_block = block;
+
+			break;
+		}
+		case nir_cf_node_if:
+			emit_if(ctx, nir_cf_node_as_if(node));
+			break;
+		case nir_cf_node_loop:
+			emit_loop(ctx, nir_cf_node_as_loop(node));
+			break;
+		case nir_cf_node_function:
+			assert(0);
+			break;
+		}
+	}
+
+	return start_block;
+}
+
+int
+midgard_compile_shader_nir(nir_shader *nir, midgard_program *program, bool is_blend)
+{
+	struct util_dynarray *compiled = &program->compiled;
+
+	compiler_context ictx = {
+		.nir = nir,
+		.stage = nir->info.stage,
+
+		.is_blend = is_blend,
+		.blend_constant_offset = -1,
+	};
+
+	compiler_context *ctx = &ictx;
+
+	/* TODO: Decide this at runtime */
+	ctx->uniform_cutoff = 8;
+
+	switch (ctx->stage) {
+		case MESA_SHADER_VERTEX:
+			ctx->special_uniforms = 1;
+			break;
+		default:
+			ctx->special_uniforms = 0;
+			break;
+	}
+
+	/* Append epilogue uniforms if necessary. The cmdstream depends on
+	 * these being at the -end-; see assign_var_locations. */
+
+        if (ctx->stage == MESA_SHADER_VERTEX) {
+                nir_variable_create(nir, nir_var_uniform, glsl_vec4_type(), "viewport");
+        }
+
+	/* Assign var locations early, so the epilogue can use them if necessary */
+
+	nir_assign_var_locations(&nir->outputs, &nir->num_outputs, glsl_type_size);
+	nir_assign_var_locations(&nir->inputs, &nir->num_inputs, glsl_type_size);
+	nir_assign_var_locations(&nir->uniforms, &nir->num_uniforms, glsl_type_size);
+
+	/* Initialize at a global (not block) level hash tables */
+
+	ctx->ssa_constants = _mesa_hash_table_u64_create(NULL); 
+	ctx->ssa_to_alias = _mesa_hash_table_u64_create(NULL); 
+	ctx->ssa_to_register = _mesa_hash_table_u64_create(NULL); 
+	ctx->hash_to_temp = _mesa_hash_table_u64_create(NULL); 
+	ctx->leftover_ssa_to_alias = _mesa_set_create(NULL, _mesa_hash_pointer, _mesa_key_pointer_equal);
+
+	/* Assign actual uniform location, skipping over samplers */
+
+	ctx->uniform_nir_to_mdg = _mesa_hash_table_u64_create(NULL); 
+
+	nir_foreach_variable(var, &nir->uniforms) {
+		if (glsl_get_base_type(var->type) == GLSL_TYPE_SAMPLER) continue;
+
+		for (int col = 0; col < glsl_get_matrix_columns(var->type); ++col) {
+			int id = ctx->uniform_count++;
+			_mesa_hash_table_u64_insert(ctx->uniform_nir_to_mdg, var->data.driver_location + col + 1, (void *) ((uintptr_t) (id + 1)));
+		}
+	}
+
+	if (ctx->stage == MESA_SHADER_VERTEX) {
+		ctx->varying_nir_to_mdg = _mesa_hash_table_u64_create(NULL); 
+		ctx->varying_count = 1; /* Offset away from gl_Position */
+
+		nir_foreach_variable(var, &nir->outputs) {
+			if (var->data.location < VARYING_SLOT_VAR0) {
+				if (var->data.location == VARYING_SLOT_POS) {
+					printf("Adding pos %d\n", var->data.driver_location + 1);
+					_mesa_hash_table_u64_insert(ctx->varying_nir_to_mdg, var->data.driver_location + 1, (void *) ((uintptr_t) (1)));
+				} else {
+					printf("Skipping unknown\n");
+				}
+				
+				continue;
+			}
+
+			for (int col = 0; col < glsl_get_matrix_columns(var->type); ++col) {
+				int id = ctx->varying_count++;
+				printf("Adding varying %d\n", var->data.driver_location + col + 1);
+				_mesa_hash_table_u64_insert(ctx->varying_nir_to_mdg, var->data.driver_location + col + 1, (void *) ((uintptr_t) (id + 1)));
+			}
+		}
+	}
+
+
+
+	/* Lower vars -- not I/O -- before epilogue */
+
+	NIR_PASS_V(nir, nir_lower_var_copies);
+	NIR_PASS_V(nir, nir_lower_vars_to_ssa);
+	NIR_PASS_V(nir, nir_split_var_copies);
+	NIR_PASS_V(nir, nir_lower_var_copies);
+	NIR_PASS_V(nir, nir_lower_global_vars_to_local);
+	NIR_PASS_V(nir, nir_lower_var_copies);
+	NIR_PASS_V(nir, nir_lower_vars_to_ssa);
+	NIR_PASS_V(nir, nir_lower_io, nir_var_all, glsl_type_size, 0);
+
+	/* Append vertex epilogue before optimisation, so the epilogue itself
+	 * is optimised */
+
+	if (ctx->stage == MESA_SHADER_VERTEX)
+		transform_position_writes(nir);
+
+	/* Optimisation passes */
+
+	optimise_nir(nir);
+
+#ifdef NIR_DEBUG
+	nir_print_shader(nir, stdout);
+#endif
+
+	/* Assign counts, now that we're sure (post-optimisation) */
+	program->uniform_count = nir->num_uniforms;
+
+	program->attribute_count = (ctx->stage == MESA_SHADER_VERTEX) ? nir->num_inputs : 0;
+	program->varying_count = (ctx->stage == MESA_SHADER_VERTEX) ? nir->num_outputs : ((ctx->stage == MESA_SHADER_FRAGMENT) ? nir->num_inputs : 0);
+
+
+	nir_foreach_function(func, nir) {
+		if (!func->impl)
+			continue;
+
+		util_dynarray_init(&ctx->blocks, NULL);
+		ctx->block_count = 0;
+		ctx->func = func;
+
+		emit_cf_list(ctx, &func->impl->body);
+		emit_block(ctx, func->impl->end_block);
+
+		break; /* TODO: Multi-function shaders */
+	}
+
+	util_dynarray_init(compiled, NULL);
+
+	/* Schedule! */
+	schedule_program(ctx);
+
+	/* Now that all the bundles are scheduled and we can calculate block
+	 * sizes, emit actual branch instructions rather than placeholders */
+
+	int br_block_idx = 0;
+
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		util_dynarray_foreach(&block->bundles, midgard_bundle, bundle) {
+			for (int c = 0; c < bundle->instruction_count; ++c) {
+				midgard_instruction *ins = &bundle->instructions[c];
+
+				if (ins->unused) continue;
+				if (ins->unit != ALU_ENAB_BR_COMPACT) continue;
+				if (ins->prepacked_branch) continue;
+
+				uint16_t compact;
+
+				/* Determine the block we're jumping to */
+				int target_number = ins->branch.target_start;
+
+				midgard_block *target = util_dynarray_element(&ctx->blocks, midgard_block, target_number);
+				assert(target);
+
+				/* Determine the destination tag */
+				midgard_bundle *first = util_dynarray_element(&target->bundles, midgard_bundle, 0);
+				assert(first);
+
+				int dest_tag = first->tag;
+
+				/* Count up the number of quadwords we're jumping over. That is, the number of quadwords in each of the blocks between (br_block_idx, target_number) */
+				assert(target_number > br_block_idx); /* TODO: Jumps backwards */
+
+				int quadword_offset = 0;
+
+				for (int idx = br_block_idx + 1; idx < target_number; ++idx) {
+					midgard_block *blk = util_dynarray_element(&ctx->blocks, midgard_block, idx);
+					assert(blk);
+					
+					quadword_offset += blk->quadword_count;
+				}
+
+				if (ins->branch.conditional) {
+					midgard_branch_cond branch = {
+						.op = midgard_jmp_writeout_op_branch_cond,
+						.dest_tag = dest_tag,
+						.offset = quadword_offset,
+						.cond = ins->branch.invert_conditional ? midgard_condition_false : midgard_condition_true
+					};
+
+					memcpy(&compact, &branch, sizeof(branch));
+				} else {
+					midgard_branch_uncond branch = {
+						.op = midgard_jmp_writeout_op_branch_uncond,
+						.dest_tag = dest_tag, 
+						.offset = quadword_offset, 
+						.unknown = 1
+					};
+
+					memcpy(&compact, &branch, sizeof(branch));
+				}
+
+				/* Swap in the generic branch for our actual branch */
+				ins->unit = ALU_ENAB_BR_COMPACT;
+				ins->br_compact = compact;
+			}
+
+		}
+
+		++br_block_idx;
+	}
+
+	/* Emit flat binary from the instruction arrays. Iterate each block in
+	 * sequence. Save instruction boundaries such that lookahead tags can
+	 * be assigned easily */
+
+	/* Cache _all_ bundles in source order for lookahead across failed branches */
+
+	int bundle_count = 0;
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		bundle_count += block->bundles.size / sizeof(midgard_bundle);
+	}
+	midgard_bundle **source_order_bundles = malloc(sizeof(midgard_bundle*) * bundle_count);
+	int bundle_idx = 0;
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		util_dynarray_foreach(&block->bundles, midgard_bundle, bundle) {
+			source_order_bundles[bundle_idx++] = bundle;
+		}
+	}
+
+	int current_bundle = 0;
+
+	util_dynarray_foreach(&ctx->blocks, midgard_block, block) {
+		util_dynarray_foreach(&block->bundles, midgard_bundle, bundle) {
+			int lookahead = 1;
+
+			if (current_bundle + 1 < bundle_count) {
+				uint8_t next = source_order_bundles[current_bundle + 1]->tag;
+
+				if (!(current_bundle + 2 < bundle_count) && IS_ALU(next)) {
+					lookahead = 1;
+				} else {
+					lookahead = next;
+				}
+			}
+
+			emit_binary_bundle(ctx, bundle, compiled, lookahead);
+			++current_bundle;
+		}
+
+		/* TODO: Free deeper */
+		util_dynarray_fini(&block->instructions);
+	}
+
+	free(source_order_bundles);
+
+	/* Due to lookahead, we need to report in the command stream the first tag executed */
+
+	midgard_block *initial_block = util_dynarray_element(&ctx->blocks, midgard_block, 0);
+	midgard_bundle *initial_bundle = util_dynarray_element(&initial_block->bundles, midgard_bundle, 0);
+	program->first_tag = initial_bundle->tag;
+
+	/* Deal with off-by-one related to the fencepost problem */
+	program->work_register_count = ctx->work_registers + 1;
+
+	program->can_discard = ctx->can_discard;
+	program->uniform_cutoff = ctx->uniform_cutoff;
+
+	program->blend_patch_offset = ctx->blend_constant_offset;
+	printf("Patch %d\n", program->blend_patch_offset);
+
+#ifdef MDG_DEBUG
+	disassemble_midgard(program->compiled.data, program->compiled.size);
+#endif
+
+	return 0;
+}
diff --git a/src/gallium/drivers/panfrost/midgard/midgard_compile.h b/src/gallium/drivers/panfrost/midgard/midgard_compile.h
new file mode 100644
index 0000000..3a111f8
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/midgard_compile.h
@@ -0,0 +1,76 @@
+/*
+ * Copyright (C) 2018 Alyssa Rosenzweig <alyssa@rosenzweig.io>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+
+#include "compiler/nir/nir.h"
+#include "util/u_dynarray.h"
+
+/* Define the general compiler entry point */
+
+typedef struct {
+	int work_register_count;
+	int uniform_count;
+	int uniform_cutoff;
+
+	int attribute_count;
+	int varying_count;
+
+	bool can_discard;
+
+	int first_tag;
+
+	struct util_dynarray compiled;
+
+	/* For a blend shader using a constant color -- patch point. If
+	 * negative, there's no constant. */
+
+	int blend_patch_offset;
+} midgard_program;
+
+int
+midgard_compile_shader_nir(nir_shader *nir, midgard_program *program, bool is_blend);
+
+/* NIR options are shared between the standalone compiler and the online
+ * compiler. Defining it here is the simplest, though maybe not the Right
+ * solution. */
+
+static const nir_shader_compiler_options midgard_nir_options = {
+	.lower_ffma = true,
+	.lower_sub = true,
+	.lower_fpow = true,
+	.lower_scmp = true,
+	.lower_flrp32 = true,
+	.lower_flrp64 = true,
+	.lower_ffract = true,
+	.lower_fmod32 = true,
+	.lower_fmod64 = true,
+	.lower_fdiv = true,
+	.lower_idiv = true,
+	.lower_b2f = true,
+
+	.vertex_id_zero_based = true,
+	.lower_extract_byte = true,
+	.lower_extract_word = true,
+
+	.native_integers = true
+};
diff --git a/src/gallium/drivers/panfrost/midgard/midgard_nir.h b/src/gallium/drivers/panfrost/midgard/midgard_nir.h
new file mode 100644
index 0000000..b7a2298
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/midgard_nir.h
@@ -0,0 +1,5 @@
+#include <stdbool.h>
+#include "nir.h"
+
+bool midgard_nir_lower_algebraic(nir_shader *shader);
+bool midgard_nir_scale_trig(nir_shader *shader);
diff --git a/src/gallium/drivers/panfrost/midgard/midgard_nir_algebraic.py b/src/gallium/drivers/panfrost/midgard/midgard_nir_algebraic.py
new file mode 100644
index 0000000..56fe93c
--- /dev/null
+++ b/src/gallium/drivers/panfrost/midgard/midgard_nir_algebraic.py
@@ -0,0 +1,71 @@
+#
+# Copyright (C) 2016 Intel Corporation
+#
+# Permission is hereby granted, free of charge, to any person obtaining a
+# copy of this software and associated documentation files (the "Software"),
+# to deal in the Software without restriction, including without limitation
+# the rights to use, copy, modify, merge, publish, distribute, sublicense,
+# and/or sell copies of the Software, and to permit persons to whom the
+# Software is furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice (including the next
+# paragraph) shall be included in all copies or substantial portions of the
+# Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+# IN THE SOFTWARE.
+
+import argparse
+import sys
+import math
+
+a = 'a'
+b = 'b'
+
+algebraic = [
+    (('f2b', a), ('fne', a, 0.0)),
+    (('isign', a), ('imin', ('imax', a, -1), 1)),
+    (('fge', a, b), ('flt', b, a)),
+
+    # XXX: We have hw ops for this, just unknown atm..
+    #(('fsign@32', a), ('i2f32@32', ('isign', ('f2i32@32', ('fmul', a, 0x43800000)))))
+    #(('fsign', a), ('fcsel', ('fge', a, 0), 1.0, ('fcsel', ('flt', a, 0.0), -1.0, 0.0)))
+    (('fsign', a), ('bcsel', ('fge', a, 0), 1.0, -1.0)),
+
+    (('b2i@32', a), ('iand', a, 1))
+]
+
+# Midgard scales fsin/fcos arguments by pi.
+# Pass must be run only once, after the main loop
+
+scale_trig = [
+        (('fsin', a), ('fsin', ('fdiv', a, math.pi))),
+        (('fcos', a), ('fcos', ('fdiv', a, math.pi))),
+]
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument('-p', '--import-path', required=True)
+    args = parser.parse_args()
+    sys.path.insert(0, args.import_path)
+    run()
+
+
+def run():
+    import nir_algebraic  # pylint: disable=import-error
+
+    print('#include "midgard_nir.h"')
+    print(nir_algebraic.AlgebraicPass("midgard_nir_lower_algebraic",
+                                      algebraic).render())
+
+    print(nir_algebraic.AlgebraicPass("midgard_nir_scale_trig",
+                                      scale_trig).render())
+
+
+if __name__ == '__main__':
+    main()
diff --git a/src/gallium/drivers/panfrost/pan_allocate.c b/src/gallium/drivers/panfrost/pan_allocate.c
new file mode 100644
index 0000000..91651a6
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_allocate.c
@@ -0,0 +1,124 @@
+/*
+ * © Copyright 2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <assert.h>
+#include "pan_nondrm.h"
+
+/* TODO: What does this actually have to be? */
+#define ALIGNMENT 128
+
+// TODO: An actual allocator, perhaps
+// TODO: Multiple stacks for multiple bases?
+
+int hack_stack_bottom = 4096; /* Don't interfere with constant offsets */
+int last_offset = 0;
+
+static inline int
+pandev_allocate_offset(int *stack, size_t sz)
+{
+	/* First, align the stack bottom to something nice; it's not critical
+	 * at this point if we waste a little space to do so. */
+
+	int excess = *stack & (ALIGNMENT - 1);
+
+	/* Add the secret of my */
+	if (excess)
+		*stack += ALIGNMENT - excess;
+
+	/* Finally, use the new bottom for the allocation and move down the
+	 * stack */
+
+	int ret = *stack;
+	*stack += sz;
+	return ret;
+}
+
+inline mali_ptr
+pandev_upload(int cheating_offset, int *stack_bottom, mali_ptr base, void *base_map, const void *data, size_t sz, bool no_pad)
+{
+	int offset;
+
+	/* We're not positive about the sizes of all objects, but we don't want
+	 * them to crash against each other either. Let the caller disable
+	 * padding if they so choose, though. */
+
+	size_t padded_size = no_pad ? sz : sz * 2;
+
+	/* If no specific bottom is specified, use a global one... don't do
+	 * this in production, kids */
+
+	if (!stack_bottom)
+		stack_bottom = &hack_stack_bottom;
+
+	/* Allocate space for the new GPU object, if required */
+
+	if (cheating_offset == -1) {
+		offset = pandev_allocate_offset(stack_bottom, padded_size);
+	} else {
+		offset = cheating_offset;
+		*stack_bottom = offset + sz;
+	}
+
+	/* Save last offset for sequential uploads (job descriptors) */
+	last_offset = offset + padded_size;
+
+	/* Upload it */
+	memcpy((uint8_t*) base_map + offset, data, sz);
+
+	/* Return the GPU address */
+	return base + offset;
+}
+
+/* Upload immediately after the last allocation */
+
+mali_ptr
+pandev_upload_sequential(mali_ptr base, void *base_map, const void *data, size_t sz) {
+	return pandev_upload(last_offset, NULL, base, base_map, data, sz, /* false */ true);
+}
+
+/* Simplified APIs for the real driver, rather than replays */
+
+mali_ptr
+panfrost_upload(struct panfrost_memory *mem, const void *data, size_t sz, bool no_pad)
+{
+	/* Bounds check */
+	if ((mem->stack_bottom + sz) >= mem->size) {
+		printf("Out of memory, tried to upload %zd but only %zd available\n", sz, mem->size - mem->stack_bottom);
+		assert(0);
+	}
+
+	return pandev_upload(-1, &mem->stack_bottom, mem->gpu, mem->cpu, data, sz, no_pad);
+}
+
+mali_ptr
+panfrost_upload_sequential(struct panfrost_memory *mem, const void *data, size_t sz)
+{
+	return pandev_upload(last_offset, &mem->stack_bottom, mem->gpu, mem->cpu, data, sz, true);
+}
+
+/* Simplified interface to allocate a chunk without any upload, to allow
+ * zero-copy uploads. This is particularly useful when the copy would happen
+ * anyway, for instance with texture swizzling. */
+
+void *
+panfrost_allocate_transfer(struct panfrost_memory *mem, size_t sz, mali_ptr *gpu)
+{
+	int offset = pandev_allocate_offset(&mem->stack_bottom, sz);
+	
+	*gpu = mem->gpu + offset;
+	return mem->cpu + offset;
+}
diff --git a/src/gallium/drivers/panfrost/pan_assemble.c b/src/gallium/drivers/panfrost/pan_assemble.c
new file mode 100644
index 0000000..e76c8cd
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_assemble.c
@@ -0,0 +1,170 @@
+/*
+ * © Copyright 2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include "pan_nondrm.h"
+#include "pan_context.h"
+
+#include "compiler/nir/nir.h"
+#include "nir/tgsi_to_nir.h"
+#include "midgard/midgard_compile.h"
+#include "util/u_dynarray.h"
+
+#include "tgsi/tgsi_dump.h"
+
+void
+panfrost_shader_compile(struct panfrost_context *ctx, struct mali_shader_meta *meta, const char *src, int type, struct panfrost_shader_state *state)
+{
+	uint8_t* dst;
+
+	nir_shader *s;
+
+	struct pipe_shader_state* cso = &state->base;
+	
+	if (cso->type == PIPE_SHADER_IR_NIR) {
+		s = cso->ir.nir;
+	} else {
+		assert (cso->type == PIPE_SHADER_IR_TGSI);
+		//tgsi_dump(cso->tokens, 0);
+		s = tgsi_to_nir(cso->tokens, &midgard_nir_options);
+	}
+
+	s->info.stage = type == JOB_TYPE_VERTEX ? MESA_SHADER_VERTEX : MESA_SHADER_FRAGMENT;
+
+	/* Call out to Midgard compiler given the above NIR */
+
+	midgard_program program;
+	midgard_compile_shader_nir(s, &program, false);
+
+	/* Prepare the compiled binary for upload */
+	int size = program.compiled.size;
+	dst = program.compiled.data;
+
+	/* Inject an external shader */
+#if 0
+	char buf[4096];
+	if (type != JOB_TYPE_VERTEX) {
+		FILE *fp = fopen("/home/alyssa/panfrost/midgard/good.bin", "rb");
+		fread(buf, 1, 2816, fp);
+		fclose(fp);
+		dst= buf;
+		size = 2816;
+	}
+#endif
+
+	/* Upload the shader. The lookahead tag is ORed on as a tagged pointer.
+	 * I bet someone just thought that would be a cute pun. At least,
+	 * that's how I'd do it. */
+
+	meta->shader = panfrost_upload(&ctx->shaders, dst, size, true) | program.first_tag;
+
+	util_dynarray_fini(&program.compiled);
+
+	meta->midgard1.uniform_count = MIN2(program.uniform_count, program.uniform_cutoff);
+	meta->attribute_count = program.attribute_count;
+	meta->varying_count = program.varying_count + 2;
+	meta->midgard1.work_count = program.work_register_count;
+
+	state->can_discard = program.can_discard;
+
+	/* Separate as primary uniform count is truncated */
+	state->uniform_count = program.uniform_count;
+
+	/* gl_Position eats up an extra spot */
+	if (type == JOB_TYPE_VERTEX)
+		meta->varying_count += 1;
+
+	/* gl_FragCoord does -not- eat an extra spot; it will be included in our count if we need it */
+
+
+    meta->midgard1.unknown2 = 8; /* XXX */
+
+    /* Varyings are known only through the shader. We choose to upload this
+     * information with the vertex shader, though the choice is perhaps
+     * arbitrary */
+
+    if (type == JOB_TYPE_VERTEX) {
+	    struct panfrost_varyings *varyings = &state->varyings;
+
+	    /* Measured in vec4 words. Don't include gl_Position */
+	    int varying_count = program.varying_count;
+
+	    /* Setup two buffers, one for position, the other for normal
+	     * varyings, as seen in traces. TODO: Are there other
+	     * configurations we might use? */
+
+	    varyings->varying_buffer_count = 2;
+
+	    /* mediump vec4s sequentially */
+	    varyings->varyings_stride[0] = (2 * sizeof(float)) * varying_count;
+
+	    /* highp gl_Position */
+	    varyings->varyings_stride[1] = 4 * sizeof(float);
+
+	    /* Setup gl_Position and its weirdo analogue */
+
+	    struct mali_attr_meta position_meta = {
+		    .index = 1,
+		    .type = 6, /* gl_Position */
+		    .nr_components = MALI_POSITIVE(4),
+		    .is_int_signed = 1,
+		    .unknown1 = 0x1a22
+	    };
+
+	    struct mali_attr_meta position_meta_prime = {
+		    .index = 0,
+		    .type = 7, /* float */
+		    .nr_components = MALI_POSITIVE(4),
+		    .is_int_signed = 1,
+		    .unknown1 = 0x2490
+	    };
+
+	    varyings->vertex_only_varyings[0] = position_meta;
+	    varyings->vertex_only_varyings[1] = position_meta_prime;
+
+	    /* Setup actual varyings. XXX: Don't assume vec4 */
+
+	    for (int i = 0; i < varying_count; ++i) {
+		    struct mali_attr_meta vec4_varying_meta = {
+			    .index = 0,
+			    .type = 7, /* float */
+			    .nr_components = MALI_POSITIVE(4),
+			    .not_normalised = 1,
+			    .unknown1 = 0x1a22,
+
+			    /* mediump => half-floats */
+			    .is_int_signed = 1,
+
+			    /* Set offset to keep everything back-to-back in
+			     * the same buffer */
+			    .src_offset = 8 * i,
+#ifdef T6XX
+			    .unknown2 = 1,
+#endif
+		    };
+
+		    varyings->varyings[i] = vec4_varying_meta;
+	    }
+
+	    /* In this context, position_meta represents the implicit
+	     * gl_FragCoord varying */
+
+	    varyings->fragment_only_varyings[0] = position_meta;
+	    varyings->fragment_only_varying_count = 1;
+
+	    varyings->varying_count = varying_count - 1;
+    }
+}
diff --git a/src/gallium/drivers/panfrost/pan_blend_shaders.c b/src/gallium/drivers/panfrost/pan_blend_shaders.c
new file mode 100644
index 0000000..91dae5d
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_blend_shaders.c
@@ -0,0 +1,107 @@
+/*
+ * © Copyright 2018 Alyssa Rosenzweig
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#include <stdio.h>
+#include "pan_blend_shaders.h"
+#include "midgard/midgard_compile.h"
+
+/*
+ * Implements the command stream portion of programmatic blend shaders.
+ *
+ * On Midgard, common blending operations are accelerated by the fixed-function
+ * blending pipeline. Panfrost supports this fast path via the code in
+ * pan_blending.c. Nevertheless, uncommon blend modes (including some seemingly
+ * simple modes present in ES2) require "blend shaders", a special internal
+ * shader type used for programmable blending.
+ *
+ * Blend shaders operate during the normal blending time, but they bypass the
+ * fixed-function blending pipeline and instead go straight to the Midgard
+ * shader cores. The shaders themselves are essentially just fragment shaders,
+ * making heavy use of uint8 arithmetic to manipulate RGB values for the
+ * framebuffer.
+ *
+ * As is typical with Midgard, shader binaries must be accompanied by
+ * information about the first tag (ORed with the bottom nibble of address,
+ * like usual) and work registers. Work register count is specified in the
+ * blend descriptor, as well as in the coresponding fragment shader's work
+ * count. This suggests that blend shader invocation is tied to fragment shader
+ * execution.
+ * 
+ * ---
+ *
+ * As for blend shaders, they use the standard ISA. 
+ *
+ * The source pixel colour, including alpha, is preloaded into r0 as a vec4 of
+ * float32.
+ *
+ * The destination pixel colour must be loaded explicitly via load/store ops.
+ * TODO: Investigate.
+ *
+ * They use fragment shader writeout; however, instead of writing a vec4 of
+ * float32 for RGBA encoding, we writeout a vec4 of uint8, using 8-bit imov
+ * instead of 32-bit fmov. The net result is that r0 encodes a single uint32
+ * containing all four channels of the color.  Accordingly, the blend shader
+ * epilogue has to scale all four channels by 255 and then type convert to a
+ * uint8.
+ *
+ * ---
+ *
+ * Blend shaders hardcode constants. Naively, this requires recompilation each
+ * time the blend color changes, which is a performance risk. Accordingly, we
+ * 'cheat' a bit: instead of loading the constant, we compile a shader with a
+ * dummy constant, exporting the offset to the immediate in the shader binary,
+ * storing this generic binary and metadata in the CSO itself at CSO create
+ * time.
+ *
+ * We then hot patch in the color into this shader at attachment / color change
+ * time, allowing for CSO create to be the only expensive operation
+ * (compilation).
+ */ 
+
+void
+panfrost_make_blend_shader(struct panfrost_context *ctx, struct panfrost_blend_state *cso, const struct pipe_blend_color *blend_color)
+{
+	//const struct pipe_rt_blend_state *blend = &cso->base.rt[0];
+	mali_ptr *out = &cso->blend_shader;
+
+	/* Upload the shader */
+	midgard_program program = {
+		.work_register_count = 3,
+		.first_tag = 9,
+		.blend_patch_offset = 16
+		//.blend_patch_offset = -1,
+	};
+
+	char dst[4096];
+
+	FILE *fp = fopen("/home/alyssa/panfrost/midgard/blend.bin", "rb");
+	fread(dst, 1, 2816, fp);
+	fclose(fp);
+	int size = 2816;
+
+	/* Hot patch in constant color */
+
+	if (program.blend_patch_offset >= 0) {
+		float *hot_color = (float *) (dst + program.blend_patch_offset);
+
+		for (int c = 0; c < 4; ++c)
+			hot_color[c] = blend_color->color[c];
+	}
+	
+	*out = panfrost_upload(&ctx->shaders, dst, size, true) | program.first_tag;
+
+	/* We need to switch to shader mode */
+	cso->has_blend_shader = true;
+	cso->blend_work_count = program.work_register_count;
+}
diff --git a/src/gallium/drivers/panfrost/pan_blend_shaders.h b/src/gallium/drivers/panfrost/pan_blend_shaders.h
new file mode 100644
index 0000000..590a2f7
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_blend_shaders.h
@@ -0,0 +1,26 @@
+/*
+ * © Copyright 2018 Alyssa Rosenzweig
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __PAN_BLEND_SHADERS_H__
+#define __PAN_BLEND_SHADERS_H__
+
+#include "pipe/p_state.h"
+#include "pipe/p_defines.h"
+#include <mali-job.h>
+#include "pan_context.h"
+
+void
+panfrost_make_blend_shader(struct panfrost_context *ctx, struct panfrost_blend_state *cso, const struct pipe_blend_color *blend_color);
+
+#endif
diff --git a/src/gallium/drivers/panfrost/pan_blending.c b/src/gallium/drivers/panfrost/pan_blending.c
new file mode 100644
index 0000000..aeaf04a
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_blending.c
@@ -0,0 +1,463 @@
+/*
+ * © Copyright 2018 Alyssa Rosenzweig
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#include <stdio.h>
+#include "pan_blending.h"
+
+/* 
+ * Implements fixed-function blending on Midgard.
+ *
+ * Midgard splits blending into a fixed-function fast path and a programmable
+ * slow path. The fixed function blending architecture is based on "dominant"
+ * blend factors. Blending is encoded separately (but identically) between RGB
+ * and alpha functions.
+ *
+ * Essentially, for a given blending operation, there is a single dominant
+ * factor. The following dominant factors are possible:
+ *
+ * 	- zero
+ * 	- source color
+ * 	- destination color
+ * 	- source alpha
+ * 	- destination alpha
+ * 	- constant float
+ *
+ * Further, a dominant factor's arithmetic compliment could be used. For
+ * instance, to encode GL_ONE_MINUS_SOURCE_ALPHA, the dominant factor would be
+ * MALI_DOMINANT_SRC_ALPHA with the complement_dominant bit set.
+ *
+ * A single constant float can be passed to the fixed-function hardware,
+ * allowing CONSTANT_ALPHA support. Further, if all components of the constant
+ * glBlendColor are identical, CONSTANT_COLOR can be implemented with the
+ * constant float mode. If the components differ, programmable blending is
+ * required.
+ *
+ * The nondominant factor can be either:
+ *
+ * 	- the same as the dominant factor (MALI_BLEND_NON_MIRROR)
+ * 	- zero (MALI_BLEND_NON_ZERO)
+ * 
+ * Exactly one of the blend operation's source or destination can be used as
+ * the dominant factor; this is selected by the
+ * MALI_BLEND_DOM_SOURCE/DESTINATION flag. 
+ *
+ * By default, all blending follows the standard OpenGL addition equation:
+ * 	
+ * 	out = source_value * source_factor + destination_value * destination_factor
+ *
+ * By setting the negate_source or negate_dest bits, other blend functions can
+ * be created. For instance, for SUBTRACT mode, set the "negate destination"
+ * flag, and similarly for REVERSE_SUBTRACT with "negate source".
+ *
+ * Finally, there is a "clip modifier" controlling the final blending
+ * behaviour, allowing for the following modes:
+ * 	
+ * 	- normal
+ * 	- force source factor to one (MALI_BLEND_MODE_SOURCE_ONE)
+ * 	- force destination factor to one (MALI_BLEND_MODE_DEST_ONE)
+ *
+ * The clipping flags can be used to encode blend modes where the nondominant
+ * factor is ONE.
+ *
+ * As an example putting it all together, to encode the following blend state:
+ *
+ * 	glBlendEquation(GL_FUNC_REVERSE_SUBTRACT);
+ * 	glBlendFunc(GL_ONE_MINUS_SRC_ALPHA, GL_ONE);
+ *
+ * We need the following configuration:
+ * 	
+ * 	- negate source (for REVERSE_SUBTRACT)
+ * 	- dominant factor "source alpha"
+ * 		- compliment dominant
+ * 		- source dominant
+ * 	- force destination to ONE
+ *
+ * The following routines implement this fixed function blending encoding
+ */
+
+/* Helper to find the uncomplemented Gallium blend factor corresponding to a
+ * complemented Gallium blend factor */
+
+static int
+complement_factor(int factor)
+{
+	switch (factor) {
+		case PIPE_BLENDFACTOR_INV_SRC_COLOR: 
+			return PIPE_BLENDFACTOR_SRC_COLOR;
+		case PIPE_BLENDFACTOR_INV_SRC_ALPHA: 
+			return PIPE_BLENDFACTOR_SRC_ALPHA;
+		case PIPE_BLENDFACTOR_INV_DST_ALPHA: 
+			return PIPE_BLENDFACTOR_DST_ALPHA;
+		case PIPE_BLENDFACTOR_INV_DST_COLOR: 
+			return PIPE_BLENDFACTOR_DST_COLOR;
+		case PIPE_BLENDFACTOR_INV_CONST_COLOR: 
+			return PIPE_BLENDFACTOR_CONST_COLOR;
+		case PIPE_BLENDFACTOR_INV_CONST_ALPHA: 
+			return PIPE_BLENDFACTOR_CONST_ALPHA;
+		default:
+			return -1;
+	}
+}
+
+/* Helper to strip the complement from any Gallium blend factor */
+
+static int
+uncomplement_factor(int factor) {
+	int complement = complement_factor(factor);
+	return (complement == -1) ? factor : complement;
+}
+
+
+/* Attempt to find the dominant factor given a particular factor, complementing
+ * as necessary */
+
+static bool
+panfrost_make_dominant_factor(unsigned src_factor, enum mali_dominant_factor *factor, bool *invert)
+{
+	switch (src_factor) {
+		case PIPE_BLENDFACTOR_SRC_COLOR: 
+		case PIPE_BLENDFACTOR_INV_SRC_COLOR: 
+			*factor = MALI_DOMINANT_SRC_COLOR;
+			break;
+
+		case PIPE_BLENDFACTOR_SRC_ALPHA: 
+		case PIPE_BLENDFACTOR_INV_SRC_ALPHA: 
+			*factor = MALI_DOMINANT_SRC_ALPHA;
+			break;
+
+		case PIPE_BLENDFACTOR_DST_COLOR: 
+		case PIPE_BLENDFACTOR_INV_DST_COLOR: 
+			*factor = MALI_DOMINANT_DST_COLOR;
+			break;
+
+		case PIPE_BLENDFACTOR_DST_ALPHA: 
+		case PIPE_BLENDFACTOR_INV_DST_ALPHA: 
+			*factor = MALI_DOMINANT_DST_ALPHA;
+			break;
+
+		case PIPE_BLENDFACTOR_ONE: 
+		case PIPE_BLENDFACTOR_ZERO: 
+			*factor = MALI_DOMINANT_ZERO;
+			break;
+
+		case PIPE_BLENDFACTOR_CONST_ALPHA:
+		case PIPE_BLENDFACTOR_INV_CONST_ALPHA:
+		case PIPE_BLENDFACTOR_CONST_COLOR:
+		case PIPE_BLENDFACTOR_INV_CONST_COLOR:
+			*factor = MALI_DOMINANT_CONSTANT;
+			break;
+
+		default:
+			/* Fancy blend modes not supported */
+			return false;
+	}
+
+	/* Set invert flags */
+
+	switch (src_factor) {
+		case PIPE_BLENDFACTOR_ONE:
+		case PIPE_BLENDFACTOR_INV_SRC_COLOR: 
+		case PIPE_BLENDFACTOR_INV_SRC_ALPHA: 
+		case PIPE_BLENDFACTOR_INV_DST_ALPHA: 
+		case PIPE_BLENDFACTOR_INV_DST_COLOR: 
+		case PIPE_BLENDFACTOR_INV_CONST_ALPHA: 
+		case PIPE_BLENDFACTOR_INV_CONST_COLOR: 
+		case PIPE_BLENDFACTOR_INV_SRC1_COLOR: 
+		case PIPE_BLENDFACTOR_INV_SRC1_ALPHA: 
+			*invert = true;
+		default:
+			break;
+	}
+	
+	return true;
+}
+
+/* Check if this is a special edge case blend factor, which may require the use
+ * of clip modifiers */
+
+static bool
+is_edge_blendfactor(unsigned factor) {
+	return factor == PIPE_BLENDFACTOR_ONE || factor == PIPE_BLENDFACTOR_ZERO;
+}
+
+/* Helper to dump fixed-function blend part for debugging */
+
+static const char *
+panfrost_factor_name(enum mali_dominant_factor factor)
+{
+	switch (factor) {
+		case MALI_DOMINANT_UNK0: 	return "unk0";
+		case MALI_DOMINANT_ZERO: 	return "zero";
+		case MALI_DOMINANT_SRC_COLOR: 	return "source color";
+		case MALI_DOMINANT_DST_COLOR: 	return "dest color";
+		case MALI_DOMINANT_UNK4: 	return "unk4";
+		case MALI_DOMINANT_SRC_ALPHA: 	return "source alpha";
+		case MALI_DOMINANT_DST_ALPHA: 	return "dest alpha";
+		case MALI_DOMINANT_CONSTANT: 	return "constant";
+	}
+
+	return "unreachable";
+}
+
+static const char *
+panfrost_modifier_name(enum mali_blend_modifier mod)
+{
+	switch (mod) {
+		case MALI_BLEND_MOD_UNK0: 	return "unk0";
+		case MALI_BLEND_MOD_NORMAL: 	return "normal";
+		case MALI_BLEND_MOD_SOURCE_ONE: return "source one";
+		case MALI_BLEND_MOD_DEST_ONE: 	return "dest one";
+	}
+
+	return "unreachable";
+}
+
+static void
+panfrost_print_fixed_part(const char *name, unsigned u)
+{
+	struct mali_blend_mode part;
+	memcpy(&part, &u, sizeof(part));
+
+	printf("%s blend mode (%X):\n", name, u);
+
+	printf(" %s dominant:\n", 
+			(part.dominant == MALI_BLEND_DOM_SOURCE) ? "source" : "destination");
+
+	printf("   %s\n", panfrost_factor_name(part.dominant_factor));
+
+	if (part.complement_dominant)
+		printf("   complement\n");
+
+	
+	printf(" nondominant %s\n",
+			(part.nondominant_mode == MALI_BLEND_NON_MIRROR) ? "mirror" : "zero");
+
+
+	printf(" mode: %s\n", panfrost_modifier_name(part.clip_modifier));
+
+	if (part.negate_source) printf(" negate source\n");
+	if (part.negate_dest) printf(" negate dest\n");
+
+	assert(!(part.unused_0 || part.unused_1));
+}
+
+static void
+panfrost_print_blend_equation(struct mali_blend_equation eq)
+{
+	printf("\n");
+	panfrost_print_fixed_part("RGB", eq.rgb_mode);
+	panfrost_print_fixed_part("Alpha", eq.alpha_mode);
+
+	assert(!eq.zero1);
+
+	printf("Mask: %s%s%s%s\n",
+			(eq.color_mask & MALI_MASK_R) ? "R" : "",
+			(eq.color_mask & MALI_MASK_G) ? "G" : "",
+			(eq.color_mask & MALI_MASK_B) ? "B" : "",
+			(eq.color_mask & MALI_MASK_A) ? "A" : "");
+
+	printf("Constant: %f\n", eq.constant);
+}
+
+/* Perform the actual fixed function encoding. Encode the function with negate
+ * bits. Check for various cases to work out the dominant/nondominant split and
+ * accompanying flags. */
+
+static bool
+panfrost_make_fixed_blend_part(unsigned func, unsigned src_factor, unsigned dst_factor, unsigned *out)
+{
+	struct mali_blend_mode part = { 0 };
+
+	/* Make sure that the blend function is representible with negate flags */
+
+	if (func == PIPE_BLEND_ADD) {
+		/* Default, no modifiers needed */
+	} else if (func == PIPE_BLEND_SUBTRACT)
+		part.negate_dest = true;
+	else if (func == PIPE_BLEND_REVERSE_SUBTRACT)
+		part.negate_source = true;
+	else
+		return false;
+
+	part.clip_modifier = MALI_BLEND_MOD_NORMAL;
+
+	/* Decide which is dominant, source or destination. If one is an edge
+	 * case, use the other as a factor. If they're the same, it doesn't
+	 * matter; we just mirror. If they're different non-edge-cases, you
+	 * need a blend shader (don't do that). */
+	
+	if (is_edge_blendfactor(dst_factor)) {
+		part.dominant = MALI_BLEND_DOM_SOURCE;
+		part.nondominant_mode = MALI_BLEND_NON_ZERO;
+
+		if (dst_factor == PIPE_BLENDFACTOR_ONE)
+			part.clip_modifier = MALI_BLEND_MOD_DEST_ONE;
+	} else if (is_edge_blendfactor(src_factor)) {
+		part.dominant = MALI_BLEND_DOM_DESTINATION;
+		part.nondominant_mode = MALI_BLEND_NON_ZERO;
+
+		if (src_factor == PIPE_BLENDFACTOR_ONE)
+			part.clip_modifier = MALI_BLEND_MOD_SOURCE_ONE;
+
+	} else if (src_factor == dst_factor) {
+		part.dominant = MALI_BLEND_DOM_DESTINATION; /* Ought to be an arbitrary choice, but we need to set destination for some reason? Align with the blob until we understand more */
+		part.nondominant_mode = MALI_BLEND_NON_MIRROR;
+	} else if (src_factor == complement_factor(dst_factor)) {
+		/* TODO: How does this work exactly? */
+		part.dominant = MALI_BLEND_DOM_SOURCE;
+		part.nondominant_mode = MALI_BLEND_NON_MIRROR;
+		part.clip_modifier = MALI_BLEND_MOD_DEST_ONE;
+	} else if (dst_factor == complement_factor(src_factor)) {
+		part.dominant = MALI_BLEND_DOM_SOURCE;
+		part.nondominant_mode = MALI_BLEND_NON_MIRROR;
+		part.clip_modifier = /*MALI_BLEND_MOD_SOURCE_ONE*/MALI_BLEND_MOD_DEST_ONE; /* Which modifier should it be? */
+	} else {
+		printf("Failed to find dominant factor?\n");
+		return false;
+	}
+
+	unsigned in_dominant_factor =
+		part.dominant == MALI_BLEND_DOM_SOURCE ? src_factor : dst_factor;
+
+	if (part.clip_modifier == MALI_BLEND_MOD_NORMAL && in_dominant_factor == PIPE_BLENDFACTOR_ONE) {
+		part.clip_modifier = part.dominant == MALI_BLEND_DOM_SOURCE ? MALI_BLEND_MOD_SOURCE_ONE : MALI_BLEND_MOD_DEST_ONE;
+		in_dominant_factor = PIPE_BLENDFACTOR_ZERO;
+	}
+
+	bool invert_dominant = false;
+	enum mali_dominant_factor dominant_factor;
+
+	if (!panfrost_make_dominant_factor(in_dominant_factor, &dominant_factor, &invert_dominant))
+		return false;
+
+	part.dominant_factor = dominant_factor;
+	part.complement_dominant = invert_dominant;
+
+	/* Write out mode */
+	memcpy(out, &part, sizeof(part));
+
+	return true;
+}
+
+/* We can upload a single constant for all of the factors. So, scan the factors
+ * for constants used, and scan the constants for the constants used. If there
+ * is a single unique constant, output that. If there are multiple,
+ * fixed-function operation breaks down. */
+
+static bool
+panfrost_make_constant(unsigned *factors, unsigned num_factors, const struct pipe_blend_color *blend_color, float *out)
+{
+	/* Color components used */
+	bool cc[4] = { false };
+
+	for (unsigned i = 0; i < num_factors; ++i) {
+		unsigned factor = uncomplement_factor(factors[i]);
+
+		if (factor == PIPE_BLENDFACTOR_CONST_COLOR)
+			cc[0] = cc[1] = cc[2] = true;
+		else if (factor == PIPE_BLENDFACTOR_CONST_ALPHA)
+			cc[3] = true;
+	}
+
+	/* Find the actual constant associated with the components used*/
+
+	float constant = 0.0;
+	bool has_constant = false;
+
+	for (unsigned i = 0; i < 4; ++i) {
+		/* If the component is unused, nothing to do */
+		if (!cc[i]) continue;
+
+		float value = blend_color->color[i];
+
+		/* Either there's a second constant, in which case we fail, or
+		 * there's no constant / a first constant, in which case we use
+		 * that constant */
+
+		if (has_constant && constant != value) {
+			return false;
+		} else {
+			has_constant = true;
+			constant = value;
+		}
+	}
+
+	/* We have the constant -- success! */
+
+	*out = constant;
+	return true;
+}
+
+/* Create the descriptor for a fixed blend mode given the corresponding Gallium
+ * state, if possible. Return true and write out the blend descriptor into
+ * blend_equation. If it is not possible with the fixed function
+ * representating, return false to handle degenerate cases with a blend shader
+ */
+
+static const struct pipe_rt_blend_state default_blend = {
+	.blend_enable = 1,
+
+	.rgb_func = PIPE_BLEND_ADD,
+	.rgb_src_factor = PIPE_BLENDFACTOR_ONE,
+	.rgb_dst_factor = PIPE_BLENDFACTOR_ZERO,
+
+	.alpha_func = PIPE_BLEND_ADD,
+	.alpha_src_factor = PIPE_BLENDFACTOR_ONE,
+	.alpha_dst_factor = PIPE_BLENDFACTOR_ZERO,
+
+	.colormask = PIPE_MASK_RGBA
+};
+
+bool
+panfrost_make_fixed_blend_mode(const struct pipe_rt_blend_state *blend, struct mali_blend_equation *out, unsigned colormask, const struct pipe_blend_color *blend_color)
+{
+	/* If no blending is enabled, default back on `replace` mode */
+
+	if (!blend->blend_enable)
+		return panfrost_make_fixed_blend_mode(&default_blend, out, colormask, blend_color);
+
+	/* We have room only for a single float32 constant between the four
+	 * components. If we need more, spill to the programmable pipeline. */
+
+	unsigned factors[] = {
+		blend->rgb_src_factor, blend->rgb_dst_factor,
+		blend->alpha_src_factor, blend->alpha_dst_factor,
+	};
+
+	if (!panfrost_make_constant(factors, ARRAY_SIZE(factors), blend_color, &out->constant))
+		return false;
+
+	unsigned rgb_mode = 0;
+	unsigned alpha_mode = 0;
+
+	if (!panfrost_make_fixed_blend_part(
+		blend->rgb_func, blend->rgb_src_factor, blend->rgb_dst_factor,
+		&rgb_mode))
+			return false;
+
+	if (!panfrost_make_fixed_blend_part(
+		blend->alpha_func, blend->alpha_src_factor, blend->alpha_dst_factor,
+		&alpha_mode))
+			return false;
+
+	out->rgb_mode = rgb_mode;
+	out->alpha_mode = alpha_mode;
+
+	/* Gallium and Mali represent colour masks identically. XXX: Static assert for future proof */
+	out->color_mask = colormask;
+
+	panfrost_print_blend_equation(*out);
+
+	return true;
+}
diff --git a/src/gallium/drivers/panfrost/pan_blending.h b/src/gallium/drivers/panfrost/pan_blending.h
new file mode 100644
index 0000000..9e00f7a
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_blending.h
@@ -0,0 +1,24 @@
+/*
+ * © Copyright 2018 Alyssa Rosenzweig
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __PAN_BLENDING_H__
+#define __PAN_BLENDING_H__
+
+#include "pipe/p_state.h"
+#include "pipe/p_defines.h"
+#include <mali-job.h>
+
+bool panfrost_make_fixed_blend_mode(const struct pipe_rt_blend_state *blend, struct mali_blend_equation *out, unsigned colormask, const struct pipe_blend_color *blend_color);
+
+#endif
diff --git a/src/gallium/drivers/panfrost/pan_context.c b/src/gallium/drivers/panfrost/pan_context.c
new file mode 100644
index 0000000..308709d
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_context.c
@@ -0,0 +1,2798 @@
+/*
+ * © Copyright 2018 Alyssa Rosenzweig
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#include <sys/poll.h>
+
+#include "pan_context.h"
+#include "pan_swizzle.h"
+
+#include "util/macros.h"
+#include "util/u_format.h"
+#include "util/u_inlines.h"
+#include "util/u_surface.h"
+#include "util/u_upload_mgr.h"
+#include "util/u_transfer.h"
+#include "util/u_transfer_helper.h"
+#include "util/u_memory.h"
+#include "indices/u_primconvert.h"
+#include "tgsi/tgsi_parse.h"
+
+#include "pan_screen.h"
+#include "pan_blending.h"
+#include "pan_blend_shaders.h"
+
+static void
+panfrost_flush(
+		struct pipe_context *pipe,
+		struct pipe_fence_handle ** fence,
+		unsigned flags);
+
+static void
+panfrost_allocate_slab(struct panfrost_context *ctx,
+		    struct panfrost_memory *mem,
+		    size_t pages,
+		    bool mapped,
+		    bool same_va,
+		    int extra_flags,
+		    int commit_count,
+		    int extent);
+
+
+static bool USE_TRANSACTION_ELIMINATION = false;
+
+/* Don't use the mesa winsys; use our own X11 window with Xshm */
+#define USE_SLOWFB
+
+/* Do not actually send anything to the GPU; merely generate the cmdstream as fast as possible. Disables framebuffer writes */
+//#define DRY_RUN
+
+#define SET_BIT(lval, bit, cond) \
+	if (cond) \
+		lval |= (bit); \
+	else \
+		lval &= ~(bit);
+
+/* MSAA is not supported in sw_winsys but it does make for nicer demos ;) so we
+ * can force it regardless of gallium saying we don't have it */
+static bool FORCE_MSAA = true;
+
+/* Descriptor is generated along with the shader compiler */
+
+static void
+panfrost_upload_varyings_descriptor(struct panfrost_context *ctx)
+{
+	/* First, upload gl_Position varyings */
+	mali_ptr gl_Position = panfrost_upload(&ctx->cmdstream_persistent, ctx->vs->varyings.vertex_only_varyings, sizeof(ctx->vs->varyings.vertex_only_varyings), true);
+
+	/* Then, upload normal varyings for vertex shaders */
+	panfrost_upload_sequential(&ctx->cmdstream_persistent, ctx->vs->varyings.varyings, sizeof(ctx->vs->varyings.varyings[0]) * ctx->vs->varyings.varying_count);
+
+	/* Then, upload normal varyings for fragment shaders (duplicating) */
+	mali_ptr varyings_fragment = panfrost_upload_sequential(&ctx->cmdstream_persistent, ctx->vs->varyings.varyings, sizeof(ctx->vs->varyings.varyings[0]) * ctx->vs->varyings.varying_count);
+
+	/* Finally, upload gl_FragCoord varying */
+	panfrost_upload_sequential(&ctx->cmdstream_persistent, ctx->vs->varyings.fragment_only_varyings, sizeof(ctx->vs->varyings.fragment_only_varyings[0]) * ctx->vs->varyings.fragment_only_varying_count);
+
+        ctx->payload_vertex.postfix.varying_meta = gl_Position;
+        ctx->payload_tiler.postfix.varying_meta = varyings_fragment;
+}
+
+/* TODO: Sample size, etc */
+
+static void
+panfrost_set_framebuffer_msaa(struct panfrost_context *ctx, bool enabled)
+{
+	enabled = false;
+
+	SET_BIT(ctx->fragment_shader_core.unknown2_3, MALI_HAS_MSAA, enabled);
+	SET_BIT(ctx->fragment_shader_core.unknown2_4, MALI_NO_MSAA, !enabled);
+
+#ifdef SFBD
+	SET_BIT(ctx->fragment_fbd.format, MALI_FRAMEBUFFER_MSAA_A | MALI_FRAMEBUFFER_MSAA_B, enabled);
+#else
+	SET_BIT(ctx->fragment_rts[0].format, MALI_MFBD_FORMAT_MSAA, enabled);
+	
+	SET_BIT(ctx->fragment_fbd.unk1, (1 << 4) | (1 << 1), enabled);
+
+	/* XXX */
+	ctx->fragment_fbd.rt_count_2 = enabled ? 4 : 1;
+#endif
+}
+
+/* AFBC is enabled on a per-resource basis (AFBC enabling is theoretically
+ * indepdent between color buffers and depth/stencil). To enable, we allocate
+ * the AFBC metadata buffer and mark that it is enabled. We do -not- actually
+ * edit the fragment job here. This routine should be called ONCE per
+ * AFBC-compressed buffer, rather than on every frame. */
+
+static void
+panfrost_enable_afbc(struct panfrost_context *ctx, struct panfrost_resource *rsrc, bool ds)
+{
+#ifdef MFBD
+	/* AFBC metadata is 16 bytes per tile */
+	int tile_w = (rsrc->base.width0 + (MALI_TILE_LENGTH - 1)) >> MALI_TILE_SHIFT;
+	int tile_h = (rsrc->base.height0 + (MALI_TILE_LENGTH - 1)) >> MALI_TILE_SHIFT;
+
+	rsrc->stride *= 2;
+	int main_size = rsrc->stride * rsrc->base.height0;
+	rsrc->afbc_metadata_size = tile_w * tile_h * 16;
+
+	/* Allocate the AFBC slab itself, large enough to hold the above */
+	panfrost_allocate_slab(ctx, &rsrc->afbc_slab, (rsrc->afbc_metadata_size + main_size + 4095) / 4096, true, true, 0, 0, 0);
+	
+	rsrc->has_afbc = true;
+
+	/* Compressed textured reads use a tagged pointer to the metadata */
+
+	rsrc->gpu[0] = rsrc->afbc_slab.gpu | (ds ? 0 : 1);
+	rsrc->cpu[0] = rsrc->afbc_slab.cpu;
+#else
+	printf("AFBC not supported yet on SFBD\n");
+	assert(0);
+#endif
+}
+
+static void
+panfrost_enable_checksum(struct panfrost_context *ctx, struct panfrost_resource *rsrc)
+{
+	int tile_w = (rsrc->base.width0 + (MALI_TILE_LENGTH - 1)) >> MALI_TILE_SHIFT;
+	int tile_h = (rsrc->base.height0 + (MALI_TILE_LENGTH - 1)) >> MALI_TILE_SHIFT;
+
+	/* 8 byte checksum per tile */
+	rsrc->checksum_stride = tile_w * 8;
+	int pages = (((rsrc->checksum_stride * tile_h) + 4095) / 4096);
+	panfrost_allocate_slab(ctx, &rsrc->checksum_slab, pages, false, false, 0, 0, 0);
+
+	rsrc->has_checksum = true;
+}
+
+/* ..by contrast, this routine runs for every FRAGMENT job, but does no
+ * allocation. AFBC is enabled on a per-surface basis */
+
+static void
+panfrost_set_fragment_afbc(struct panfrost_context *ctx)
+{
+	for (int cb = 0; cb < ctx->pipe_framebuffer.nr_cbufs; ++cb) {
+		struct panfrost_resource *rsrc = (struct panfrost_resource *) ctx->pipe_framebuffer.cbufs[cb]->texture;
+
+		/* Non-AFBC is the default */
+		if (!rsrc->has_afbc)
+			continue;
+
+		/* Enable AFBC for the render target */
+		ctx->fragment_rts[0].afbc.metadata = rsrc->afbc_slab.gpu;
+		ctx->fragment_rts[0].afbc.stride = 0;
+		ctx->fragment_rts[0].afbc.unk = 0x30009;
+
+		ctx->fragment_rts[0].format |= MALI_MFBD_FORMAT_AFBC;
+
+		/* Change colourspace from RGB to BGR? */
+#if 0
+		ctx->fragment_rts[0].format |= 0x800000;
+		ctx->fragment_rts[0].format &= ~0x20000;
+#endif
+
+		/* Point rendering to our special framebuffer */
+		ctx->fragment_rts[0].framebuffer = rsrc->afbc_slab.gpu + rsrc->afbc_metadata_size;
+
+		/* WAT? Stride is diff from the scanout case */
+		ctx->fragment_rts[0].framebuffer_stride = ctx->pipe_framebuffer.width * 2 * 4;
+	}
+
+	/* Enable depth/stencil AFBC for the framebuffer (not the render target) */
+	if (ctx->pipe_framebuffer.zsbuf) {
+		struct panfrost_resource *rsrc = (struct panfrost_resource *) ctx->pipe_framebuffer.zsbuf->texture;
+
+		if (rsrc->has_afbc) {
+			ctx->fragment_fbd.unk3 |= MALI_MFBD_EXTRA;
+
+			ctx->fragment_extra.ds_afbc.depth_stencil_afbc_metadata = rsrc->afbc_slab.gpu;
+			ctx->fragment_extra.ds_afbc.depth_stencil_afbc_stride = 0;
+
+			ctx->fragment_extra.ds_afbc.depth_stencil = rsrc->afbc_slab.gpu + rsrc->afbc_metadata_size;
+
+			ctx->fragment_extra.ds_afbc.zero1 = 0x10009;
+			ctx->fragment_extra.ds_afbc.padding = 0x1000;
+
+			ctx->fragment_extra.unk = 0x435; /* General 0x400 in all unks. 0x5 for depth/stencil. 0x10 for AFBC encoded depth stencil. Unclear where the 0x20 is from */
+
+			ctx->fragment_fbd.unk3 |= 0x400;
+		}
+	}
+	
+	/* For the special case of a depth-only FBO, we need to attach a dummy render target */
+
+	if (ctx->pipe_framebuffer.nr_cbufs == 0) {
+		ctx->fragment_rts[0].format = 0x80008000;
+		ctx->fragment_rts[0].framebuffer = 0;
+		ctx->fragment_rts[0].framebuffer_stride = 0;
+	}
+}
+
+/* Framebuffer descriptor */
+
+#ifdef SFBD
+static void
+panfrost_set_framebuffer_resolution(struct mali_single_framebuffer *fb, int w, int h)
+{
+	fb->width = MALI_POSITIVE(w);
+	fb->height = MALI_POSITIVE(h);
+
+	/* No idea why this is needed, but it's how resolution_check is
+	 * calculated.  It's not clear to us yet why the hardware wants this.
+	 * The formula itself was discovered mostly by manual bruteforce and
+	 * aggressive algebraic simplification. */
+
+	fb->resolution_check = ((w + h) / 3) << 4;
+}
+#endif
+
+static PANFROST_FRAMEBUFFER
+panfrost_emit_fbd(struct panfrost_context *ctx)
+{
+#ifdef SFBD
+    struct mali_single_framebuffer framebuffer = {
+	    .unknown2 = 0x1f,
+	    .format = 0x30000000,
+	    .clear_flags = 0x1000,
+	    .unknown_address_0 = ctx->scratchpad.gpu,
+	    .unknown_address_1 = ctx->scratchpad.gpu + 0x6000,
+	    .unknown_address_2 = ctx->scratchpad.gpu + 0x6200,
+	    .tiler_flags = 0xf0,
+	    .tiler_heap_free = ctx->tiler_heap.gpu,
+	    .tiler_heap_end = ctx->tiler_heap.gpu + ctx->tiler_heap.size,
+    };
+
+    panfrost_set_framebuffer_resolution(&framebuffer, ctx->pipe_framebuffer.width, ctx->pipe_framebuffer.height);
+#else
+	struct bifrost_framebuffer framebuffer = {
+		.tiler_meta = 0xf00000c600,
+
+		.width1 = MALI_POSITIVE(ctx->pipe_framebuffer.width),
+		.height1 = MALI_POSITIVE(ctx->pipe_framebuffer.height),
+		.width2 = MALI_POSITIVE(ctx->pipe_framebuffer.width),
+		.height2 = MALI_POSITIVE(ctx->pipe_framebuffer.height),
+
+		.unk1 = 0x1080,
+
+		/* TODO: MRT */
+		.rt_count_1 = MALI_POSITIVE(1),
+		.rt_count_2 = 4,
+
+		/* TODO: Refactor to parallel SFBD */
+		/* unknown2 */
+		.zero1 = 0x1f,
+
+		/* Presumably corresponds to unknown_address_X of SFBD */
+		.zero2 = ctx->scratchpad.gpu,
+		.zero5 = ctx->misc_0.gpu,
+		.zero6 = ctx->misc_0.gpu + /*ctx->misc_0.size*/4096, /* Size depends on the size of the framebuffer and the number of vertices */
+
+		/* tiler_heap_start */
+		.zero7 = ctx->tiler_heap.gpu,
+		
+		/* tiler_heap_end */
+		.zero8 = ctx->tiler_heap.gpu + ctx->tiler_heap.size,
+	};
+
+#endif
+
+    return framebuffer;
+}
+
+/* Are we currently rendering to the screen (rather than an FBO)? */
+
+static bool
+panfrost_is_scanout(struct panfrost_context *ctx)
+{
+	/* If there is no color buffer, it's an FBO */
+	if (!ctx->pipe_framebuffer.nr_cbufs)
+		return false;
+
+	/* If we're too early that no framebuffer was sent, it's scanout */
+	if (!ctx->pipe_framebuffer.cbufs[0])
+		return true;
+
+	return ctx->pipe_framebuffer.cbufs[0]->texture->bind & PIPE_BIND_DISPLAY_TARGET;
+}
+
+/* The above function is for generalised fbd emission, used in both fragment as
+ * well as vertex/tiler payloads. This payload is specific to fragment
+ * payloads. */
+
+static void
+panfrost_new_frag_framebuffer(struct panfrost_context *ctx)
+{
+	mali_ptr framebuffer = ctx->framebuffer.gpu;
+	int stride;
+
+	/* The default is upside down from OpenGL's perspective. Plus, for scanout we supply our own framebuffer / stride */
+	if (panfrost_is_scanout(ctx)) {
+		stride = ctx->scanout_stride;
+
+		framebuffer += stride * (ctx->pipe_framebuffer.height - 1);
+		stride = -stride;
+	} else if (ctx->pipe_framebuffer.nr_cbufs > 0) {
+		stride = util_format_get_stride(ctx->pipe_framebuffer.cbufs[0]->format, ctx->pipe_framebuffer.width);
+	} else {
+		/* Depth-only framebuffer -> dummy RT */
+		framebuffer = 0;
+		stride = 0;
+	}
+
+#ifdef SFBD
+        struct mali_single_framebuffer fb = panfrost_emit_fbd(ctx);
+
+        fb.framebuffer = framebuffer;
+	fb.stride = stride;
+
+        fb.format = 0xb84e0281; /* RGB32, no MSAA */
+#else
+	struct bifrost_framebuffer fb = panfrost_emit_fbd(ctx);
+
+	/* XXX: MRT case */
+	fb.rt_count_2 = 1;
+	fb.unk3 = 0x100;
+
+	struct bifrost_render_target rt = {
+		.unk1 = 0x4000000,
+		.format = 0x860a8899, /* RGBA32, no MSAA */
+		.framebuffer = framebuffer,
+		.framebuffer_stride = (stride / 16) & 0xfffffff,
+	};
+
+	memcpy(&ctx->fragment_rts[0], &rt, sizeof(rt));
+
+	memset(&ctx->fragment_extra, 0, sizeof(ctx->fragment_extra));
+#endif
+
+        memcpy(&ctx->fragment_fbd, &fb, sizeof(fb));
+}
+
+/* Maps float 0.0-1.0 to int 0x00-0xFF */
+static uint8_t
+normalised_float_to_u8(float f) {
+	return (uint8_t) (int) (f * 255.0f);
+}
+
+static void
+panfrost_clear(
+		struct pipe_context *pipe,
+		unsigned buffers,
+		const union pipe_color_union *color,
+		double depth, unsigned stencil)
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+
+	/* Save settings for FBO switch */
+	ctx->last_clear.buffers = buffers;
+	ctx->last_clear.color = color;
+	ctx->last_clear.depth = depth;
+	ctx->last_clear.depth = depth;
+
+	bool clear_color = buffers & PIPE_CLEAR_COLOR;
+	bool clear_depth = buffers & PIPE_CLEAR_DEPTH;
+	bool clear_stencil = buffers & PIPE_CLEAR_STENCIL;
+
+	/* Remember that we've done something */
+	ctx->dirty |= PAN_DIRTY_DUMMY;
+
+	/* Alpha clear only meaningful without alpha channel */
+	bool has_alpha = ctx->pipe_framebuffer.nr_cbufs && util_format_has_alpha(ctx->pipe_framebuffer.cbufs[0]->format);
+	float clear_alpha = has_alpha ? color->f[3] : 1.0f;
+
+	uint32_t packed_color =
+		(normalised_float_to_u8(clear_alpha) << 24) |
+		(normalised_float_to_u8(color->f[2]) << 16) |
+		(normalised_float_to_u8(color->f[1]) <<  8) |
+		(normalised_float_to_u8(color->f[0]) <<  0);
+
+#ifdef MFBD
+	struct bifrost_render_target* buffer_color = &ctx->fragment_rts[0];
+#else
+	struct mali_single_framebuffer* buffer_color = &ctx->fragment_fbd;
+#endif
+
+#ifdef MFBD
+	struct bifrost_framebuffer *buffer_ds = &ctx->fragment_fbd;
+#else
+	struct mali_single_framebuffer *buffer_ds = buffer_color;
+#endif
+
+	if (clear_color) {
+		/* Fields duplicated 4x for unknown reasons. Same in Utgard,
+		 * too, which is doubly weird. */
+
+		buffer_color->clear_color_1 = packed_color;
+		buffer_color->clear_color_2 = packed_color;
+		buffer_color->clear_color_3 = packed_color;
+		buffer_color->clear_color_4 = packed_color;
+	}
+
+	if (clear_depth) {
+#ifdef SFBD
+		buffer_ds->clear_depth_1 = depth;
+		buffer_ds->clear_depth_2 = depth;
+		buffer_ds->clear_depth_3 = depth;
+		buffer_ds->clear_depth_4 = depth;
+#else
+		buffer_ds->clear_depth = depth;
+#endif
+	}
+
+	if (clear_stencil) {
+		buffer_ds->clear_stencil = stencil;
+	}
+
+	/* Setup buffers depending on MFBD/SFBD */
+
+#ifdef MFBD
+	if (clear_depth || clear_stencil) {
+		/* Setup combined 24/8 depth/stencil */
+		ctx->fragment_fbd.unk3 |= MALI_MFBD_EXTRA;
+		//ctx->fragment_extra.unk = /*0x405*/0x404;
+		ctx->fragment_extra.unk = 0x405;
+		ctx->fragment_extra.ds_linear.depth = ctx->depth_stencil_buffer.gpu;
+		ctx->fragment_extra.ds_linear.depth_stride = ctx->pipe_framebuffer.width * 4;
+	}
+#else
+	if (clear_depth) {
+		buffer_ds->depth_buffer = ctx->depth_stencil_buffer.gpu;
+		buffer_ds->depth_buffer_enable = MALI_DEPTH_STENCIL_ENABLE;
+	}
+
+	if (clear_stencil) {
+		buffer_ds->stencil_buffer = ctx->depth_stencil_buffer.gpu;
+		buffer_ds->stencil_buffer_enable = MALI_DEPTH_STENCIL_ENABLE;
+	}
+#endif
+
+#ifdef SFBD
+	/* Set flags based on what has been cleared, for the SFBD case */
+	/* XXX: What do these flags mean? */
+	int clear_flags = 0x101100;
+
+	if (clear_color && clear_depth && clear_stencil) {
+		/* On a tiler like this, it's fastest to clear all three buffers at once */
+
+		clear_flags |= MALI_CLEAR_FAST;
+	} else {
+		clear_flags |= MALI_CLEAR_SLOW;
+
+		if (clear_stencil)
+			clear_flags |= MALI_CLEAR_SLOW_STENCIL;
+	}
+
+	fbd->clear_flags = clear_flags;
+#endif
+}
+
+static void
+panfrost_attach_vt_framebuffer(struct panfrost_context *ctx)
+{
+#ifdef MFBD
+	mali_ptr who_knows = panfrost_reserve(&ctx->cmdstream, 1024);
+#endif
+
+	mali_ptr framebuffer_1_p = panfrost_upload(&ctx->cmdstream, &ctx->vt_framebuffer, sizeof(ctx->vt_framebuffer), true) | PANFROST_DEFAULT_FBD;
+
+#ifdef MFBD
+	/* MFBD needs a sequential semi-render target upload */
+
+	/* What this is, is beyond me for now */
+	struct bifrost_render_target rts_list[] = {
+		{
+			.chunknown = {
+				.unk = 0x30005,
+				.pointer = who_knows,
+			},
+			.framebuffer = ctx->misc_0.gpu,
+			.zero2 = 0x3,
+		},
+	};
+
+	panfrost_upload_sequential(&ctx->cmdstream, rts_list, sizeof(rts_list));
+#endif
+	ctx->payload_vertex.postfix.framebuffer = framebuffer_1_p;
+	ctx->payload_tiler.postfix.framebuffer = framebuffer_1_p;
+}
+
+static void
+panfrost_viewport(struct panfrost_context *ctx,
+		float depth_range_n,
+		float depth_range_f,
+		int viewport_x0, int viewport_y0,
+		int viewport_x1, int viewport_y1)
+{
+	/* Viewport encoding is asymmetric. Purpose of the floats is unknown? */
+
+	struct mali_viewport ret = {
+		.floats = {
+			-inff, -inff,
+			inff, inff,
+		},
+
+		.depth_range_n = depth_range_n, 
+		.depth_range_f = depth_range_f,
+
+		.viewport0 = { viewport_x0, viewport_y0 },
+		.viewport1 = { MALI_POSITIVE(viewport_x1), MALI_POSITIVE(viewport_y1) },
+	};
+
+	memcpy(&ctx->viewport, &ret, sizeof(ret));
+	ctx->dirty |= PAN_DIRTY_VIEWPORT;
+}
+
+/* Reset per-frame context, called on context initialisation as well as after
+ * flushing a frame */
+
+static void
+panfrost_invalidate_frame(struct panfrost_context *ctx)
+{
+	/* Rotate cmdstream */
+	ctx->cmdstream = ctx->cmdstream_rings[ctx->cmdstream_i];
+
+	if ((++ctx->cmdstream_i) == (sizeof(ctx->cmdstream_rings) / sizeof(ctx->cmdstream_rings[0])))
+		ctx->cmdstream_i = 0;
+
+	ctx->vt_framebuffer = panfrost_emit_fbd(ctx);
+	panfrost_new_frag_framebuffer(ctx);
+	
+	/* Reset varyings allocated */
+	ctx->varying_height = 0;
+
+	/* The cmdstream is dirty every frame; the only bits worth preserving
+	 * (textures, shaders, etc) are in other buffers anyways */
+	ctx->cmdstream.stack_bottom = 0;
+
+	/* Regenerate payloads */
+	panfrost_attach_vt_framebuffer(ctx);
+
+	if (ctx->rasterizer)
+		ctx->dirty |= PAN_DIRTY_RASTERIZER;
+
+	/* Uniforms are all discarded with the above stack discard */
+
+	for (int i = 0; i <= PIPE_SHADER_FRAGMENT; ++i)
+		ctx->constant_buffer[i].dirty = true;
+
+	/* XXX */
+	ctx->dirty |= PAN_DIRTY_SAMPLERS | PAN_DIRTY_TEXTURES;
+}
+
+/* In practice, every field of these payloads should be configurable
+ * arbitrarily, which means these functions are basically catch-all's for
+ * as-of-yet unwavering unknowns */
+
+static void
+panfrost_emit_vertex_payload(struct panfrost_context *ctx)
+{
+	struct midgard_payload_vertex_tiler payload = {
+		.prefix = {
+			.workgroups_z_shift = 32,
+			.workgroups_x_shift_2 = 0x2,
+			.workgroups_x_shift_3 = 0x5,
+		},
+		.gl_enables = 0x6
+	};
+
+	memcpy(&ctx->payload_vertex, &payload, sizeof(payload));
+}
+
+static void
+panfrost_emit_tiler_payload(struct panfrost_context *ctx)
+{
+	struct midgard_payload_vertex_tiler payload_1 = {
+		.prefix = {
+			.workgroups_z_shift = 32,
+			.workgroups_x_shift_2 = 0x2,
+			.workgroups_x_shift_3 = 0x6,
+
+			.zero1 = 0xffff, /* Why is this only seen on test-quad-textured? */
+		},
+	};
+
+	memcpy(&ctx->payload_tiler, &payload_1, sizeof(payload_1));
+}
+
+static unsigned
+panfrost_translate_texture_swizzle(enum pipe_swizzle s) {
+	switch (s) {
+		case PIPE_SWIZZLE_X: return MALI_CHANNEL_RED;
+		case PIPE_SWIZZLE_Y: return MALI_CHANNEL_GREEN;
+		case PIPE_SWIZZLE_Z: return MALI_CHANNEL_BLUE;
+		case PIPE_SWIZZLE_W: return MALI_CHANNEL_ALPHA;
+		case PIPE_SWIZZLE_0: return MALI_CHANNEL_ZERO;
+		case PIPE_SWIZZLE_1: return MALI_CHANNEL_ONE;
+		default: assert(0);
+	}
+}
+
+static unsigned
+translate_tex_wrap(enum pipe_tex_wrap w) {
+	switch (w) {
+		case PIPE_TEX_WRAP_REPEAT: return MALI_WRAP_REPEAT;
+		case PIPE_TEX_WRAP_CLAMP_TO_EDGE: return MALI_WRAP_CLAMP_TO_EDGE;
+		case PIPE_TEX_WRAP_CLAMP_TO_BORDER: return MALI_WRAP_CLAMP_TO_BORDER;
+		case PIPE_TEX_WRAP_MIRROR_REPEAT: return MALI_WRAP_MIRRORED_REPEAT;
+		default: assert(0);
+	}
+}
+
+static unsigned
+translate_tex_filter(enum pipe_tex_filter f) {
+	switch (f) {
+		case PIPE_TEX_FILTER_NEAREST: return MALI_GL_NEAREST;
+		case PIPE_TEX_FILTER_LINEAR: return MALI_GL_LINEAR;
+		default: assert(0);
+	}
+}
+
+static unsigned
+translate_mip_filter(enum pipe_tex_mipfilter f)
+{
+	return (f == PIPE_TEX_MIPFILTER_LINEAR) ? MALI_GL_MIP_LINEAR : 0;
+}	
+
+static unsigned
+panfrost_translate_compare_func(enum pipe_compare_func in)
+{
+	switch (in) {
+		case PIPE_FUNC_NEVER:	return MALI_FUNC_NEVER;
+		case PIPE_FUNC_LESS: 	return MALI_FUNC_LESS;
+		case PIPE_FUNC_EQUAL: 	return MALI_FUNC_EQUAL;
+		case PIPE_FUNC_LEQUAL: 	return MALI_FUNC_LEQUAL;
+		case PIPE_FUNC_GREATER: return MALI_FUNC_GREATER;
+		case PIPE_FUNC_NOTEQUAL:return MALI_FUNC_NOTEQUAL;
+		case PIPE_FUNC_GEQUAL: 	return MALI_FUNC_GEQUAL;
+		case PIPE_FUNC_ALWAYS: 	return MALI_FUNC_ALWAYS;
+	}
+
+	return 0; /* Unreachable */
+}
+
+static unsigned
+panfrost_translate_alt_compare_func(enum pipe_compare_func in)
+{
+	switch (in) {
+		case PIPE_FUNC_NEVER:	return MALI_ALT_FUNC_NEVER;
+		case PIPE_FUNC_LESS: 	return MALI_ALT_FUNC_LESS;
+		case PIPE_FUNC_EQUAL: 	return MALI_ALT_FUNC_EQUAL;
+		case PIPE_FUNC_LEQUAL: 	return MALI_ALT_FUNC_LEQUAL;
+		case PIPE_FUNC_GREATER: return MALI_ALT_FUNC_GREATER;
+		case PIPE_FUNC_NOTEQUAL:return MALI_ALT_FUNC_NOTEQUAL;
+		case PIPE_FUNC_GEQUAL: 	return MALI_ALT_FUNC_GEQUAL;
+		case PIPE_FUNC_ALWAYS: 	return MALI_ALT_FUNC_ALWAYS;
+	}
+
+	return 0; /* Unreachable */
+}
+
+static unsigned
+panfrost_translate_stencil_op(enum pipe_stencil_op in)
+{
+	switch (in) {
+		case PIPE_STENCIL_OP_KEEP:	return MALI_STENCIL_KEEP;
+		case PIPE_STENCIL_OP_ZERO:	return MALI_STENCIL_ZERO;
+		case PIPE_STENCIL_OP_REPLACE: 	return MALI_STENCIL_REPLACE;
+		case PIPE_STENCIL_OP_INCR: 	return MALI_STENCIL_INCR;
+		case PIPE_STENCIL_OP_DECR: 	return MALI_STENCIL_DECR;
+		case PIPE_STENCIL_OP_INCR_WRAP: return MALI_STENCIL_INCR_WRAP;
+		case PIPE_STENCIL_OP_DECR_WRAP: return MALI_STENCIL_DECR_WRAP;
+		case PIPE_STENCIL_OP_INVERT: 	return MALI_STENCIL_INVERT;
+	}
+
+	return 0; /* Unreachable */
+}
+
+static void
+panfrost_make_stencil_state(const struct pipe_stencil_state *in, struct mali_stencil_test *out)
+{
+	out->ref = 0; /* Gallium gets it from elsewhere */
+
+	out->mask = in->valuemask; 
+	out->func = panfrost_translate_compare_func(in->func);
+	out->sfail = panfrost_translate_stencil_op(in->fail_op);
+	out->dpfail = panfrost_translate_stencil_op(in->zfail_op);
+	out->dppass = panfrost_translate_stencil_op(in->zpass_op);
+}
+
+static void
+panfrost_default_shader_backend(struct panfrost_context *ctx)
+{
+	struct mali_shader_meta shader = {
+		.alpha_coverage = ~MALI_ALPHA_COVERAGE(0.000000),
+
+		.unknown2_3 = MALI_DEPTH_FUNC(MALI_FUNC_ALWAYS) | 0x3010 /*| MALI_CAN_DISCARD*/,
+#ifdef T8XX
+		.unknown2_4 = MALI_NO_MSAA | 0x4e0,
+#else
+		.unknown2_4 = MALI_NO_MSAA | 0x4f0,
+#endif
+	};
+
+	struct pipe_stencil_state default_stencil = {
+		.enabled = 0,
+		.func = PIPE_FUNC_ALWAYS,
+		.fail_op = MALI_STENCIL_KEEP,
+		.zfail_op = MALI_STENCIL_KEEP,
+		.zpass_op = MALI_STENCIL_KEEP,
+		.writemask = 0xFF,
+		.valuemask = 0xFF
+	};
+
+	panfrost_make_stencil_state(&default_stencil, &shader.stencil_front);
+	shader.stencil_mask_front = default_stencil.writemask;
+
+	panfrost_make_stencil_state(&default_stencil, &shader.stencil_back);
+	shader.stencil_mask_back = default_stencil.writemask;
+	
+	if (default_stencil.enabled)
+		shader.unknown2_4 |= MALI_STENCIL_TEST;
+
+    memcpy(&ctx->fragment_shader_core, &shader, sizeof(shader));
+}
+
+/* Generates a vertex/tiler job. This is, in some sense, the heart of the
+ * graphics command stream. It should be called once per draw, accordding to
+ * presentations. Set is_tiler for "tiler" jobs (fragment shader jobs, but in
+ * Mali parlance, "fragment" refers to framebuffer writeout). Clear it for
+ * vertex jobs. */
+
+static mali_ptr
+panfrost_vertex_tiler_job(struct panfrost_context *ctx, bool is_tiler)
+{
+	/* Each draw call corresponds to two jobs, and we want to offset to leave room for the set-value job */
+	int draw_job_index = 1 + (2 * ctx->draw_count);
+
+	struct mali_job_descriptor_header job = {
+		.job_type = is_tiler ? JOB_TYPE_TILER : JOB_TYPE_VERTEX,
+		.job_index = draw_job_index + (is_tiler ? 1 : 0),
+#ifdef BIT64
+		.job_descriptor_size = 1,
+#endif 
+	};
+
+	/* XXX: What is this? */
+#ifdef T6XX
+	if (is_tiler)
+		job.unknown_flags = ctx->draw_count ? 64 : 1;
+#endif
+
+	/* Only tiler jobs have dependencies which are known at this point */
+
+	if (is_tiler) {
+		/* Tiler jobs depend on vertex jobs */
+
+		job.job_dependency_index_1 = draw_job_index;
+
+		/* Tiler jobs also depend on the previous tiler job */
+
+		if (ctx->draw_count)
+			job.job_dependency_index_2 = draw_job_index - 1;
+	}
+	struct midgard_payload_vertex_tiler *payload = is_tiler ? &ctx->payload_tiler : &ctx->payload_vertex;
+
+	/* There's some padding hacks on 32-bit */
+
+#ifdef BIT64
+	int offset = 0;
+#else
+	int offset = 4;
+#endif
+
+	mali_ptr job_p = panfrost_upload(&ctx->cmdstream, &job, sizeof(job) - offset, true);
+	panfrost_upload_sequential(&ctx->cmdstream, payload, sizeof(*payload));
+	return job_p;
+}
+
+/* Generates a set value job. It's unclear what exactly this does, why it's
+ * necessary, and when to call it. */
+
+static mali_ptr
+panfrost_set_value_job(struct panfrost_context *ctx)
+{
+	struct mali_job_descriptor_header job_0 = {
+		.job_type = JOB_TYPE_SET_VALUE,
+		.job_descriptor_size = 1,
+		.job_index = 1 + (2 * ctx->draw_count),
+	};
+
+	struct mali_payload_set_value payload_0 = {
+		.out = ctx->misc_0.gpu,
+		.unknown = 0x3,
+	};
+
+	mali_ptr job_0_p = panfrost_upload(&ctx->cmdstream, &job_0, sizeof(job_0), true);
+	panfrost_upload_sequential(&ctx->cmdstream, &payload_0, sizeof(payload_0));
+
+	return job_0_p;
+}
+
+/* Generate a fragment job. This should be called once per frame. (According to
+ * presentations, this is supposed to correspond to eglSwapBuffers) */
+
+static mali_ptr
+panfrost_fragment_job(struct panfrost_context *ctx)
+{
+	/* Update fragment FBD */
+	panfrost_set_fragment_afbc(ctx);
+
+	if (ctx->pipe_framebuffer.nr_cbufs == 1) {
+		struct panfrost_resource *rsrc = (struct panfrost_resource *) ctx->pipe_framebuffer.cbufs[0]->texture;
+
+		if (rsrc->has_checksum) {
+			//ctx->fragment_fbd.unk3 |= 0xa00000;
+			ctx->fragment_fbd.unk3 = 0xa02100;
+			ctx->fragment_fbd.unk3 |= MALI_MFBD_EXTRA;
+			ctx->fragment_extra.unk = 0x420;
+			ctx->fragment_extra.checksum_stride = rsrc->checksum_stride;
+			ctx->fragment_extra.checksum = /*rsrc->checksum_slab.gpu*/ctx->framebuffer.gpu + (ctx->scanout_stride)*rsrc->base.height0;
+		}
+	}
+
+	/* The frame is complete and therefore the framebuffer descriptor is
+	 * ready for linkage and upload */
+
+	mali_ptr fbd = panfrost_upload(&ctx->cmdstream, &ctx->fragment_fbd, sizeof(ctx->fragment_fbd), true);
+
+	/* Upload extra framebuffer info if necessary */
+	if (ctx->fragment_fbd.unk3 & MALI_MFBD_EXTRA) {
+		panfrost_upload_sequential(&ctx->cmdstream, &ctx->fragment_extra, sizeof(struct bifrost_fb_extra));
+	}
+
+	/* Upload (single) render target */
+	panfrost_upload_sequential(&ctx->cmdstream, &ctx->fragment_rts[0], sizeof(struct bifrost_render_target) * 1);
+
+	/* Generate the fragment (frame) job */
+
+	struct mali_job_descriptor_header header = {
+		.job_type = JOB_TYPE_FRAGMENT,
+		.job_index = 1,
+#ifdef BIT64
+		.job_descriptor_size = 1
+#endif
+	};
+
+	struct mali_payload_fragment payload = {
+		.min_tile_coord = MALI_COORDINATE_TO_TILE_MIN(0, 0),
+		.max_tile_coord = MALI_COORDINATE_TO_TILE_MAX(ctx->pipe_framebuffer.width, ctx->pipe_framebuffer.height),
+		.framebuffer = fbd | PANFROST_DEFAULT_FBD | (ctx->fragment_fbd.unk3 & MALI_MFBD_EXTRA ? 2 : 0),
+	};
+
+	/* Normally, there should be no padding. However, fragment jobs are
+	 * shared with 64-bit Bifrost systems, and accordingly there is 4-bytes
+	 * of zero padding in between. */
+
+	mali_ptr job_pointer = panfrost_upload(&ctx->cmdstream, &header, sizeof(header), true);
+	panfrost_upload_sequential(&ctx->cmdstream, &payload, sizeof(payload));
+
+	return job_pointer;
+}
+
+/* Emits attributes and varying descriptors, which should be called every draw,
+ * excepting some obscure circumstances */
+
+static void
+panfrost_emit_vertex_data(struct panfrost_context *ctx)
+{
+	/* TODO: Only update the dirtied buffers */
+	struct mali_attr attrs[PIPE_MAX_ATTRIBS];
+	struct mali_attr varyings[PIPE_MAX_ATTRIBS];
+
+	for (int i = 0; i < ctx->vertex_buffer_count; ++i) {
+		struct pipe_vertex_buffer *buf = &ctx->vertex_buffers[i];
+		struct panfrost_resource *rsrc = (struct panfrost_resource *) (buf->buffer.resource);
+
+		/* Offset vertex count by draw_start to make sure we upload enough */
+		attrs[i].stride = buf->stride;
+		//attrs[i].size = buf->stride * (ctx->payload_vertex.draw_start + ctx->vertex_count);
+
+		/* TODO: The above calculation is wrong. Do it better. For now, force resources */
+		assert(!buf->is_user_buffer);
+		attrs[i].size = buf->buffer.resource->width0 - buf->buffer_offset;
+
+		attrs[i].elements = panfrost_upload(&ctx->cmdstream, rsrc->cpu[0] + buf->buffer_offset, attrs[i].size, false) | 1;
+	}
+
+	for (int i = 0; i < ctx->vs->varyings.varying_buffer_count; ++i) {
+		varyings[i].elements = (ctx->varying_mem.gpu + ctx->varying_height) | 1;
+		varyings[i].stride = ctx->vs->varyings.varyings_stride[i];
+		
+		/* XXX: Why does adding an extra ~8000 vertices fix missing triangles in glmark2-es2 -bshadow? */
+		varyings[i].size = ctx->vs->varyings.varyings_stride[i] * MALI_NEGATIVE(ctx->payload_tiler.prefix.invocation_count);
+
+		/* gl_Position varying is always last by convention */
+		if ((i + 1) == ctx->vs->varyings.varying_buffer_count)
+			ctx->payload_tiler.postfix.position_varying = ctx->varying_mem.gpu + ctx->varying_height;
+
+		/* Varyings appear to need 64-byte alignment */
+		ctx->varying_height += ALIGN(varyings[i].size, 64);
+
+		/* Ensure that we fit */
+		assert(ctx->varying_height < ctx->varying_mem.size);
+	}
+
+	ctx->payload_vertex.postfix.attributes = panfrost_upload(&ctx->cmdstream, attrs, ctx->vertex_buffer_count * sizeof(struct mali_attr), false);
+
+	mali_ptr varyings_p = panfrost_upload(&ctx->cmdstream, &varyings, ctx->vs->varyings.varying_buffer_count * sizeof(struct mali_attr), false);
+	ctx->payload_vertex.postfix.varyings = varyings_p;
+	ctx->payload_tiler.postfix.varyings = varyings_p;
+}
+
+/* Go through dirty flags and actualise them in the cmdstream. */
+
+static void
+panfrost_emit_for_draw(struct panfrost_context *ctx)
+{
+	panfrost_emit_vertex_data(ctx);
+
+	if (ctx->dirty & PAN_DIRTY_RASTERIZER) {
+		ctx->payload_tiler.line_width = ctx->rasterizer->base.line_width;
+		ctx->payload_tiler.gl_enables = ctx->rasterizer->tiler_gl_enables;
+
+		panfrost_set_framebuffer_msaa(ctx, FORCE_MSAA || ctx->rasterizer->base.multisample);
+
+	}
+
+	if (ctx->dirty & PAN_DIRTY_VS) {
+		assert(ctx->vs);
+
+		/* Late shader descriptor assignments */
+		ctx->vs->tripipe.texture_count = ctx->sampler_view_count[PIPE_SHADER_VERTEX];
+		ctx->vs->tripipe.sampler_count = ctx->sampler_count[PIPE_SHADER_VERTEX];
+
+		/* Who knows */
+		ctx->vs->tripipe.midgard1.unknown1 = 0x2201;
+
+		ctx->payload_vertex.postfix._shader_upper = panfrost_upload(&ctx->cmdstream_persistent, &ctx->vs->tripipe, sizeof(struct mali_shader_meta), true) >> 4;
+
+		/* Varying descriptor is tied to the vertex shader. Also the
+		 * fragment shader, I suppose, but it's generated with the
+		 * vertex shader so */
+
+		panfrost_upload_varyings_descriptor(ctx);
+	}
+
+	if (ctx->dirty & PAN_DIRTY_FS) { 
+		assert(ctx->fs);
+#define COPY(name) ctx->fragment_shader_core.name = ctx->fs->tripipe.name
+
+		COPY(shader);
+		COPY(attribute_count);
+		COPY(varying_count);
+		COPY(midgard1.uniform_count);
+		COPY(midgard1.work_count);
+		COPY(midgard1.unknown2);
+
+#undef COPY
+		/* If there is a blend shader, work registers are shared */
+
+		if (ctx->blend->has_blend_shader)
+			ctx->fragment_shader_core.midgard1.work_count = /*MAX2(ctx->fragment_shader_core.midgard1.work_count, ctx->blend->blend_work_count)*/16;
+
+		/* Set late due to depending on render state */
+		/* The one at the end seems to mean "1 UBO" */
+		ctx->fragment_shader_core.midgard1.unknown1 = MALI_NO_ALPHA_TO_COVERAGE | 0x200 | 0x2201;
+
+		/* Assign texture/sample count right before upload */
+		ctx->fragment_shader_core.texture_count = ctx->sampler_view_count[PIPE_SHADER_FRAGMENT];
+		ctx->fragment_shader_core.sampler_count = ctx->sampler_count[PIPE_SHADER_FRAGMENT];
+
+		/* Assign the stencil refs late */
+		ctx->fragment_shader_core.stencil_front.ref = ctx->stencil_ref.ref_value[0];
+		ctx->fragment_shader_core.stencil_back.ref = ctx->stencil_ref.ref_value[1];
+
+		/* CAN_DISCARD should be set if the fragment shader possibly
+		 * contains a 'discard' instruction, or maybe other
+		 * circumstances. It is likely this is related to optimizations
+		 * related to forward-pixel kill, as per "Mali Performance 3:
+		 * Is EGL_BUFFER_PRESERVED a good thing?" by Peter Harris
+		 */
+
+		if (ctx->fs->can_discard) {
+			ctx->fragment_shader_core.unknown2_3 |= MALI_CAN_DISCARD;
+			ctx->fragment_shader_core.midgard1.unknown1 &= ~MALI_NO_ALPHA_TO_COVERAGE;
+			ctx->fragment_shader_core.midgard1.unknown1 |= 0x4000;
+			ctx->fragment_shader_core.midgard1.unknown1 = 0x4200;
+		}
+
+		if (ctx->blend->has_blend_shader)
+			ctx->fragment_shader_core.blend_shader = ctx->blend->blend_shader;
+
+		ctx->payload_tiler.postfix._shader_upper = panfrost_upload(&ctx->cmdstream_persistent, &ctx->fragment_shader_core, sizeof(struct mali_shader_meta), true) >> 4;
+
+#ifdef T8XX
+		/* Additional blend descriptor tacked on for newer systems */
+
+		unsigned blend_count = 0;
+
+		if (ctx->blend->has_blend_shader) {
+			/* For a blend shader, the bottom nibble corresponds to
+			 * the number of work registers used, which signals the
+			 * -existence- of a blend shader */
+
+			assert(ctx->blend->blend_work_count >= 2);
+			blend_count |= ctx->blend->blend_work_count;
+		} else {
+			/* Otherwise, the bottom bit simply specifies if
+			 * blending (anything other than REPLACE) is enabled */
+
+			/* XXX: Less ugly way to do this? */
+			bool no_blending =
+				(ctx->blend->equation.rgb_mode == 0x122) &&
+				(ctx->blend->equation.alpha_mode == 0x122) &&
+				(ctx->blend->equation.color_mask == 0xf);
+
+			if (!no_blending)
+				blend_count |= 0x1;
+		}
+
+		/* Second blend equation is always a simple replace */
+
+		uint64_t replace_magic = 0xf0122122;
+		struct mali_blend_equation replace_mode;
+		memcpy(&replace_mode, &replace_magic, sizeof(replace_mode));
+
+		struct mali_blend_meta blend_meta[] = {
+			{
+				.unk1 = 0x200 | blend_count,
+				.blend_equation_1 = ctx->blend->equation,
+				.blend_equation_2 = replace_mode
+			},
+		};
+
+		if (ctx->blend->has_blend_shader)
+			memcpy(&blend_meta[0].blend_equation_1, &ctx->blend->blend_shader, sizeof(ctx->blend->blend_shader));
+
+		panfrost_upload_sequential(&ctx->cmdstream_persistent, blend_meta, sizeof(blend_meta));
+#endif
+	}
+
+	if (ctx->dirty & PAN_DIRTY_VERTEX) {
+		ctx->payload_vertex.postfix.attribute_meta = panfrost_upload(&
+				ctx->cmdstream_persistent, &ctx->vertex->hw,
+				sizeof(struct mali_attr_meta) * ctx->vertex->num_elements, false);
+	}
+
+	ctx->dirty |= PAN_DIRTY_VIEWPORT; /* TODO: Viewport dirty track */
+	if (ctx->dirty & PAN_DIRTY_VIEWPORT) {
+		ctx->payload_tiler.postfix.viewport = panfrost_upload(&ctx->cmdstream, &ctx->viewport, sizeof(struct mali_viewport), false);
+	}
+
+	if (ctx->dirty & PAN_DIRTY_SAMPLERS) {
+		/* Upload samplers back to back, no padding */
+
+		for (int t = 0; t <= PIPE_SHADER_FRAGMENT; ++t) {
+			mali_ptr samplers_base = 0;
+
+			for (int i = 0; i < ctx->sampler_count[t]; ++i) {
+				if (i)
+					panfrost_upload_sequential(&ctx->cmdstream, &ctx->samplers[t][i]->hw, sizeof(struct mali_sampler_descriptor));
+				else
+					samplers_base = panfrost_upload(&ctx->cmdstream, &ctx->samplers[t][i]->hw, sizeof(struct mali_sampler_descriptor), true);
+			}
+
+			if (t == PIPE_SHADER_FRAGMENT)
+				ctx->payload_tiler.postfix.sampler_descriptor = samplers_base;
+			else if (t == PIPE_SHADER_VERTEX)
+				ctx->payload_vertex.postfix.sampler_descriptor = samplers_base;
+			else
+				assert(0);
+		}
+	}
+
+	if (ctx->dirty & PAN_DIRTY_TEXTURES) {
+		for (int t = 0; t <= PIPE_SHADER_FRAGMENT; ++t) {
+			/* Shortcircuit */
+			if (!ctx->sampler_view_count[t]) continue;
+
+			uint64_t trampolines[PIPE_MAX_SHADER_SAMPLER_VIEWS];
+
+			for (int i = 0; i < ctx->sampler_view_count[t]; ++i) {
+				/* XXX: Why does this work? */
+				if (!ctx->sampler_views[t][i])
+					continue;
+
+				struct pipe_resource *tex_rsrc = ctx->sampler_views[t][i]->base.texture;
+				struct panfrost_resource *rsrc = (struct panfrost_resource *) tex_rsrc;
+
+				/* Inject the address in. */
+				for (int l = 0; l < (tex_rsrc->last_level + 1); ++l)
+					ctx->sampler_views[t][i]->hw.swizzled_bitmaps[l] = rsrc->gpu[l];
+
+				/* Workaround maybe-errata (?) with non-mipmaps */
+				int s = ctx->sampler_views[t][i]->hw.nr_mipmap_levels;
+
+				if (!rsrc->is_mipmap) {
+#ifdef T6XX
+					/* HW ERRATA, not needed after T6XX */
+					ctx->sampler_views[t][i]->hw.swizzled_bitmaps[1] = rsrc->gpu[0];
+
+					ctx->sampler_views[t][i]->hw.unknown3A = 1;
+#endif
+					ctx->sampler_views[t][i]->hw.nr_mipmap_levels = 0;
+				}
+
+				trampolines[i] = panfrost_upload(&ctx->cmdstream, &ctx->sampler_views[t][i]->hw, sizeof(struct mali_texture_descriptor), false);
+
+				/* Restore */
+				ctx->sampler_views[t][i]->hw.nr_mipmap_levels = s;
+				ctx->sampler_views[t][i]->hw.unknown3A = 0;
+			}
+
+			mali_ptr trampoline = panfrost_upload(&ctx->cmdstream, trampolines, sizeof(uint64_t) * ctx->sampler_view_count[t], false);
+
+			if (t == PIPE_SHADER_FRAGMENT)
+				ctx->payload_tiler.postfix.texture_trampoline = trampoline;
+			else if (t == PIPE_SHADER_VERTEX)
+				ctx->payload_vertex.postfix.texture_trampoline = trampoline;
+			else
+				assert(0);
+		}
+	}
+
+	/* Generate the viewport vector of the form: <width, height, centerx, centery> */
+	const struct pipe_viewport_state *vp = &ctx->pipe_viewport;
+
+	float viewport_vec4[] = {
+		2.0 * vp->scale[0],
+		abs(2.0 * vp->scale[1]),
+
+		vp->translate[0],
+		/* -1.0 * vp->translate[1] */ abs(1.0 * vp->scale[1]) /* XXX */
+	};
+
+	for (int i = 0; i < PIPE_SHADER_TYPES; ++i) {
+		struct panfrost_constant_buffer *buf = &ctx->constant_buffer[i];
+
+		if (buf->dirty) {
+			mali_ptr address_prefix = 0, address = 0;
+
+			/* Attach vertex prefix */
+			if (i == PIPE_SHADER_VERTEX)
+				address_prefix = panfrost_upload(&ctx->cmdstream, viewport_vec4, sizeof(viewport_vec4), true);
+		
+			/* Attach uniforms */
+			if (buf->size)
+				address = panfrost_upload_sequential(&ctx->cmdstream, buf->buffer, buf->size);
+
+			/* Always fill out -something- */
+			if (!address)
+				address = panfrost_reserve(&ctx->cmdstream, 256);
+
+			/* Use whichever came first */
+			if (address_prefix)
+				address = address_prefix;
+
+			int uniform_count = 0;
+
+			struct mali_vertex_tiler_postfix *postfix;
+		
+			switch (i) {
+				case PIPE_SHADER_VERTEX:
+					uniform_count = ctx->vs->uniform_count;
+					postfix = &ctx->payload_vertex.postfix;
+					break;
+
+				case PIPE_SHADER_FRAGMENT:
+					uniform_count = ctx->fs->uniform_count;
+					postfix = &ctx->payload_tiler.postfix;
+					break;
+
+				default:
+					printf("Unknown shader stage %d in uniform upload\n", i);
+					assert(0);
+			}
+
+			/* Also attach the same buffer as a UBO for extended access */
+
+
+			struct mali_uniform_buffer_meta uniform_buffers[] = {
+				{
+					.size = MALI_POSITIVE((2 + uniform_count)),
+					.ptr = address >> 2,
+				},
+			};
+
+			mali_ptr ubufs = panfrost_upload(&ctx->cmdstream, uniform_buffers, sizeof(uniform_buffers), false);
+			postfix->uniforms = address;
+			postfix->uniform_buffers = ubufs;
+
+			buf->dirty = 0;
+		}
+	}
+
+
+	ctx->dirty = 0;
+}
+
+/* Corresponds to exactly one draw, but does not submit anything */
+
+static void
+panfrost_queue_draw(struct panfrost_context *ctx)
+{
+	/* TODO: Expand the array? */
+	if (ctx->draw_count >= MAX_DRAW_CALLS) {
+		printf("Job buffer overflow, ignoring draw\n");
+		assert(0);
+	}
+
+	/* Handle dirty flags now */
+	panfrost_emit_for_draw(ctx);
+
+	ctx->vertex_jobs[ctx->draw_count] = panfrost_vertex_tiler_job(ctx, false);
+	ctx->tiler_jobs[ctx->draw_count] = panfrost_vertex_tiler_job(ctx, true);
+	ctx->draw_count++;
+}
+
+/* At the end of the frame, the vertex and tiler jobs are linked together and
+ * then the fragment job is plonked at the end. Set value job is first for
+ * unknown reasons. */
+
+#define JOB_DESC(ptr) ((struct mali_job_descriptor_header *) (uintptr_t) (ptr - mem.gpu + (uintptr_t) mem.cpu))
+static void
+panfrost_link_job_pair(struct panfrost_memory mem, mali_ptr first, mali_ptr next)
+{
+	if (JOB_DESC(first)->job_descriptor_size)
+		JOB_DESC(first)->next_job_64 = (u64) (uintptr_t) next;
+	else
+		JOB_DESC(first)->next_job_32 = (u32) (uintptr_t) next;
+}
+
+static void
+panfrost_link_jobs(struct panfrost_context *ctx)
+{
+	if (ctx->draw_count) {
+		/* Generate the set_value_job */
+		ctx->set_value_job = panfrost_set_value_job(ctx);
+
+		struct panfrost_memory mem = ctx->cmdstream;
+
+		/* Have the first vertex job depend on the set value job */
+		JOB_DESC(ctx->vertex_jobs[0])->job_dependency_index_1 = JOB_DESC(ctx->set_value_job)->job_index;
+
+		/* SV -> V */
+		panfrost_link_job_pair(mem, ctx->set_value_job, ctx->vertex_jobs[0]);
+	}
+
+	/* V -> V/T ; T -> T/null */
+	for (int i = 0; i < ctx->draw_count; ++i) {
+		bool isLast = (i + 1) == ctx->draw_count;
+
+		panfrost_link_job_pair(ctx->cmdstream, ctx->vertex_jobs[i], isLast ? ctx->tiler_jobs[0] : ctx->vertex_jobs[i + 1]);
+		panfrost_link_job_pair(ctx->cmdstream, ctx->tiler_jobs[i], isLast ? 0 : ctx->tiler_jobs[i + 1]);
+	}
+}
+
+/* Use to allocate atom numbers for jobs. We probably want to overhaul this in kernel space at some point. */
+uint8_t atom_counter = 0;
+
+static uint8_t
+allocate_atom()
+{
+	atom_counter++;
+
+	/* Workaround quirk where atoms must be strictly positive */
+
+	if (atom_counter == 0)
+		atom_counter++;
+
+	return atom_counter;
+}
+
+int last_fragment_id = -1;
+int last_fragment_flushed = true;
+
+/* Forces a flush, to make sure everything is consistent.
+ * Bad for parallelism. Necessary for glReadPixels etc. Use cautiously.
+ */
+
+static void
+force_flush_fragment(struct panfrost_context *ctx)
+{
+	if (!last_fragment_flushed) {
+		uint8_t ev[/* 1 */ 4 + 4 + 8 + 8];
+
+		do {
+			read(ctx->fd, ev, sizeof(ev));
+		} while (ev[4] != last_fragment_id);
+
+		last_fragment_flushed = true;
+	}
+}
+
+/* The entire frame is in memory -- send it off to the kernel! */
+
+static void
+panfrost_submit_frame(struct panfrost_context *ctx, bool flush_immediate)
+{
+	/* Edge case if screen is cleared and nothing else */
+	bool has_draws = ctx->draw_count > 0;
+
+	/* Workaround a bizarre lockup (a hardware errata?) */
+	//if (!has_draws)
+		flush_immediate = true;
+
+	/* A number of jobs are batched -- this must be linked and cleared */
+	panfrost_link_jobs(ctx);
+
+	ctx->draw_count = 0;
+
+#ifndef DRY_RUN
+	/* XXX: flush_immediate was causing lock-ups wrt readpixels in dEQP. Investigate. */
+
+	mali_external_resource framebuffer[] = {
+		ctx->framebuffer.gpu | MALI_EXT_RES_ACCESS_EXCLUSIVE,
+	};
+
+	int vt_atom = allocate_atom();
+
+	struct mali_jd_atom_v2 atoms[] = {
+		{
+			.jc = ctx->set_value_job,
+			.atom_number = vt_atom,
+			.core_req = MALI_JD_REQ_CS | MALI_JD_REQ_T | MALI_JD_REQ_CF | MALI_JD_REQ_COHERENT_GROUP | MALI_JD_REQ_EVENT_NEVER,
+		},
+		{
+			.jc = panfrost_fragment_job(ctx),
+			.nr_ext_res = 1,
+			.ext_res_list = framebuffer,
+			.atom_number = allocate_atom(),
+			.core_req = MALI_JD_REQ_FS | MALI_JD_REQ_SKIP_CACHE_START,
+		},
+	};
+
+	if (last_fragment_id != -1) {
+		atoms[0].pre_dep[0].atom_id = last_fragment_id;
+		atoms[0].pre_dep[0].dependency_type = MALI_JD_DEP_TYPE_ORDER;
+	}
+
+	if (has_draws) {
+		atoms[1].pre_dep[0].atom_id = vt_atom;
+		atoms[1].pre_dep[0].dependency_type = MALI_JD_DEP_TYPE_DATA;
+	}
+
+	atoms[1].core_req |= panfrost_is_scanout(ctx) ? MALI_JD_REQ_EXTERNAL_RESOURCES : MALI_JD_REQ_FS_AFBC;
+
+	/* Copy over core reqs for old kernels */
+
+	for (int i = 0; i < 2; ++i)
+		atoms[i].compat_core_req = atoms[i].core_req;
+
+	struct mali_ioctl_job_submit submit = {
+		.addr = atoms + (has_draws ? 0 : 1),
+		.nr_atoms = has_draws ? 2 : 1,
+		.stride = sizeof(struct mali_jd_atom_v2),
+	};
+
+	if (pandev_ioctl(ctx->fd, MALI_IOCTL_JOB_SUBMIT, &submit))
+	    printf("Error submitting\n");
+
+	/* If visual, we can stall a frame */
+
+	if (!flush_immediate)
+		force_flush_fragment(ctx);
+
+	last_fragment_id = atoms[1].atom_number;
+	last_fragment_flushed = false;
+
+	/* If readback, flush now (hurts the pipelined performance) */
+	if (flush_immediate)
+		force_flush_fragment(ctx);
+#endif
+}
+
+bool dont_scanout = false;
+
+static void
+panfrost_flush(
+		struct pipe_context *pipe,
+		struct pipe_fence_handle ** fence,
+		unsigned flags)
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+
+	/* If there is nothing drawn, skip the frame */
+	if (!ctx->draw_count && !(ctx->dirty & PAN_DIRTY_DUMMY)) return;
+
+	/* Whether to stall the pipeline for immediately correct results */
+	bool flush_immediate = flags & PIPE_FLUSH_END_OF_FRAME;
+
+	/* Submit the frame itself */
+	panfrost_submit_frame(ctx, flush_immediate);
+
+	/* Prepare for the next frame */
+	panfrost_invalidate_frame(ctx);
+
+#ifdef USE_SLOWFB
+#ifndef DRY_RUN
+	if (panfrost_is_scanout(ctx) && !dont_scanout) {
+		/* Display the frame in our cute little window */
+		slowfb_update((uint8_t*) ctx->framebuffer.cpu, ctx->pipe_framebuffer.width, ctx->pipe_framebuffer.height);
+	} 
+#endif
+#endif
+}	
+
+#define DEFINE_CASE(c) case PIPE_PRIM_##c: return MALI_GL_##c;
+
+static int
+g2m_draw_mode(enum pipe_prim_type mode)
+{
+	switch(mode) {
+		DEFINE_CASE(POINTS);
+		DEFINE_CASE(LINES);
+		DEFINE_CASE(LINE_LOOP);
+		DEFINE_CASE(LINE_STRIP);
+		DEFINE_CASE(TRIANGLES);
+		DEFINE_CASE(TRIANGLE_STRIP);
+		DEFINE_CASE(TRIANGLE_FAN);
+
+		default:
+			printf("Illegal draw mode %d\n", mode);
+			assert(0);
+			return MALI_GL_LINE_LOOP;
+	}
+}
+
+#undef DEFINE_CASE
+
+static unsigned
+panfrost_translate_index_size(unsigned size)
+{
+	switch (size) {
+		case 1: return MALI_DRAW_INDEXED_UINT8;
+		case 2: return MALI_DRAW_INDEXED_UINT16;
+		case 4: return MALI_DRAW_INDEXED_UINT32;
+		default:
+			printf("Unknown index size %d\n", size);
+			assert(0);
+			return 0;
+	}
+}
+
+static const uint8_t *
+panfrost_get_index_buffer_raw(const struct pipe_draw_info *info)
+{
+	if (info->has_user_indices) {
+		return (const uint8_t *) info->index.user;
+	} else {
+		struct panfrost_resource *rsrc = (struct panfrost_resource *) (info->index.resource);
+		return (const uint8_t *) rsrc->cpu[0];
+	}
+}
+
+bool needs_dummy_draw = true;
+
+static void
+panfrost_draw_vbo(
+		struct pipe_context *pipe,
+		const struct pipe_draw_info *info);
+
+/* XXX: First frame w/ a draw seems to fail... so inject a fake frame */
+
+static void
+panfrost_maybe_dummy_draw(struct panfrost_context *ctx, const struct pipe_draw_info *info)
+{
+	if (!needs_dummy_draw)
+		return;
+
+	needs_dummy_draw = false;
+	dont_scanout = true;
+
+	panfrost_draw_vbo((struct pipe_context *) ctx, info);
+	panfrost_flush((struct pipe_context *) ctx, NULL, 0);
+
+	dont_scanout = false;
+}
+
+static void
+panfrost_draw_vbo(
+		struct pipe_context *pipe,
+		const struct pipe_draw_info *info)
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+
+	panfrost_maybe_dummy_draw(ctx, info);
+
+	ctx->payload_vertex.draw_start = info->start;
+	ctx->payload_tiler.draw_start = info->start;
+
+	int mode = info->mode;
+
+	/* Fallback for non-ES draw modes */
+
+	if (info->mode >= PIPE_PRIM_QUADS) {
+		mode = PIPE_PRIM_TRIANGLE_STRIP;
+		/*
+		util_primconvert_save_rasterizer_state(ctx->primconvert, &ctx->rasterizer->base);
+		util_primconvert_draw_vbo(ctx->primconvert, info);
+		printf("Fallback\n");
+		return; */
+	}
+
+        ctx->payload_tiler.prefix.draw_mode = g2m_draw_mode(mode);
+
+	ctx->vertex_count = info->count;
+
+	int invocation_count = info->index_size ? (info->start + ctx->vertex_count) : ctx->vertex_count;
+        ctx->payload_vertex.prefix.invocation_count = MALI_POSITIVE(invocation_count);
+        ctx->payload_tiler.prefix.invocation_count = MALI_POSITIVE(invocation_count);
+	
+	/* For higher amounts of vertices (greater than what fits in a 16-bit
+	 * short), the other value is needed, otherwise there will be bizarre
+	 * rendering artefacts. It's not clear what these values mean yet. */
+
+	ctx->payload_tiler.prefix.unknown_draw &= ~(0x3000 | 0x18000);
+	ctx->payload_tiler.prefix.unknown_draw |= (ctx->vertex_count > 65535) ? 0x3000 : 0x18000;
+
+	if (info->index_size) {
+
+	ctx->payload_vertex.draw_start = 0;
+	ctx->payload_tiler.draw_start = 0;
+		//ctx->payload_tiler.prefix.negative_start = -info->start;
+		ctx->payload_tiler.prefix.index_count = MALI_POSITIVE(info->count);
+
+		//assert(!info->restart_index); /* TODO: Research */
+		assert(!info->index_bias);
+		//assert(!info->min_index); /* TODO: Use value */
+
+		ctx->payload_tiler.prefix.unknown_draw |= panfrost_translate_index_size(info->index_size);
+
+		const uint8_t *ibuf8 = panfrost_get_index_buffer_raw(info);
+		
+		ctx->payload_tiler.prefix.indices = panfrost_upload(&ctx->cmdstream, ibuf8 + (info->start * info->index_size), info->count * info->index_size, true);
+	} else {
+		/* Index count == vertex count, if no indexing is applied, as
+		 * if it is internally indexed in the expected order */
+
+		ctx->payload_tiler.prefix.negative_start = 0;
+		ctx->payload_tiler.prefix.index_count = MALI_POSITIVE(ctx->vertex_count);
+
+		/* Reverse index state */
+		ctx->payload_tiler.prefix.unknown_draw &= ~MALI_DRAW_INDEXED_UINT32;
+		ctx->payload_tiler.prefix.indices = (uintptr_t) NULL;
+	}
+
+	/* Fire off the draw itself */
+	panfrost_queue_draw(ctx);
+}
+
+/* CSO state */
+
+static void
+panfrost_generic_cso_delete(struct pipe_context *pctx, void *hwcso)
+{
+	free(hwcso);
+}
+
+static void
+panfrost_set_scissor(struct panfrost_context *ctx)
+{
+	const struct pipe_scissor_state *ss = &ctx->scissor;
+
+	if (ss && ctx->rasterizer && ctx->rasterizer->base.scissor) {
+		ctx->viewport.viewport0[0] = ss->minx;
+		ctx->viewport.viewport0[1] = ss->miny;
+		ctx->viewport.viewport1[0] = MALI_POSITIVE(ss->maxx);
+		ctx->viewport.viewport1[1] = MALI_POSITIVE(ss->maxy);
+	} else {
+		ctx->viewport.viewport0[0] = 0;
+		ctx->viewport.viewport0[1] = 0;
+		ctx->viewport.viewport1[0] = MALI_POSITIVE(ctx->pipe_framebuffer.width);
+		ctx->viewport.viewport1[1] = MALI_POSITIVE(ctx->pipe_framebuffer.height);
+
+	}
+
+	ctx->dirty |= PAN_DIRTY_VIEWPORT;
+}
+
+static void*
+panfrost_create_rasterizer_state(
+		struct pipe_context *pctx,
+		const struct pipe_rasterizer_state *cso)
+{
+	struct panfrost_rasterizer *so = CALLOC_STRUCT(panfrost_rasterizer);
+
+	so->base = *cso;
+
+	/* Bitmask, unknown meaning of the start value */
+#ifdef T8XX
+	so->tiler_gl_enables = 0x7;
+#else
+	so->tiler_gl_enables = 0x105;
+#endif
+
+	so->tiler_gl_enables |= MALI_GL_FRONT_FACE(
+			cso->front_ccw ? MALI_GL_CCW : MALI_GL_CW);
+
+	if (cso->cull_face & PIPE_FACE_FRONT)
+		so->tiler_gl_enables |= MALI_GL_CULL_FACE_FRONT;
+
+	if (cso->cull_face & PIPE_FACE_BACK)
+		so->tiler_gl_enables |= MALI_GL_CULL_FACE_BACK;
+
+	return so;
+}
+
+static void
+panfrost_bind_rasterizer_state(
+		struct pipe_context *pctx,
+		void *hwcso)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+	struct pipe_rasterizer_state *cso = hwcso;
+
+	if (!hwcso) {
+		/* XXX: How to unbind rasterizer state? */
+		return;
+	}
+
+	/* If scissor test has changed, we'll need to update that now */
+	bool update_scissor = !ctx->rasterizer || ctx->rasterizer->base.scissor != cso->scissor;
+
+	ctx->rasterizer = hwcso;
+
+	/* Actualise late changes */
+	if (update_scissor)
+		panfrost_set_scissor(ctx);
+
+	ctx->dirty |= PAN_DIRTY_RASTERIZER;
+}
+
+static void*
+panfrost_create_vertex_elements_state(
+		struct pipe_context *pctx,
+		unsigned num_elements,
+		const struct pipe_vertex_element *elements)
+{
+	struct panfrost_vertex_state *so = CALLOC_STRUCT(panfrost_vertex_state);
+
+	so->num_elements = num_elements;
+	memcpy(so->pipe, elements, sizeof(*elements) * num_elements);
+
+	for (int i = 0; i < num_elements; ++i) {
+		so->hw[i].index = elements[i].vertex_buffer_index;
+		
+		enum pipe_format fmt = elements[i].src_format;
+		const struct util_format_description *desc = util_format_description(fmt);
+		struct util_format_channel_description chan = desc->channel[0];
+
+		int type = 0;
+
+		switch (chan.type) {
+			case UTIL_FORMAT_TYPE_UNSIGNED:
+			case UTIL_FORMAT_TYPE_SIGNED:
+				if (chan.size == 8)
+					type = MALI_ATYPE_BYTE;
+				else if (chan.size == 16)
+					type = MALI_ATYPE_SHORT;
+				else if (chan.size == 32)
+					type = MALI_ATYPE_INT;
+				else {
+					printf("BAD INT SIZE %d\n", chan.size);
+					assert(0);
+				}
+
+				break;
+
+			case UTIL_FORMAT_TYPE_FLOAT:
+				type = MALI_ATYPE_FLOAT;
+				break;
+
+			default:
+				printf("Unknown atype %d\n", chan.type);
+				assert(0);
+		}
+
+		so->hw[i].type = type;
+		so->nr_components[i] = desc->nr_channels;
+		so->hw[i].nr_components = MALI_POSITIVE(4); /* XXX: Why is this needed? */
+                so->hw[i].not_normalised = !chan.normalized;
+
+		/* Bit used for both signed/unsigned and full/half designation */
+		so->hw[i].is_int_signed =
+			(chan.type == UTIL_FORMAT_TYPE_SIGNED) ? 1 :
+			(chan.type == UTIL_FORMAT_TYPE_FLOAT && chan.size != 32) ? 1 :
+			0;
+
+                so->hw[i].unknown1 = 0x2a22;
+                so->hw[i].unknown2 = 0x1;
+
+		/* The field itself should probably be shifted over */
+                so->hw[i].src_offset = elements[i].src_offset;
+	}
+
+	return so;
+}
+
+static void
+panfrost_bind_vertex_elements_state(
+		struct pipe_context *pctx,
+		void *hwcso)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+
+	ctx->vertex = hwcso;
+	ctx->dirty |= PAN_DIRTY_VERTEX;
+}
+
+static void *
+panfrost_create_shader_state(
+		struct pipe_context *pctx,
+		const struct pipe_shader_state *cso)
+{
+	struct panfrost_shader_state *so = CALLOC_STRUCT(panfrost_shader_state);
+	so->base = *cso;
+	
+	/* Token deep copy to prevent memory corruption */
+
+	if (cso->type == PIPE_SHADER_IR_TGSI)
+		so->base.tokens = tgsi_dup_tokens(so->base.tokens);
+
+	return so;
+}
+
+static void
+panfrost_delete_shader_state(
+		struct pipe_context *pctx,
+		void *so)
+{
+	free(so);
+}
+
+static void *
+panfrost_create_sampler_state(
+		struct pipe_context *pctx,
+		const struct pipe_sampler_state *cso)
+{
+	struct panfrost_sampler_state *so = CALLOC_STRUCT(panfrost_sampler_state);
+	so->base = *cso;
+
+	/* sampler_state corresponds to mali_sampler_descriptor, which we can generate entirely here */
+
+	struct mali_sampler_descriptor sampler_descriptor = {
+		.filter_mode = MALI_GL_TEX_MIN(translate_tex_filter(cso->min_img_filter))
+			     | MALI_GL_TEX_MAG(translate_tex_filter(cso->mag_img_filter))
+			     | translate_mip_filter(cso->min_mip_filter)
+			     | 0x20,
+
+		.wrap_s = translate_tex_wrap(cso->wrap_s),
+		.wrap_t = translate_tex_wrap(cso->wrap_t),
+		.wrap_r = translate_tex_wrap(cso->wrap_r),
+		.compare_func = panfrost_translate_alt_compare_func(cso->compare_func),
+		.border_color = {
+			cso->border_color.f[0],
+			cso->border_color.f[1],
+			cso->border_color.f[2],
+			cso->border_color.f[3]
+		},
+		.min_lod = FIXED_16(0.0),
+		.max_lod = FIXED_16(31.0),
+		.unknown2 = 1,
+	};
+
+	so->hw = sampler_descriptor;
+
+	return so;
+}
+
+static void
+panfrost_bind_sampler_states(
+		struct pipe_context *pctx,
+		enum pipe_shader_type shader,
+		unsigned start_slot, unsigned num_sampler,
+		void **sampler)
+{
+	assert(start_slot == 0);
+
+	struct panfrost_context *ctx = panfrost_context(pctx);
+	
+	/* XXX: Should upload, not just copy? */
+	ctx->sampler_count[shader] = num_sampler;
+	memcpy(ctx->samplers[shader], sampler, num_sampler * sizeof (void *));
+
+	ctx->dirty |= PAN_DIRTY_SAMPLERS;
+}
+
+static void
+panfrost_bind_fs_state(
+		struct pipe_context *pctx,
+		void *hwcso)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+
+	ctx->fs = hwcso;
+
+	if (hwcso) {
+		if (!ctx->fs->compiled) {
+			panfrost_shader_compile(ctx, &ctx->fs->tripipe, NULL, JOB_TYPE_TILER, hwcso);
+			ctx->fs->compiled = true;
+		}
+	}
+
+	ctx->dirty |= PAN_DIRTY_FS;
+}
+
+static void
+panfrost_bind_vs_state(
+		struct pipe_context *pctx,
+		void *hwcso)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+
+	ctx->vs = hwcso;
+
+	if (hwcso) {
+		if (!ctx->vs->compiled) {
+			panfrost_shader_compile(ctx, &ctx->vs->tripipe, NULL, JOB_TYPE_VERTEX, hwcso);
+			ctx->vs->compiled = true;
+		}
+	} 
+
+	ctx->dirty |= PAN_DIRTY_VS;
+}
+
+static void
+panfrost_set_vertex_buffers(
+		struct pipe_context *pctx,
+		unsigned start_slot,
+		unsigned num_buffers,
+		const struct pipe_vertex_buffer *buffers)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+	assert(num_buffers <= PIPE_MAX_ATTRIBS);
+
+	/* XXX: Dirty tracking? etc */
+	if (buffers) {
+		size_t sz = sizeof(buffers[0]) * num_buffers;
+		ctx->vertex_buffers = malloc(sz);
+		ctx->vertex_buffer_count = num_buffers;
+		memcpy(ctx->vertex_buffers, buffers, sz);
+	} else {
+		/* XXX leak */
+		ctx->vertex_buffers = NULL;
+		ctx->vertex_buffer_count = 0;
+	}
+}
+
+static void
+panfrost_set_constant_buffer(
+		struct pipe_context *pctx,
+		enum pipe_shader_type shader, uint index,
+		const struct pipe_constant_buffer *buf)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+	struct panfrost_constant_buffer *pbuf = &ctx->constant_buffer[shader];
+
+	size_t sz = buf ? buf->buffer_size : 0;
+
+	/* Free previous buffer */
+
+	pbuf->dirty = true;
+	pbuf->size = sz;
+
+	if (pbuf->buffer) {
+		free(pbuf->buffer);
+		pbuf->buffer = NULL;
+	}
+
+	/* If unbinding, we're done */
+
+	if (!buf)
+		return;
+
+	/* Multiple constant buffers not yet supported */
+	assert(index == 0);
+
+	const void *cpu;
+
+	struct panfrost_resource *rsrc = (struct panfrost_resource *) (buf->buffer);
+
+	if (rsrc) {
+		cpu = rsrc->cpu[0];
+	} else if (buf->user_buffer) {
+		cpu = buf->user_buffer;
+	} else {
+		printf("No constant buffer?\n");
+		return;
+	}
+
+	/* Copy the constant buffer into the driver context for later upload */
+
+	pbuf->buffer = malloc(sz);
+	memcpy(pbuf->buffer, cpu, sz);
+}
+
+static void
+panfrost_set_stencil_ref(
+		struct pipe_context *pctx,
+		const struct pipe_stencil_ref *ref)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+	ctx->stencil_ref = *ref;
+
+	/* Shader core dirty */
+	ctx->dirty |= PAN_DIRTY_FS;
+}
+
+static struct pipe_sampler_view *
+panfrost_create_sampler_view(
+		struct pipe_context *pctx,
+		struct pipe_resource *texture,
+		const struct pipe_sampler_view *template)
+{
+	struct panfrost_sampler_view *so = CALLOC_STRUCT(panfrost_sampler_view);
+
+	pipe_reference(NULL, &texture->reference);
+
+	struct panfrost_resource *prsrc = (struct panfrost_resource *) texture;
+
+	so->base = *template;
+	so->base.texture = texture;
+	so->base.reference.count = 1;
+	so->base.context = pctx;
+
+	/* sampler_views correspond to texture descriptors, minus the texture
+	 * (data) itself. So, we serialise the descriptor here and cache it for
+	 * later. */
+
+	/* TODO: Other types of textures */
+	assert(template->target == PIPE_TEXTURE_2D);
+
+	/* Make sure it's something with which we're familiar */
+	assert(prsrc->bytes_per_pixel >= 1 && prsrc->bytes_per_pixel <= 4);
+	
+	/* TODO: Detect from format better */
+	bool depth = prsrc->base.format == PIPE_FORMAT_Z32_UNORM;
+	bool has_alpha = true;
+	bool alpha_only = prsrc->base.format == PIPE_FORMAT_A8_UNORM;
+
+	struct mali_texture_descriptor texture_descriptor = {
+		.width = MALI_POSITIVE(texture->width0),
+		.height = MALI_POSITIVE(texture->height0),
+		.depth = MALI_POSITIVE(texture->depth0),
+
+		/* TODO: Decode */
+		.format = {
+			.bottom = alpha_only ? 0x24 : ((depth ? 0x20 : 0x88)),
+			.unk1 = alpha_only ? 0x1 : (has_alpha ? 0x6 : 0xb),
+			.component_size = depth ? 0x5 : 0x3,
+			.nr_channels = MALI_POSITIVE((depth ? 2 : prsrc->bytes_per_pixel)),
+			.typeA = depth ? 2 : 5,
+
+			.usage1 = 0x0,
+			.is_not_cubemap = 1,
+
+			/* 0x11 - regular texture 2d, uncompressed tiled */
+			/* 0x12 - regular texture 2d, uncompressed linear */
+			/* 0x1c - AFBC compressed (internally tiled, probably) texture 2D */
+
+			.usage2 = prsrc->has_afbc ? 0x1c : (prsrc->tiled ? 0x11 : 0x12),
+		},
+
+		.swizzle_r = panfrost_translate_texture_swizzle(template->swizzle_r),
+		.swizzle_g = panfrost_translate_texture_swizzle(template->swizzle_g),
+		.swizzle_b = panfrost_translate_texture_swizzle(template->swizzle_b),
+		.swizzle_a = panfrost_translate_texture_swizzle(template->swizzle_a),
+	};
+
+	/* TODO: Other base levels require adjusting dimensions / level numbers / etc */
+	assert (template->u.tex.first_level == 0);
+	
+	texture_descriptor.nr_mipmap_levels = template->u.tex.last_level - template->u.tex.first_level;
+
+	so->hw = texture_descriptor;
+
+	return (struct pipe_sampler_view *) so;
+}
+
+static void
+panfrost_set_sampler_views(
+		struct pipe_context *pctx,
+		enum pipe_shader_type shader,
+		unsigned start_slot, unsigned num_views,
+		struct pipe_sampler_view **views)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+
+	assert(start_slot == 0);
+
+	ctx->sampler_view_count[shader] = num_views;
+	memcpy(ctx->sampler_views[shader], views, num_views * sizeof (void *));
+
+	ctx->dirty |= PAN_DIRTY_TEXTURES;
+}
+
+static void
+panfrost_sampler_view_destroy(
+		struct pipe_context *pctx,
+		struct pipe_sampler_view *views)
+{
+	//struct panfrost_context *ctx = panfrost_context(pctx);
+
+	/* TODO */
+	
+	free(views);
+}
+
+/* TODO: Proper resource tracking depends on, well, proper resources. This
+ * section will be woefully incomplete until we can sort out a proper DRM
+ * driver. */
+
+struct pipe_resource *
+panfrost_resource_create_front(struct pipe_screen *screen,
+			       const struct pipe_resource *template,
+			       const void *map_front_private)
+{
+	struct panfrost_resource *so = CALLOC_STRUCT(panfrost_resource);
+	struct panfrost_screen *pscreen = (struct panfrost_screen *) screen;
+
+	so->base = *template;
+	so->base.screen = screen;
+
+	pipe_reference_init(&so->base.reference, 1);
+
+	/* Fill out fields based on format itself */
+	so->bytes_per_pixel = util_format_get_blocksize(template->format);
+
+	/* TODO: Alignment? */
+	so->stride = so->bytes_per_pixel * template->width0;
+	
+	size_t sz = so->stride;
+
+	if (template->height0) sz *= template->height0;
+	if (template->depth0) sz *= template->depth0;
+
+	if ((template->bind & PIPE_BIND_RENDER_TARGET) || (template->bind & PIPE_BIND_DEPTH_STENCIL)) {
+		if (template->bind & PIPE_BIND_DISPLAY_TARGET) {
+			/* TODO: Allocate display target surface */
+			so->cpu[0] = pscreen->any_context->framebuffer.cpu;
+			so->gpu[0] = pscreen->any_context->framebuffer.gpu;
+		} else {
+			/* TODO: Mipmapped RTs */
+			//assert(template->last_level == 0);
+
+			/* Allocate the framebuffer as its own slab of GPU-accessible memory */
+			struct panfrost_memory slab;
+			panfrost_allocate_slab(pscreen->any_context, &slab, (sz / 4096) + 1, true, false, 0, 0, 0);
+
+			/* Make the resource out of the slab */
+			so->cpu[0] = slab.cpu;
+			so->gpu[0] = slab.gpu;
+		}
+	} else {
+		/* TODO: For linear resources, allocate straight on the cmdstream for
+		 * zero-copy operation */
+
+		/* Tiling textures is almost always faster, unless we only use it once */
+		so->tiled = (template->usage != PIPE_USAGE_STREAM);
+
+		if (so->tiled) {
+			/* For tiled, we don't map directly, so just malloc any old buffer */
+
+			for (int l = 0; l < (template->last_level + 1); ++l) {
+				so->cpu[l] = malloc(sz);
+				//sz >>= 2;
+			}
+		} else {
+			/* But for linear, we can! */
+
+			struct panfrost_memory slab;
+			panfrost_allocate_slab(pscreen->any_context, &slab, (sz / 4096) + 1, true, true, 0, 0, 0);
+
+			/* Make the resource out of the slab */
+			so->cpu[0] = slab.cpu;
+			so->gpu[0] = slab.gpu;
+		}
+	}
+
+	return (struct pipe_resource *)so;
+}
+
+static struct pipe_resource *
+panfrost_resource_create(struct pipe_screen *screen,
+		const struct pipe_resource *templat)
+{
+	return panfrost_resource_create_front(screen, templat, NULL);
+}
+
+static void
+panfrost_resource_destroy(struct pipe_screen *screen,
+		struct pipe_resource *pt)
+{
+	printf("--resource destroy--\n");
+	/* TODO */
+}
+
+static void *
+panfrost_transfer_map(struct pipe_context *pctx,
+                      struct pipe_resource *resource,
+                      unsigned level,
+                      unsigned usage,  /* a combination of PIPE_TRANSFER_x */
+                      const struct pipe_box *box,
+                      struct pipe_transfer **out_transfer)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+	struct panfrost_resource *rsrc = (struct panfrost_resource *) resource;
+
+	struct pipe_transfer *transfer = CALLOC_STRUCT(pipe_transfer);
+
+	transfer->level = level;
+	transfer->usage = usage;
+	transfer->box = *box;
+	transfer->stride = rsrc->stride;
+	assert(!transfer->box.z);
+
+        pipe_resource_reference(&transfer->resource, resource);
+
+	*out_transfer = transfer;
+
+	/* If non-zero level, it's a mipmapped resource and needs to be treated as such */
+	rsrc->is_mipmap |= transfer->level;
+
+	if (transfer->usage & PIPE_TRANSFER_MAP_DIRECTLY) {
+		/* We cannot directly map tiled textures */
+
+		if (rsrc->tiled)
+			return NULL;
+
+		/* Otherwise, we're good to go! */
+		rsrc->mapped_direct = true;
+	}
+
+	if (resource->bind & PIPE_BIND_DISPLAY_TARGET) {
+		/* Mipmapped readpixels?! */
+		assert(level == 0);
+
+		/* Set the CPU mapping to that of the framebuffer in memory, untiled */
+		rsrc->cpu[level] = ctx->framebuffer.cpu;
+
+		/* Force a flush -- kill the pipeline */
+		panfrost_flush(pctx, NULL, PIPE_FLUSH_END_OF_FRAME);
+	} else if (resource->bind & PIPE_BIND_DEPTH_STENCIL) {
+		/* Mipmapped readpixels?! */
+		assert(level == 0);
+
+		/* Set the CPU mapping to that of the depth/stencil buffer in memory, untiled */
+		rsrc->cpu[level] = ctx->depth_stencil_buffer.cpu;
+	}
+
+	return rsrc->cpu[level] + transfer->box.x * rsrc->bytes_per_pixel + transfer->box.y * transfer->stride;
+}
+
+static void
+panfrost_set_framebuffer_state(struct pipe_context *pctx,
+                               const struct pipe_framebuffer_state *fb)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+
+	/* Flush when switching away from an FBO */
+
+	if (!panfrost_is_scanout(ctx)) {
+		panfrost_flush(pctx, NULL, 0);
+	}
+
+	ctx->pipe_framebuffer.nr_cbufs = fb->nr_cbufs;
+	ctx->pipe_framebuffer.samples = fb->samples;
+	ctx->pipe_framebuffer.layers = fb->layers;
+	ctx->pipe_framebuffer.width = fb->width;
+	ctx->pipe_framebuffer.height = fb->height;
+
+	for (int i = 0; i < PIPE_MAX_COLOR_BUFS; i++) {
+		struct pipe_surface *cb = i < fb->nr_cbufs ? fb->cbufs[i] : NULL;
+
+		/* check if changing cbuf */
+		if (ctx->pipe_framebuffer.cbufs[i] == cb) continue;
+
+		if (cb && (i != 0)) {
+			printf("XXX: Multiple render targets not supported before t7xx!\n");
+			assert(0);
+		}
+
+		/* assign new */
+		pipe_surface_reference(&ctx->pipe_framebuffer.cbufs[i], cb);
+
+		if (!cb)
+			continue;
+
+		bool is_scanout = panfrost_is_scanout(ctx);
+
+		if (is_scanout) {
+			/* Lie to use our own */
+			((struct panfrost_resource *) ctx->pipe_framebuffer.cbufs[i]->texture)->gpu[0] = ctx->framebuffer.gpu;
+			ctx->pipe_framebuffer.width = 2048;
+                        ctx->pipe_framebuffer.height = 1280;
+		} 
+
+		ctx->vt_framebuffer = panfrost_emit_fbd(ctx);
+		panfrost_attach_vt_framebuffer(ctx);
+		panfrost_new_frag_framebuffer(ctx);
+		panfrost_set_scissor(ctx);
+
+		struct panfrost_resource *tex = ((struct panfrost_resource *) ctx->pipe_framebuffer.cbufs[i]->texture);
+
+		if (!is_scanout && !tex->has_afbc) {
+			/* The blob is aggressive about enabling AFBC. As such,
+			 * it's pretty much necessary to use it here, since we
+			 * have no traces of non-compressed FBO. */
+
+			panfrost_enable_afbc(ctx, tex, false);
+		}
+
+		if (is_scanout && !tex->has_checksum && USE_TRANSACTION_ELIMINATION) {
+			/* Enable transaction elimination if we can */
+			panfrost_enable_checksum(ctx, tex);
+		}
+	}
+
+	{
+		struct pipe_surface *zb = fb->zsbuf;
+
+		if (ctx->pipe_framebuffer.zsbuf != zb) {
+			pipe_surface_reference(&ctx->pipe_framebuffer.zsbuf, zb);
+
+			if (zb) {
+				/* FBO has depth */
+
+				ctx->vt_framebuffer = panfrost_emit_fbd(ctx);
+				panfrost_attach_vt_framebuffer(ctx);
+				panfrost_new_frag_framebuffer(ctx);
+				panfrost_set_scissor(ctx);
+
+				struct panfrost_resource *tex = ((struct panfrost_resource *) ctx->pipe_framebuffer.zsbuf->texture);
+
+				if (!tex->has_afbc && !panfrost_is_scanout(ctx))
+					panfrost_enable_afbc(ctx, tex, true);
+			}
+		}
+	}
+
+	/* Force a clear XXX wrong? */
+	if (ctx->last_clear.color)
+		panfrost_clear(&ctx->base, ctx->last_clear.buffers, ctx->last_clear.color, ctx->last_clear.depth, ctx->last_clear.stencil);
+
+	/* Don't consider the buffer dirty */
+	ctx->dirty &= ~PAN_DIRTY_DUMMY;
+}
+
+static void *
+panfrost_create_blend_state(struct pipe_context *pipe,
+                            const struct pipe_blend_state *blend)
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+	struct panfrost_blend_state *so = CALLOC_STRUCT(panfrost_blend_state);
+	so->base = *blend;
+
+	/* TODO: The following features are not yet implemented */
+	assert(!blend->logicop_enable);
+	assert(!blend->alpha_to_coverage);
+	assert(!blend->alpha_to_one);
+
+	/* Compile the blend state, first as fixed-function if we can */
+
+	if (panfrost_make_fixed_blend_mode(&blend->rt[0], &so->equation, blend->rt[0].colormask, &ctx->blend_color))
+		return so;
+
+	/* If we can't, compile a blend shader instead */
+
+	panfrost_make_blend_shader(ctx, so, &ctx->blend_color);
+
+	return so;
+}
+
+static void
+panfrost_bind_blend_state(struct pipe_context *pipe,
+                          void *cso)
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+	struct pipe_blend_state *blend = (struct pipe_blend_state *) cso;
+	struct panfrost_blend_state *pblend = (struct panfrost_blend_state *) cso;
+	ctx->blend = pblend;
+
+	if (!blend)
+		return;
+
+	SET_BIT(ctx->fragment_shader_core.unknown2_4, MALI_NO_DITHER, !blend->dither);
+
+	/* TODO: Attach color */
+
+	/* Shader itself is not dirty, but the shader core is */
+	ctx->dirty |= PAN_DIRTY_FS;
+}
+
+static void
+panfrost_delete_blend_state(struct pipe_context *pipe,
+                            void *blend)
+{
+	free(blend);
+}
+
+static void
+panfrost_set_blend_color(struct pipe_context *pipe,
+                         const struct pipe_blend_color *blend_color)
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+
+	/* If blend_color is we're unbinding, so ctx->blend_color is now undefined -> nothing to do */
+
+	if (blend_color) {
+		ctx->blend_color = *blend_color;
+
+		/* The blend mode depends on the blend constant color, due to the
+		 * fixed/programmable split. So, we're forced to regenerate the blend
+		 * equation */
+
+		/* TODO: Attach color */
+	}
+}
+
+static void *
+panfrost_create_depth_stencil_state(struct pipe_context *pipe,
+                                    const struct pipe_depth_stencil_alpha_state *depth_stencil)
+{
+   return mem_dup(depth_stencil, sizeof(*depth_stencil));
+}
+
+static void
+panfrost_bind_depth_stencil_state(struct pipe_context *pipe,
+					void *cso)		
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+	struct pipe_depth_stencil_alpha_state *depth_stencil = cso;
+	ctx->depth_stencil = depth_stencil;
+
+	if (!depth_stencil)
+		return;
+
+	/* Alpha does not exist on ES2... */
+	assert(!depth_stencil->alpha.enabled);
+
+	/* Stencil state */
+	SET_BIT(ctx->fragment_shader_core.unknown2_4, MALI_STENCIL_TEST, depth_stencil->stencil[0].enabled); /* XXX: which one? */
+
+	panfrost_make_stencil_state(&depth_stencil->stencil[0], &ctx->fragment_shader_core.stencil_front);
+	ctx->fragment_shader_core.stencil_mask_front = depth_stencil->stencil[0].writemask;
+
+	panfrost_make_stencil_state(&depth_stencil->stencil[1], &ctx->fragment_shader_core.stencil_back);
+	ctx->fragment_shader_core.stencil_mask_back = depth_stencil->stencil[1].writemask;
+
+	/* Depth state (TODO: Refactor) */
+	SET_BIT(ctx->fragment_shader_core.unknown2_3, MALI_DEPTH_TEST, depth_stencil->depth.enabled);
+	
+	int func = depth_stencil->depth.enabled ? depth_stencil->depth.func : PIPE_FUNC_ALWAYS;
+
+	ctx->fragment_shader_core.unknown2_3 &= ~MALI_DEPTH_FUNC_MASK;
+	ctx->fragment_shader_core.unknown2_3 |= MALI_DEPTH_FUNC(panfrost_translate_compare_func(func));
+
+	/* Bounds test not implemented */
+	assert(!depth_stencil->depth.bounds_test);
+
+	ctx->dirty |= PAN_DIRTY_FS;
+}
+
+static void
+panfrost_delete_depth_stencil_state(struct pipe_context *pipe, void *depth)
+{
+   free( depth );
+}
+
+static void
+panfrost_set_sample_mask(struct pipe_context *pipe,
+                         unsigned sample_mask)
+{
+}
+
+static struct pipe_surface *
+panfrost_create_surface(struct pipe_context *pipe,
+                        struct pipe_resource *pt,
+                        const struct pipe_surface *surf_tmpl)
+{
+   struct pipe_surface *ps = NULL;
+
+   ps = CALLOC_STRUCT(pipe_surface);
+   if (ps) {
+      pipe_reference_init(&ps->reference, 1);
+      pipe_resource_reference(&ps->texture, pt);
+      ps->context = pipe;
+      ps->format = surf_tmpl->format;
+      if (pt->target != PIPE_BUFFER) {
+         assert(surf_tmpl->u.tex.level <= pt->last_level);
+         ps->width = u_minify(pt->width0, surf_tmpl->u.tex.level);
+         ps->height = u_minify(pt->height0, surf_tmpl->u.tex.level);
+         ps->u.tex.level = surf_tmpl->u.tex.level;
+         ps->u.tex.first_layer = surf_tmpl->u.tex.first_layer;
+         ps->u.tex.last_layer = surf_tmpl->u.tex.last_layer;
+      }
+      else {
+         /* setting width as number of elements should get us correct renderbuffer width */
+         ps->width = surf_tmpl->u.buf.last_element - surf_tmpl->u.buf.first_element + 1;
+         ps->height = pt->height0;
+         ps->u.buf.first_element = surf_tmpl->u.buf.first_element;
+         ps->u.buf.last_element = surf_tmpl->u.buf.last_element;
+         assert(ps->u.buf.first_element <= ps->u.buf.last_element);
+         assert(ps->u.buf.last_element < ps->width);
+      }
+   }
+
+   return ps;
+}
+
+static void 
+panfrost_surface_destroy(struct pipe_context *pipe,
+                         struct pipe_surface *surf)
+{
+   assert(surf->texture);
+   pipe_resource_reference(&surf->texture, NULL);
+   free(surf);
+}
+
+static void
+panfrost_set_clip_state(struct pipe_context *pipe,
+                        const struct pipe_clip_state *clip)
+{
+	//struct panfrost_context *panfrost = panfrost_context(pipe);
+}
+
+static void
+panfrost_set_viewport_states(struct pipe_context *pipe,
+                             unsigned start_slot,
+                             unsigned num_viewports,
+                             const struct pipe_viewport_state *viewports)
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+
+	assert(start_slot == 0);
+	assert(num_viewports == 1);
+
+	ctx->pipe_viewport = *viewports;
+
+	/* TODO: What if not centered? */
+	float w = abs(viewports->scale[0]) * 2.0;
+	float h = abs(viewports->scale[1]) * 2.0;
+
+	ctx->viewport.viewport1[0] = MALI_POSITIVE((int) w);
+	ctx->viewport.viewport1[1] = MALI_POSITIVE((int) h);
+}
+
+static void
+panfrost_set_scissor_states(struct pipe_context *pipe,
+                            unsigned start_slot,
+                            unsigned num_scissors,
+                            const struct pipe_scissor_state *scissors)
+{
+	struct panfrost_context *ctx = panfrost_context(pipe);
+
+	assert(start_slot == 0);
+	assert(num_scissors == 1);
+
+	ctx->scissor = *scissors;
+
+	panfrost_set_scissor(ctx);
+}
+
+static void
+panfrost_set_polygon_stipple(struct pipe_context *pipe,
+                             const struct pipe_poly_stipple *stipple)
+{
+   //struct panfrost_context *panfrost = panfrost_context(pipe);
+}
+
+static void
+panfrost_set_active_query_state(struct pipe_context *pipe,
+                             boolean enable)
+{
+   //struct panfrost_context *panfrost = panfrost_context(pipe);
+}
+
+static void
+panfrost_destroy(struct pipe_context *pipe)
+{
+	struct panfrost_context *panfrost = panfrost_context(pipe);
+
+	if (panfrost->blitter)
+		util_blitter_destroy(panfrost->blitter);
+}
+
+static void
+panfrost_tile_texture(struct panfrost_context *ctx, struct panfrost_resource *rsrc, int level)
+{
+	/* If we're direct mapped, we're done; don't do any swizzling / copies / etc */
+	if (rsrc->mapped_direct)
+		return;
+
+	int width = rsrc->base.width0 >> level;
+	int height = rsrc->base.height0 >> level;
+
+	/* Estimate swizzled bitmap size. Slight overestimates are fine.
+	 * Underestimates will result in memory corruption or worse. */
+
+	int swizzled_sz = panfrost_swizzled_size(width, height, rsrc->bytes_per_pixel);
+
+	/* Allocate the transfer given that known size but do not copy */
+	uint8_t *swizzled = panfrost_allocate_transfer(&ctx->textures, swizzled_sz, &rsrc->gpu[level]);
+
+	if (rsrc->tiled) {
+		/* Run actual texture swizzle, writing directly to the mapped
+		 * GPU chunk we allocated */
+
+		panfrost_texture_swizzle(width, height, rsrc->bytes_per_pixel, rsrc->stride, rsrc->cpu[level], swizzled);
+	} else {
+		/* If indirect linear, just do a dumb copy */
+
+		memcpy(swizzled, rsrc->cpu[level], rsrc->stride * height);
+	}
+}
+
+static void
+panfrost_transfer_unmap(struct pipe_context *pctx,
+                       struct pipe_transfer *transfer)
+{
+	struct panfrost_context *ctx = panfrost_context(pctx);
+
+	if (transfer->usage & PIPE_TRANSFER_WRITE) {
+		if (transfer->resource->target == PIPE_TEXTURE_2D) {
+			struct panfrost_resource *prsrc = (struct panfrost_resource *) transfer->resource;
+
+			/* Gallium thinks writeback happens here; instead, this is our cue to tile */
+			assert(!prsrc->has_afbc);
+			panfrost_tile_texture(ctx, prsrc, transfer->level);
+		}
+	}
+
+	/* Derefence the resource */
+        pipe_resource_reference(&transfer->resource, NULL);
+
+	/* Transfer itself is CALLOCed at the moment */
+	free(transfer);
+}
+
+static void panfrost_blit(struct pipe_context *pipe,
+                    const struct pipe_blit_info *info)
+{
+   /* STUB */
+   printf("Skipping blit XXX\n");
+   return;
+}
+
+static void
+panfrost_allocate_slab(struct panfrost_context *ctx,
+		    struct panfrost_memory *mem,
+		    size_t pages,
+		    bool mapped,
+		    bool same_va,
+		    int extra_flags,
+		    int commit_count,
+		    int extent)
+{
+	int flags = MALI_MEM_PROT_CPU_RD | MALI_MEM_PROT_CPU_WR | MALI_MEM_PROT_GPU_RD | MALI_MEM_PROT_GPU_WR;
+
+	flags |= extra_flags;
+
+	/* w+x are mutually exclusive */
+	if (extra_flags & MALI_MEM_PROT_GPU_EX)
+		flags &= ~MALI_MEM_PROT_GPU_WR;
+
+	if (same_va)
+		flags |= MALI_MEM_SAME_VA;
+
+	if (commit_count || extent)
+		pandev_general_allocate(ctx->fd, pages, commit_count, extent, flags, &mem->gpu);
+	else
+		pandev_standard_allocate(ctx->fd, pages, flags, &mem->gpu);
+
+	mem->size = pages * 4096;
+
+	/* mmap for 64-bit, mmap64 for 32-bit. ironic, I know */
+	if (mapped) {
+		if ((mem->cpu = mmap(NULL, mem->size, 3, 1, ctx->fd, mem->gpu)) == MAP_FAILED) {
+			perror("mmap");
+			abort();
+		}
+	}
+
+	mem->stack_bottom = 0;
+}
+
+/* Setups a framebuffer, either by itself (with the independent slow-fb
+ * interface) or from an existing pointer (for shared memory, from the winsys)
+ * */
+
+static void
+panfrost_setup_framebuffer(struct panfrost_context *ctx, int width, int height)
+{
+	/* drisw rounds the stride */
+	int rw = 16.0 * (int) ceil((float) width / 16.0);
+
+	size_t framebuffer_sz = rw * height * 4;
+	posix_memalign((void **) &ctx->framebuffer.cpu, CACHE_LINE_SIZE, framebuffer_sz);
+	struct slowfb_info info = slowfb_init((uint8_t*) (ctx->framebuffer.cpu), rw, height);
+
+	/* May not be the same as our original alloc if we're using XShm, etc */
+	ctx->framebuffer.cpu = info.framebuffer;
+
+	struct mali_mem_import_user_buffer framebuffer_handle = { .ptr = (uint64_t) (uintptr_t) ctx->framebuffer.cpu, .length = framebuffer_sz };
+
+	struct mali_ioctl_mem_import framebuffer_import = {
+		.phandle = (uint64_t) (uintptr_t) &framebuffer_handle,
+		.type = MALI_MEM_IMPORT_TYPE_USER_BUFFER,
+		.flags = MALI_MEM_PROT_CPU_RD | MALI_MEM_PROT_CPU_WR | MALI_MEM_PROT_GPU_RD | MALI_MEM_PROT_GPU_WR | MALI_MEM_IMPORT_SHARED,
+	};
+
+	pandev_ioctl(ctx->fd, MALI_IOCTL_MEM_IMPORT, &framebuffer_import);
+
+	/* It feels like this mmap is backwards :p */
+	uint64_t gpu_addr = (uint64_t) mmap(NULL, framebuffer_import.va_pages * 4096, 3, 1, ctx->fd, framebuffer_import.gpu_va);
+
+	ctx->framebuffer.gpu = gpu_addr;
+	ctx->framebuffer.size = info.stride * height;
+	ctx->scanout_stride = info.stride;
+
+	ctx->pipe_framebuffer.nr_cbufs = 1;
+	ctx->pipe_framebuffer.width = width;
+	ctx->pipe_framebuffer.height = height;
+}
+
+static void
+panfrost_setup_hardware(struct panfrost_context *ctx)
+{
+	ctx->fd = pandev_open();
+
+#ifdef USE_SLOWFB
+	panfrost_setup_framebuffer(ctx, 2048, 1280);
+#endif
+
+	for (int i = 0; i < sizeof(ctx->cmdstream_rings) / sizeof(ctx->cmdstream_rings[0]); ++i)
+		panfrost_allocate_slab(ctx, &ctx->cmdstream_rings[i], 8*64*8*16, true, true, 0, 0, 0);
+
+	panfrost_allocate_slab(ctx, &ctx->cmdstream_persistent, 8*64*8*2, true, true, 0, 0, 0);
+	panfrost_allocate_slab(ctx, &ctx->textures, 4*64*64*4, true, true, 0, 0, 0);
+	panfrost_allocate_slab(ctx, &ctx->scratchpad, 64, true, true, 0, 0, 0);
+	panfrost_allocate_slab(ctx, &ctx->varying_mem, 16384, false, true, 0, 0, 0);
+	panfrost_allocate_slab(ctx, &ctx->shaders, 4096, true, false, MALI_MEM_PROT_GPU_EX, 0, 0);
+	panfrost_allocate_slab(ctx, &ctx->tiler_heap, 32768, false, false, 0, 0, 0);
+	panfrost_allocate_slab(ctx, &ctx->misc_0, 128, false, false, 0, 0, 0);
+
+}
+
+static const struct u_transfer_vtbl transfer_vtbl = {
+        .resource_create          = panfrost_resource_create,
+        .resource_destroy         = panfrost_resource_destroy,
+        .transfer_map             = panfrost_transfer_map,
+        .transfer_unmap           = panfrost_transfer_unmap,
+        .transfer_flush_region    = u_default_transfer_flush_region,
+        //.get_internal_format      = panfrost_resource_get_internal_format,
+        //.set_stencil              = panfrost_resource_set_stencil,
+        //.get_stencil              = panfrost_resource_get_stencil,
+};
+
+/* New context creation, which also does hardware initialisation since I don't
+ * know the better way to structure this :smirk: */
+
+struct pipe_context *
+panfrost_create_context(struct pipe_screen *screen, void *priv, unsigned flags)
+{
+	screen->resource_create = panfrost_resource_create;
+	screen->resource_destroy = panfrost_resource_destroy;
+	screen->resource_create_front = panfrost_resource_create_front;
+	screen->transfer_helper = u_transfer_helper_create(&transfer_vtbl, true, true, true, true);
+
+	struct panfrost_context *ctx = CALLOC_STRUCT(panfrost_context);
+	memset(ctx, 0, sizeof(*ctx));
+	struct pipe_context *gallium = (struct pipe_context *) ctx;
+
+	struct panfrost_screen *pscreen = (struct panfrost_screen *) screen;
+	if (!pscreen->any_context)
+		pscreen->any_context = ctx;
+
+	gallium->screen = screen;
+
+	gallium->destroy = panfrost_destroy;
+
+	gallium->set_framebuffer_state = panfrost_set_framebuffer_state;
+
+	gallium->transfer_map = panfrost_transfer_map;
+	gallium->transfer_unmap = panfrost_transfer_unmap;
+
+	gallium->transfer_flush_region = u_transfer_helper_transfer_flush_region;
+	gallium->buffer_subdata = u_default_buffer_subdata;
+	gallium->texture_subdata = u_default_texture_subdata;
+	gallium->clear_texture = util_clear_texture;
+
+	gallium->create_surface = panfrost_create_surface;
+	gallium->surface_destroy = panfrost_surface_destroy;
+
+	gallium->flush = panfrost_flush;
+	gallium->clear = panfrost_clear;
+	gallium->draw_vbo = panfrost_draw_vbo;
+
+	gallium->set_vertex_buffers = panfrost_set_vertex_buffers;
+	gallium->set_constant_buffer = panfrost_set_constant_buffer;
+
+	gallium->set_stencil_ref = panfrost_set_stencil_ref;
+
+	gallium->create_sampler_view = panfrost_create_sampler_view;
+	gallium->set_sampler_views = panfrost_set_sampler_views;
+	gallium->sampler_view_destroy = panfrost_sampler_view_destroy;
+
+	gallium->create_rasterizer_state = panfrost_create_rasterizer_state;
+	gallium->bind_rasterizer_state = panfrost_bind_rasterizer_state;
+	gallium->delete_rasterizer_state = panfrost_generic_cso_delete;
+
+	gallium->create_vertex_elements_state = panfrost_create_vertex_elements_state;
+	gallium->bind_vertex_elements_state = panfrost_bind_vertex_elements_state;
+	gallium->delete_vertex_elements_state = panfrost_generic_cso_delete;
+
+	gallium->create_fs_state = panfrost_create_shader_state;
+	gallium->delete_fs_state = panfrost_delete_shader_state;
+	gallium->bind_fs_state = panfrost_bind_fs_state;
+
+	gallium->create_vs_state = panfrost_create_shader_state;
+	gallium->delete_vs_state = panfrost_delete_shader_state;
+	gallium->bind_vs_state = panfrost_bind_vs_state;
+
+	gallium->create_sampler_state = panfrost_create_sampler_state;
+	gallium->delete_sampler_state = panfrost_generic_cso_delete;
+	gallium->bind_sampler_states = panfrost_bind_sampler_states;
+
+	gallium->create_blend_state = panfrost_create_blend_state;
+	gallium->bind_blend_state   = panfrost_bind_blend_state;
+	gallium->delete_blend_state = panfrost_delete_blend_state;
+
+	gallium->set_blend_color = panfrost_set_blend_color;
+
+	gallium->create_depth_stencil_alpha_state = panfrost_create_depth_stencil_state;
+	gallium->bind_depth_stencil_alpha_state   = panfrost_bind_depth_stencil_state;
+	gallium->delete_depth_stencil_alpha_state = panfrost_delete_depth_stencil_state;
+
+	gallium->set_sample_mask = panfrost_set_sample_mask;
+
+	gallium->set_clip_state = panfrost_set_clip_state;
+	gallium->set_viewport_states = panfrost_set_viewport_states;
+	gallium->set_scissor_states = panfrost_set_scissor_states;
+	gallium->set_polygon_stipple = panfrost_set_polygon_stipple;
+	gallium->set_active_query_state = panfrost_set_active_query_state;
+
+	gallium->blit = panfrost_blit;
+
+	/* XXX: leaks */
+	gallium->stream_uploader = u_upload_create_default(gallium);
+	gallium->const_uploader = gallium->stream_uploader;
+	assert(gallium->stream_uploader);
+
+	ctx->primconvert = util_primconvert_create(gallium,
+			(1 << PIPE_PRIM_QUADS) - 1);
+	assert(ctx->primconvert);
+
+	ctx->blitter = util_blitter_create(gallium);
+	assert(ctx->blitter);
+
+	/* Prepare for render! */
+	panfrost_setup_hardware(ctx);
+	
+	/* TODO: XXX */
+	ctx->vt_framebuffer = panfrost_emit_fbd(ctx);
+	
+	panfrost_emit_vertex_payload(ctx);
+	panfrost_emit_tiler_payload(ctx);
+	panfrost_invalidate_frame(ctx);
+	panfrost_viewport(ctx, 0.0, 1.0, 0, 0, ctx->pipe_framebuffer.width, ctx->pipe_framebuffer.height);
+	panfrost_new_frag_framebuffer(ctx);
+	panfrost_default_shader_backend(ctx);
+	panfrost_generate_space_filler_indices();
+
+	
+	return gallium;
+}
diff --git a/src/gallium/drivers/panfrost/pan_context.h b/src/gallium/drivers/panfrost/pan_context.h
new file mode 100644
index 0000000..c4fce9b
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_context.h
@@ -0,0 +1,322 @@
+/*
+ * © Copyright 2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __BUILDER_H__
+#define __BUILDER_H__
+
+#define BIT64
+#define T8XX
+#define MFBD
+
+#define _LARGEFILE64_SOURCE 1
+#define CACHE_LINE_SIZE 1024 /* TODO */ 
+#include <sys/mman.h>
+#include <assert.h>
+#include "pan_nondrm.h"
+
+#include "pipe/p_compiler.h"
+#include "pipe/p_config.h"
+#include "pipe/p_context.h"
+#include "pipe/p_defines.h"
+#include "pipe/p_format.h"
+#include "pipe/p_screen.h"
+#include "pipe/p_state.h"
+#include "util/u_blitter.h"
+
+/* Forward declare to avoid extra header dep */
+struct prim_convert_context;
+
+/* TODO: Handle on newer hardware */
+#ifdef MFBD
+#define PANFROST_DEFAULT_FBD (MALI_MFBD)
+#define PANFROST_FRAMEBUFFER struct bifrost_framebuffer
+#else
+#define PANFROST_DEFAULT_FBD (MALI_SFBD)
+#define PANFROST_FRAMEBUFFER struct mali_single_framebuffer
+#endif
+
+#define MAX_DRAW_CALLS 4096
+#define MAX_VARYINGS   4096
+
+#define PAN_DIRTY_DUMMY	     (1 << 0) 
+#define PAN_DIRTY_RASTERIZER (1 << 2)
+#define PAN_DIRTY_FS	     (1 << 3)
+#define PAN_DIRTY_FRAG_CORE  (PAN_DIRTY_FS) /* Dirty writes are tied */
+#define PAN_DIRTY_VS	     (1 << 4)
+#define PAN_DIRTY_VERTEX     (1 << 5)
+#define PAN_DIRTY_VERT_BUF   (1 << 6)
+#define PAN_DIRTY_VIEWPORT   (1 << 7)
+#define PAN_DIRTY_SAMPLERS   (1 << 8)
+#define PAN_DIRTY_TEXTURES   (1 << 9)
+
+struct panfrost_constant_buffer {
+	bool dirty;
+	size_t size;
+	void *buffer;
+};
+
+struct panfrost_context {
+	/* Gallium context */
+	struct pipe_context base;
+
+	/* TODO: DRM driver? */
+	int fd;
+	struct pipe_framebuffer_state pipe_framebuffer;
+
+	/* The number of concurrent FBOs allowed depends on the number of rings used */
+	struct panfrost_memory cmdstream_rings[2];
+	int cmdstream_i;
+
+	struct panfrost_memory cmdstream;
+
+	struct panfrost_memory cmdstream_persistent;
+	struct panfrost_memory textures;
+	struct panfrost_memory shaders;
+	struct panfrost_memory scratchpad;
+	struct panfrost_memory tiler_heap;
+	struct panfrost_memory varying_mem;
+	struct panfrost_memory framebuffer;
+	struct panfrost_memory misc_0;
+	struct panfrost_memory misc_1;
+	struct panfrost_memory depth_stencil_buffer;
+
+	struct {
+		unsigned buffers;
+		const union pipe_color_union *color;
+		double depth;
+		unsigned stencil;
+	} last_clear;
+
+	int scanout_stride;
+
+	/* Each render job has multiple framebuffer descriptors associated with
+	 * it, used for various purposes with more or less the same format. The
+	 * most obvious is the fragment framebuffer descriptor, which carries
+	 * e.g. clearing information */
+	
+#ifdef SFBD
+	struct mali_single_framebuffer fragment_fbd;
+#else
+	struct bifrost_framebuffer fragment_fbd;
+
+	struct bifrost_fb_extra fragment_extra;
+
+	struct bifrost_render_target fragment_rts[4];
+#endif
+
+	/* Each draw has corresponding vertex and tiler payloads */
+	struct midgard_payload_vertex_tiler payload_vertex;
+	struct midgard_payload_vertex_tiler payload_tiler;
+
+	/* The fragment shader binary itself is pointed here (for the tripipe) but
+	 * also everything else in the shader core, including blending, the
+	 * stencil/depth tests, etc. Refer to the presentations. */
+
+	struct mali_shader_meta fragment_shader_core;
+
+	/* A frame is composed of a starting set value job, a number of vertex
+	 * and tiler jobs, linked to the fragment job at the end. See the
+	 * presentations for more information how this works */
+
+	unsigned draw_count;
+
+	mali_ptr set_value_job;
+	mali_ptr vertex_jobs[MAX_DRAW_CALLS];
+	mali_ptr tiler_jobs[MAX_DRAW_CALLS];
+
+	/* Dirty flags are setup like any other driver */
+	int dirty;
+
+	unsigned vertex_count;
+
+	struct mali_attr attributes[PIPE_MAX_ATTRIBS];
+
+	unsigned varying_height;
+
+	struct mali_viewport viewport;
+	PANFROST_FRAMEBUFFER vt_framebuffer;
+
+	/* TODO: Multiple uniform buffers (index =/= 0), finer updates? */
+
+	struct panfrost_constant_buffer constant_buffer[PIPE_SHADER_TYPES];
+
+	/* CSOs */
+	struct panfrost_rasterizer *rasterizer;
+
+	struct panfrost_shader_state *vs;
+	struct panfrost_shader_state *fs;
+
+	struct panfrost_vertex_state *vertex;
+
+	struct pipe_vertex_buffer *vertex_buffers;
+	unsigned vertex_buffer_count;
+
+	struct panfrost_sampler_state *samplers[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
+	unsigned sampler_count[PIPE_SHADER_TYPES];
+
+	struct panfrost_sampler_view *sampler_views[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_SAMPLER_VIEWS];
+	unsigned sampler_view_count[PIPE_SHADER_TYPES];
+
+	struct primconvert_context *primconvert;
+	struct blitter_context *blitter;
+
+	struct panfrost_blend_state *blend;
+
+	struct pipe_viewport_state pipe_viewport;
+	struct pipe_scissor_state scissor;
+	struct pipe_blend_color blend_color;
+	struct pipe_depth_stencil_alpha_state *depth_stencil;
+	struct pipe_stencil_ref stencil_ref;
+};
+
+/* Corresponds to the CSO */
+
+struct panfrost_rasterizer {
+	struct pipe_rasterizer_state base;
+
+	/* Bitmask of front face, etc */
+	unsigned tiler_gl_enables;
+};
+
+struct panfrost_blend_state {
+	struct pipe_blend_state base;
+
+	/* Whether a blend shader is in use */
+	bool has_blend_shader;
+
+	/* Compiled fixed function command */
+	struct mali_blend_equation equation;
+
+	/* Compiled blend shader */
+	mali_ptr blend_shader;
+	int blend_work_count;
+};
+
+/* Internal varyings descriptor */
+struct panfrost_varyings {
+	/* Varyings information: stride of each chunk of memory used for
+	 * varyings (similar structure with attributes). Count is just the
+	 * number of vec4's. Buffer count is the number of varying chunks (<=
+	 * count). Height is used to calculate gl_Position's position ("it's
+	 * not a pun, Alyssa!"). Vertex-only varyings == descriptor for
+	 * gl_Position and something else apparently occupying the same space.
+	 * Varyings == main varyings descriptors following typical mali_attr
+	 * conventions. */
+
+	unsigned varyings_stride[MAX_VARYINGS];
+	unsigned varying_count;
+	unsigned varying_buffer_count;
+
+	struct mali_attr_meta vertex_only_varyings[2];
+	struct mali_attr_meta varyings[PIPE_MAX_ATTRIBS];
+	struct mali_attr_meta fragment_only_varyings[1];
+	int fragment_only_varying_count;
+};
+
+struct panfrost_shader_state {
+	struct pipe_shader_state base;
+
+	/* Compiled descriptor, ready for the hardware */
+	bool compiled;
+	struct mali_shader_meta tripipe;
+
+	/* Non-descript information */
+	int uniform_count;
+	bool can_discard;
+
+	/* Valid for vertex shaders only due to when this is calculated */
+	struct panfrost_varyings varyings;
+};
+
+struct panfrost_vertex_state {
+	unsigned num_elements;
+
+	struct pipe_vertex_element pipe[PIPE_MAX_ATTRIBS];
+	struct mali_attr_meta hw[PIPE_MAX_ATTRIBS];
+	int nr_components[PIPE_MAX_ATTRIBS];
+};
+
+struct panfrost_sampler_state {
+	struct pipe_sampler_state base;
+	struct mali_sampler_descriptor hw;
+};
+
+/* Misnomer: Sampler view corresponds to textures, not samplers */
+
+struct panfrost_sampler_view {
+	struct pipe_sampler_view base;
+	struct mali_texture_descriptor hw;
+};
+
+/* Corresponds to pipe_resource for our hacky pre-DRM interface */
+
+struct sw_displaytarget;
+
+struct panfrost_resource {
+	struct pipe_resource base;
+
+	/* Address to the resource in question */
+
+	uint8_t *cpu[MAX_MIP_LEVELS];
+
+	/* Not necessarily a GPU mapping of cpu! In case of texture tiling, gpu
+	 * points to the GPU-side, tiled texture, while cpu points to the
+	 * CPU-side, untiled texture from mesa */
+
+	mali_ptr gpu[MAX_MIP_LEVELS];
+
+	/* Is something other than level 0 ever written? */
+	bool is_mipmap;
+
+	/* Valid for textures; 1 otherwise */
+	int bytes_per_pixel;
+
+	/* bytes_per_pixel*width + padding */
+	int stride;
+
+	struct sw_displaytarget *dt;
+
+	/* Set for tiled, clear for linear. For linear, set if directly mapped and clear for memcpy */
+	bool tiled;
+	bool mapped_direct;
+
+	/* If AFBC is enabled for this resource, we lug around an AFBC
+	 * metadata buffer as well. The actual AFBC resource is also in
+	 * afbc_slab (only defined for AFBC) at position afbc_main_offset */
+
+	bool has_afbc;
+	struct panfrost_memory afbc_slab;
+	int afbc_metadata_size;
+
+	/* Similarly for TE */
+	bool has_checksum;
+	struct panfrost_memory checksum_slab;
+	int checksum_stride;
+};
+
+static inline struct panfrost_context *
+panfrost_context(struct pipe_context *pcontext)
+{
+	return (struct panfrost_context *) pcontext;
+}
+
+struct pipe_context *
+panfrost_create_context(struct pipe_screen *screen, void *priv, unsigned flags);
+
+struct pipe_resource *
+panfrost_resource_create_front(struct pipe_screen *screen,
+			       const struct pipe_resource *template,
+			       const void *map_front_private);
+
+#endif
diff --git a/src/gallium/drivers/panfrost/pan_nondrm.c b/src/gallium/drivers/panfrost/pan_nondrm.c
new file mode 100644
index 0000000..6642ea6
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_nondrm.c
@@ -0,0 +1,94 @@
+/*
+ * © Copyright 2017 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+#include <stdio.h>
+#include <stdlib.h>
+#include <fcntl.h>
+#include <sys/ioctl.h>
+#include <string.h>
+#include <sys/mman.h>
+#include <assert.h>
+
+#include <mali-ioctl.h>
+#include "pan_nondrm.h"
+
+/* From the kernel module */
+
+#define USE_LEGACY_KERNEL
+#define MALI_MEM_MAP_TRACKING_HANDLE (3ull << 12)
+
+int
+pandev_ioctl(int fd, unsigned long request, void *args)
+{
+	union mali_ioctl_header *h = args;
+	h->id = ((_IOC_TYPE(request) & 0xF) << 8) | _IOC_NR(request);
+	return ioctl(fd, request, args);
+}
+
+int
+pandev_general_allocate(int fd, int va_pages, int commit_pages, int extent, int flags, u64 *out)
+{
+	struct mali_ioctl_mem_alloc args = {
+		.va_pages = va_pages,
+		.commit_pages = commit_pages,
+		.extent = extent,
+		.flags = flags
+	};
+
+	if (pandev_ioctl(fd, MALI_IOCTL_MEM_ALLOC, &args) != 0) {
+		perror("pandev_ioctl MALI_IOCTL_MEM_ALLOC");
+		abort();
+	}
+
+	*out = args.gpu_va;
+
+	return 0;
+}
+
+int
+pandev_standard_allocate(int fd, int va_pages, int flags, u64 *out)
+{
+	return pandev_general_allocate(fd, va_pages, va_pages, 0, flags, out);
+}
+
+int
+pandev_open()
+{
+
+	int fd = open("/dev/mali0", O_RDWR | O_CLOEXEC);
+	assert(fd != -1);
+
+#ifdef USE_LEGACY_KERNEL
+	struct mali_ioctl_get_version version = { .major = 10, .minor = 4 };
+	struct mali_ioctl_set_flags args = {};
+
+	if (pandev_ioctl(fd, MALI_IOCTL_GET_VERSION, &version) != 0) {
+		perror("pandev_ioctl: MALI_IOCTL_GET_VERSION");
+		abort();
+	}
+
+	printf("(%d, %d)\n", version.major, version.minor);
+
+	if (mmap(NULL, 4096, PROT_NONE, MAP_SHARED, fd, MALI_MEM_MAP_TRACKING_HANDLE) == MAP_FAILED) {
+		perror("mmap");
+		abort();
+	}
+
+	if (pandev_ioctl(fd, MALI_IOCTL_SET_FLAGS, &args) != 0) {
+		perror("pandev_ioctl: MALI_IOCTL_SET_FLAGS");
+		abort();
+	}
+#endif
+
+	return fd;
+}
diff --git a/src/gallium/drivers/panfrost/pan_nondrm.h b/src/gallium/drivers/panfrost/pan_nondrm.h
new file mode 100644
index 0000000..675208d
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_nondrm.h
@@ -0,0 +1,72 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __PANDEV_H__
+#define __PANDEV_H__
+
+#include <unistd.h>
+#include <sys/mman.h>
+#include <stdbool.h>
+#include <mali-ioctl.h>
+#include <mali-job.h>
+#include <linux/ioctl.h>
+#include "pan_slowfb.h"
+
+int pandev_open(void);
+
+/* Calls used while replaying */
+int pandev_raw_open(void);
+u8* pandev_map_mtp(int fd);
+int pandev_ioctl(int fd, unsigned long request, void *args);
+
+int pandev_standard_allocate(int fd, int va_pages, int flags, u64 *out);
+int pandev_general_allocate(int fd, int va_pages, int commit_pages, int extent, int flags, u64 *out);
+
+struct panfrost_context;
+struct panfrost_shader_state;
+void
+panfrost_shader_compile(struct panfrost_context *ctx, struct mali_shader_meta *meta, const char *src, int type, struct panfrost_shader_state *state);
+
+struct panfrost_memory {
+	uint8_t* cpu;
+	mali_ptr gpu;
+	int stack_bottom;
+	size_t size;
+};
+
+/* Functions for replay */
+mali_ptr pandev_upload(int cheating_offset, int *stack_bottom, mali_ptr base, void *base_map, const void *data, size_t sz, bool no_pad);
+mali_ptr pandev_upload_sequential(mali_ptr base, void *base_map, const void *data, size_t sz);
+
+/* Functions for the actual Galliumish driver */
+mali_ptr panfrost_upload(struct panfrost_memory *mem, const void *data, size_t sz, bool no_pad);
+mali_ptr panfrost_upload_sequential(struct panfrost_memory *mem, const void *data, size_t sz);
+
+void *
+panfrost_allocate_transfer(struct panfrost_memory *mem, size_t sz, mali_ptr *gpu);
+
+static inline mali_ptr
+panfrost_reserve(struct panfrost_memory *mem, size_t sz)
+{
+	mem->stack_bottom += sz;
+	return mem->gpu + (mem->stack_bottom - sz);
+}
+
+#include <math.h>
+#define inff INFINITY
+
+#define R(...) #__VA_ARGS__
+#define ALIGN(x, y) (((x) + ((y) - 1)) & ~((y) - 1))
+
+#endif /* __PANDEV_H__ */
diff --git a/src/gallium/drivers/panfrost/pan_public.h b/src/gallium/drivers/panfrost/pan_public.h
new file mode 100644
index 0000000..5cd7d05
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_public.h
@@ -0,0 +1,18 @@
+#ifndef SP_PUBLIC_H
+#define SP_PUBLIC_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct pipe_screen;
+struct renderonly;
+
+struct pipe_screen *
+panfrost_create_screen(int fd, struct renderonly *ro);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/src/gallium/drivers/panfrost/pan_screen.c b/src/gallium/drivers/panfrost/pan_screen.c
new file mode 100644
index 0000000..a102179
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_screen.c
@@ -0,0 +1,580 @@
+/**************************************************************************
+ *
+ * Copyright 2008 VMware, Inc.
+ * Copyright 2014 Broadcom
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+
+
+#include "util/u_memory.h"
+#include "util/u_format.h"
+#include "util/u_format_s3tc.h"
+#include "util/u_video.h"
+#include "util/os_time.h"
+#include "pipe/p_defines.h"
+#include "pipe/p_screen.h"
+#include "draw/draw_context.h"
+
+#include "pan_texture.h"
+#include "pan_screen.h"
+#include "pan_public.h"
+
+#include "pan_context.h"
+#include "midgard/midgard_compile.h"
+
+static const char *
+panfrost_get_name(struct pipe_screen *screen)
+{
+   return "panfrost";
+}
+
+static const char *
+panfrost_get_vendor(struct pipe_screen *screen)
+{
+   return "panfrost";
+}
+
+static const char *
+panfrost_get_device_vendor(struct pipe_screen *screen)
+{
+   return "ARM";
+}
+
+static int
+panfrost_get_param(struct pipe_screen *screen, enum pipe_cap param)
+{
+   switch (param) {
+   case PIPE_CAP_NPOT_TEXTURES:
+   case PIPE_CAP_MIXED_FRAMEBUFFER_SIZES:
+   case PIPE_CAP_MIXED_COLOR_DEPTH_BITS:
+      return 1;
+   case PIPE_CAP_SM3:
+      return 1;
+   case PIPE_CAP_POINT_SPRITE:
+      return 1;
+   case PIPE_CAP_MAX_RENDER_TARGETS:
+      return PIPE_MAX_COLOR_BUFS;
+   case PIPE_CAP_MAX_DUAL_SOURCE_RENDER_TARGETS:
+      return 1;
+   case PIPE_CAP_OCCLUSION_QUERY:
+   case PIPE_CAP_QUERY_TIME_ELAPSED:
+   case PIPE_CAP_QUERY_PIPELINE_STATISTICS:
+      return 1; /* TODO: Queries */
+   case PIPE_CAP_TEXTURE_MIRROR_CLAMP:
+      return 1;
+   case PIPE_CAP_TEXTURE_SWIZZLE:
+      return 1;
+   case PIPE_CAP_TEXTURE_BORDER_COLOR_QUIRK:
+      return 0;
+   case PIPE_CAP_MAX_TEXTURE_2D_LEVELS:
+   case PIPE_CAP_MAX_TEXTURE_3D_LEVELS:
+   case PIPE_CAP_MAX_TEXTURE_CUBE_LEVELS:
+      return 13;
+   case PIPE_CAP_BLEND_EQUATION_SEPARATE:
+      return 1;
+   case PIPE_CAP_INDEP_BLEND_ENABLE:
+      return 1;
+   case PIPE_CAP_INDEP_BLEND_FUNC:
+      return 1;
+   case PIPE_CAP_TGSI_FS_COORD_ORIGIN_UPPER_LEFT:
+   case PIPE_CAP_TGSI_FS_COORD_ORIGIN_LOWER_LEFT:
+   case PIPE_CAP_TGSI_FS_COORD_PIXEL_CENTER_HALF_INTEGER:
+   case PIPE_CAP_TGSI_FS_COORD_PIXEL_CENTER_INTEGER:
+      return 1;
+   case PIPE_CAP_DEPTH_CLIP_DISABLE:
+      return 1;
+   case PIPE_CAP_MAX_STREAM_OUTPUT_BUFFERS:
+      return 0; /* no streamout */
+   case PIPE_CAP_MAX_STREAM_OUTPUT_SEPARATE_COMPONENTS:
+   case PIPE_CAP_MAX_STREAM_OUTPUT_INTERLEAVED_COMPONENTS:
+      return 16*4;
+   case PIPE_CAP_MAX_GEOMETRY_OUTPUT_VERTICES:
+   case PIPE_CAP_MAX_GEOMETRY_TOTAL_OUTPUT_COMPONENTS:
+      return 1024;
+   case PIPE_CAP_MAX_VERTEX_STREAMS:
+      return 1;
+   case PIPE_CAP_MAX_VERTEX_ATTRIB_STRIDE:
+      return 2048;
+   case PIPE_CAP_PRIMITIVE_RESTART:
+      return 0; /* We don't understand this yet */
+   case PIPE_CAP_SHADER_STENCIL_EXPORT:
+      return 1;
+
+   case PIPE_CAP_TGSI_INSTANCEID:
+   case PIPE_CAP_VERTEX_ELEMENT_INSTANCE_DIVISOR:
+   case PIPE_CAP_START_INSTANCE:
+      return 0; /* TODO: Instances */
+   case PIPE_CAP_SEAMLESS_CUBE_MAP:
+   case PIPE_CAP_SEAMLESS_CUBE_MAP_PER_TEXTURE:
+      return 1;
+   case PIPE_CAP_MAX_TEXTURE_ARRAY_LAYERS:
+      return 256; /* for GL3 */
+   case PIPE_CAP_MIN_TEXEL_OFFSET:
+      return -8;
+   case PIPE_CAP_MAX_TEXEL_OFFSET:
+      return 7;
+   case PIPE_CAP_CONDITIONAL_RENDER:
+      return 1;
+   case PIPE_CAP_TEXTURE_BARRIER:
+      return 0;
+   case PIPE_CAP_FRAGMENT_COLOR_CLAMPED:
+   case PIPE_CAP_VERTEX_COLOR_UNCLAMPED: /* draw module */
+   case PIPE_CAP_VERTEX_COLOR_CLAMPED: /* draw module */
+      return 1;
+   case PIPE_CAP_MIXED_COLORBUFFER_FORMATS:
+      return 0;
+   case PIPE_CAP_GLSL_FEATURE_LEVEL:
+      return 330;
+   case PIPE_CAP_QUADS_FOLLOW_PROVOKING_VERTEX_CONVENTION:
+   case PIPE_CAP_TGSI_TEX_TXF_LZ:
+      return 0;
+   case PIPE_CAP_COMPUTE:
+      return 0;
+   case PIPE_CAP_USER_VERTEX_BUFFERS: /* XXX XXX */
+   case PIPE_CAP_RESOURCE_FROM_USER_MEMORY:
+      return 0;
+   case PIPE_CAP_STREAM_OUTPUT_PAUSE_RESUME:
+   case PIPE_CAP_STREAM_OUTPUT_INTERLEAVE_BUFFERS:
+   case PIPE_CAP_TGSI_VS_LAYER_VIEWPORT:
+   case PIPE_CAP_DOUBLES:
+   case PIPE_CAP_INT64:
+   case PIPE_CAP_INT64_DIVMOD:
+      return 1;
+   case PIPE_CAP_CONSTANT_BUFFER_OFFSET_ALIGNMENT:
+      return 16;
+   case PIPE_CAP_TGSI_CAN_COMPACT_CONSTANTS:
+   case PIPE_CAP_VERTEX_BUFFER_OFFSET_4BYTE_ALIGNED_ONLY:
+   case PIPE_CAP_VERTEX_BUFFER_STRIDE_4BYTE_ALIGNED_ONLY:
+   case PIPE_CAP_VERTEX_ELEMENT_SRC_OFFSET_4BYTE_ALIGNED_ONLY:
+   case PIPE_CAP_TEXTURE_MULTISAMPLE:
+      return 0;
+   case PIPE_CAP_MIN_MAP_BUFFER_ALIGNMENT:
+      return 64;
+   case PIPE_CAP_QUERY_TIMESTAMP:
+   case PIPE_CAP_CUBE_MAP_ARRAY:
+      return 1;
+   case PIPE_CAP_TEXTURE_BUFFER_OBJECTS:
+      return 1;
+   case PIPE_CAP_BUFFER_SAMPLER_VIEW_RGBA_ONLY:
+      return 0;
+   case PIPE_CAP_MAX_TEXTURE_BUFFER_SIZE:
+      return 65536;
+   case PIPE_CAP_TEXTURE_BUFFER_OFFSET_ALIGNMENT:
+      return 0;
+   case PIPE_CAP_TGSI_TEXCOORD:
+      return 1; /* XXX: What should this me exactly? */
+   case PIPE_CAP_PREFER_BLIT_BASED_TEXTURE_TRANSFER:
+      return 0;
+   case PIPE_CAP_MAX_VIEWPORTS:
+      return PIPE_MAX_VIEWPORTS;
+   case PIPE_CAP_ENDIANNESS:
+      return PIPE_ENDIAN_NATIVE;
+   case PIPE_CAP_MAX_TEXTURE_GATHER_COMPONENTS:
+      return 4;
+   case PIPE_CAP_TEXTURE_GATHER_SM5:
+   case PIPE_CAP_TEXTURE_QUERY_LOD:
+      return 1;
+   case PIPE_CAP_BUFFER_MAP_PERSISTENT_COHERENT:
+   case PIPE_CAP_SAMPLE_SHADING:
+   case PIPE_CAP_TEXTURE_GATHER_OFFSETS:
+      return 0;
+   case PIPE_CAP_TGSI_VS_WINDOW_SPACE_POSITION:
+      return 1;
+   case PIPE_CAP_TGSI_FS_FINE_DERIVATIVE:
+      return 0;
+   case PIPE_CAP_SAMPLER_VIEW_TARGET:
+      return 1;
+   case PIPE_CAP_FAKE_SW_MSAA:
+      return 1;
+   case PIPE_CAP_MIN_TEXTURE_GATHER_OFFSET:
+      return -32;
+   case PIPE_CAP_MAX_TEXTURE_GATHER_OFFSET:
+      return 31;
+   case PIPE_CAP_DRAW_INDIRECT:
+      return 1;
+   case PIPE_CAP_QUERY_SO_OVERFLOW:
+      return 1;
+
+   case PIPE_CAP_VENDOR_ID:
+      return 0xFFFFFFFF;
+   case PIPE_CAP_DEVICE_ID:
+      return 0xFFFFFFFF;
+   case PIPE_CAP_ACCELERATED:
+      return 1;
+   case PIPE_CAP_VIDEO_MEMORY: {
+      /* XXX: Do we want to return the full amount fo system memory ? */
+      uint64_t system_memory;
+
+      if (!os_get_total_physical_memory(&system_memory))
+         return 0;
+
+      if (sizeof(void *) == 4)
+         /* Cap to 2 GB on 32 bits system. We do this because panfrost does
+          * eat application memory, which is quite limited on 32 bits. App
+          * shouldn't expect too much available memory. */
+         system_memory = MIN2(system_memory, 2048 << 20);
+
+      return (int)(system_memory >> 20);
+   }
+   case PIPE_CAP_UMA:
+      return 0;
+   case PIPE_CAP_CONDITIONAL_RENDER_INVERTED:
+      return 1;
+   case PIPE_CAP_CLIP_HALFZ:
+   case PIPE_CAP_TEXTURE_FLOAT_LINEAR:
+   case PIPE_CAP_TEXTURE_HALF_FLOAT_LINEAR:
+      return 1;
+   case PIPE_CAP_FRAMEBUFFER_NO_ATTACHMENT:
+   case PIPE_CAP_CULL_DISTANCE:
+      return 1;
+   case PIPE_CAP_VERTEXID_NOBASE:
+      return 0;
+   case PIPE_CAP_POLYGON_OFFSET_CLAMP:
+      return 0;
+   case PIPE_CAP_COPY_BETWEEN_COMPRESSED_AND_PLAIN_FORMATS:
+   case PIPE_CAP_TGSI_ARRAY_COMPONENTS:
+      return 1;
+   case PIPE_CAP_CLEAR_TEXTURE:
+      return 1;
+   case PIPE_CAP_ANISOTROPIC_FILTER:
+   case PIPE_CAP_MULTISAMPLE_Z_RESOLVE:
+   case PIPE_CAP_DEVICE_RESET_STATUS_QUERY:
+   case PIPE_CAP_MAX_SHADER_PATCH_VARYINGS:
+   case PIPE_CAP_DEPTH_BOUNDS_TEST:
+   case PIPE_CAP_TGSI_TXQS:
+   case PIPE_CAP_FORCE_PERSAMPLE_INTERP:
+   case PIPE_CAP_SHAREABLE_SHADERS:
+   case PIPE_CAP_DRAW_PARAMETERS:
+   case PIPE_CAP_TGSI_PACK_HALF_FLOAT:
+   case PIPE_CAP_MULTI_DRAW_INDIRECT:
+   case PIPE_CAP_MULTI_DRAW_INDIRECT_PARAMS:
+   case PIPE_CAP_TGSI_FS_POSITION_IS_SYSVAL:
+   case PIPE_CAP_TGSI_FS_FACE_IS_INTEGER_SYSVAL:
+   case PIPE_CAP_INVALIDATE_BUFFER:
+   case PIPE_CAP_GENERATE_MIPMAP:
+   case PIPE_CAP_STRING_MARKER:
+   case PIPE_CAP_SURFACE_REINTERPRET_BLOCKS:
+   case PIPE_CAP_QUERY_BUFFER_OBJECT:
+   case PIPE_CAP_QUERY_MEMORY_INFO:
+   case PIPE_CAP_PCI_GROUP:
+   case PIPE_CAP_PCI_BUS:
+   case PIPE_CAP_PCI_DEVICE:
+   case PIPE_CAP_PCI_FUNCTION:
+   case PIPE_CAP_ROBUST_BUFFER_ACCESS_BEHAVIOR:
+   case PIPE_CAP_PRIMITIVE_RESTART_FOR_PATCHES:
+   case PIPE_CAP_TGSI_VOTE:
+   case PIPE_CAP_MAX_WINDOW_RECTANGLES:
+   case PIPE_CAP_POLYGON_OFFSET_UNITS_UNSCALED:
+   case PIPE_CAP_VIEWPORT_SUBPIXEL_BITS:
+   case PIPE_CAP_TGSI_CAN_READ_OUTPUTS:
+   case PIPE_CAP_NATIVE_FENCE_FD:
+   case PIPE_CAP_GLSL_OPTIMIZE_CONSERVATIVELY:
+   case PIPE_CAP_TGSI_FS_FBFETCH:
+   case PIPE_CAP_TGSI_MUL_ZERO_WINS:
+   case PIPE_CAP_TGSI_CLOCK:
+   case PIPE_CAP_POLYGON_MODE_FILL_RECTANGLE:
+   case PIPE_CAP_SPARSE_BUFFER_PAGE_SIZE:
+   case PIPE_CAP_TGSI_BALLOT:
+   case PIPE_CAP_TGSI_TES_LAYER_VIEWPORT:
+   case PIPE_CAP_CAN_BIND_CONST_BUFFER_AS_VERTEX:
+   case PIPE_CAP_ALLOW_MAPPED_BUFFERS_DURING_EXECUTION:
+   case PIPE_CAP_POST_DEPTH_COVERAGE:
+   case PIPE_CAP_BINDLESS_TEXTURE:
+   case PIPE_CAP_NIR_SAMPLERS_AS_DEREF:
+   case PIPE_CAP_MEMOBJ:
+   case PIPE_CAP_LOAD_CONSTBUF:
+   case PIPE_CAP_TGSI_ANY_REG_AS_ADDRESS:
+   case PIPE_CAP_TILE_RASTER_ORDER:
+   case PIPE_CAP_MAX_COMBINED_SHADER_OUTPUT_RESOURCES:
+   case PIPE_CAP_SIGNED_VERTEX_BUFFER_OFFSET:
+   case PIPE_CAP_CONTEXT_PRIORITY_MASK:
+   case PIPE_CAP_FENCE_SIGNAL:
+   case PIPE_CAP_CONSTBUF0_FLAGS:
+      return 0;
+   case PIPE_CAP_SHADER_BUFFER_OFFSET_ALIGNMENT:
+      return 4;
+   default:
+	   debug_printf("Unexpected PIPE_CAP %d query\n", param);
+      return 0;
+   }
+}
+
+static int
+panfrost_get_shader_param(struct pipe_screen *screen,
+                          enum pipe_shader_type shader,
+                          enum pipe_shader_cap param)
+{
+        if (shader != PIPE_SHADER_VERTEX &&
+            shader != PIPE_SHADER_FRAGMENT) {
+                return 0;
+        }
+
+        /* this is probably not totally correct.. but it's a start: */
+        switch (param) {
+	case PIPE_SHADER_CAP_SCALAR_ISA:
+		return 0;
+
+        case PIPE_SHADER_CAP_MAX_INSTRUCTIONS:
+        case PIPE_SHADER_CAP_MAX_ALU_INSTRUCTIONS:
+        case PIPE_SHADER_CAP_MAX_TEX_INSTRUCTIONS:
+        case PIPE_SHADER_CAP_MAX_TEX_INDIRECTIONS:
+                return 16384;
+
+        case PIPE_SHADER_CAP_MAX_CONTROL_FLOW_DEPTH:
+                return 1024; 
+
+        case PIPE_SHADER_CAP_MAX_INPUTS:
+                return 16;
+        case PIPE_SHADER_CAP_MAX_OUTPUTS:
+                return shader == PIPE_SHADER_FRAGMENT ? 1 : 8;
+        case PIPE_SHADER_CAP_MAX_TEMPS:
+                return 256; /* GL_MAX_PROGRAM_TEMPORARIES_ARB */
+        case PIPE_SHADER_CAP_MAX_CONST_BUFFER_SIZE:
+                return 16 * 1024 * sizeof(float);
+        case PIPE_SHADER_CAP_MAX_CONST_BUFFERS:
+                return 1;
+        case PIPE_SHADER_CAP_TGSI_CONT_SUPPORTED:
+                return 0;
+        case PIPE_SHADER_CAP_INDIRECT_INPUT_ADDR:
+        case PIPE_SHADER_CAP_INDIRECT_OUTPUT_ADDR:
+        case PIPE_SHADER_CAP_INDIRECT_TEMP_ADDR:
+                return 0;
+        case PIPE_SHADER_CAP_INDIRECT_CONST_ADDR:
+                return 1;
+        case PIPE_SHADER_CAP_SUBROUTINES:
+                return 0;
+        case PIPE_SHADER_CAP_TGSI_SQRT_SUPPORTED:
+                return 0;
+        case PIPE_SHADER_CAP_INTEGERS:
+                return 1;
+        case PIPE_SHADER_CAP_INT64_ATOMICS:
+        case PIPE_SHADER_CAP_FP16:
+        case PIPE_SHADER_CAP_TGSI_DROUND_SUPPORTED:
+        case PIPE_SHADER_CAP_TGSI_DFRACEXP_DLDEXP_SUPPORTED:
+        case PIPE_SHADER_CAP_TGSI_LDEXP_SUPPORTED:
+        case PIPE_SHADER_CAP_TGSI_FMA_SUPPORTED:
+        case PIPE_SHADER_CAP_TGSI_ANY_INOUT_DECL_RANGE:
+                return 0;
+        case PIPE_SHADER_CAP_MAX_TEXTURE_SAMPLERS:
+        case PIPE_SHADER_CAP_MAX_SAMPLER_VIEWS:
+                return 16; /* XXX: How many? */
+        case PIPE_SHADER_CAP_PREFERRED_IR:
+                return PIPE_SHADER_IR_NIR;
+        case PIPE_SHADER_CAP_SUPPORTED_IRS:
+                return 0;
+        case PIPE_SHADER_CAP_MAX_UNROLL_ITERATIONS_HINT:
+                return 32;
+        case PIPE_SHADER_CAP_MAX_SHADER_BUFFERS:
+        case PIPE_SHADER_CAP_MAX_SHADER_IMAGES:
+        case PIPE_SHADER_CAP_LOWER_IF_THRESHOLD:
+        case PIPE_SHADER_CAP_TGSI_SKIP_MERGE_REGISTERS:
+        case PIPE_SHADER_CAP_MAX_HW_ATOMIC_COUNTERS:
+        case PIPE_SHADER_CAP_MAX_HW_ATOMIC_COUNTER_BUFFERS:
+                return 0;
+        default:
+                fprintf(stderr, "unknown shader param %d\n", param);
+                return 0;
+        }
+        return 0;
+}
+
+static float
+panfrost_get_paramf(struct pipe_screen *screen, enum pipe_capf param)
+{
+   switch (param) {
+   case PIPE_CAPF_MAX_LINE_WIDTH:
+      /* fall-through */
+   case PIPE_CAPF_MAX_LINE_WIDTH_AA:
+      return 255.0; /* arbitrary */
+   case PIPE_CAPF_MAX_POINT_WIDTH:
+      /* fall-through */
+   case PIPE_CAPF_MAX_POINT_WIDTH_AA:
+      return 255.0; /* arbitrary */
+   case PIPE_CAPF_MAX_TEXTURE_ANISOTROPY:
+      return 16.0;
+   case PIPE_CAPF_MAX_TEXTURE_LOD_BIAS:
+      return 16.0; /* arbitrary */
+   default:
+	   debug_printf("Unexpected PIPE_CAPF %d query\n", param);
+	   return 0.0;
+   }
+}
+
+/**
+ * Query format support for creating a texture, drawing surface, etc.
+ * \param format  the format to test
+ * \param type  one of PIPE_TEXTURE, PIPE_SURFACE
+ */
+static boolean
+panfrost_is_format_supported( struct pipe_screen *screen,
+                              enum pipe_format format,
+                              enum pipe_texture_target target,
+                              unsigned sample_count,
+			      unsigned storage_sample_count,
+                              unsigned bind)
+{
+   const struct util_format_description *format_desc;
+
+   assert(target == PIPE_BUFFER ||
+          target == PIPE_TEXTURE_1D ||
+          target == PIPE_TEXTURE_1D_ARRAY ||
+          target == PIPE_TEXTURE_2D ||
+          target == PIPE_TEXTURE_2D_ARRAY ||
+          target == PIPE_TEXTURE_RECT ||
+          target == PIPE_TEXTURE_3D ||
+          target == PIPE_TEXTURE_CUBE ||
+          target == PIPE_TEXTURE_CUBE_ARRAY);
+
+   format_desc = util_format_description(format);
+   if (!format_desc)
+      return FALSE;
+
+   if (sample_count > 1)
+      return FALSE;
+
+   if (bind & PIPE_BIND_RENDER_TARGET) {
+      if (format_desc->colorspace == UTIL_FORMAT_COLORSPACE_ZS)
+         return FALSE;
+
+      /*
+       * Although possible, it is unnatural to render into compressed or YUV
+       * surfaces. So disable these here to avoid going into weird paths
+       * inside the state trackers.
+       */
+      if (format_desc->block.width != 1 ||
+          format_desc->block.height != 1)
+         return FALSE;
+   }
+
+   if (bind & PIPE_BIND_DEPTH_STENCIL) {
+      if (format_desc->colorspace != UTIL_FORMAT_COLORSPACE_ZS)
+         return FALSE;
+   }
+
+   if (format_desc->layout == UTIL_FORMAT_LAYOUT_BPTC ||
+       format_desc->layout == UTIL_FORMAT_LAYOUT_ASTC) {
+      /* Software decoding is not hooked up. */
+      return FALSE;
+   }
+
+   if ((bind & (PIPE_BIND_RENDER_TARGET | PIPE_BIND_SAMPLER_VIEW)) &&
+       ((bind & PIPE_BIND_DISPLAY_TARGET) == 0) &&
+       target != PIPE_BUFFER) {
+      const struct util_format_description *desc =
+         util_format_description(format);
+      if (desc->nr_channels == 3 && desc->is_array) {
+         /* Don't support any 3-component formats for rendering/texturing
+          * since we don't support the corresponding 8-bit 3 channel UNORM
+          * formats.  This allows us to support GL_ARB_copy_image between
+          * GL_RGB8 and GL_RGB8UI, for example.  Otherwise, we may be asked to
+          * do a resource copy between PIPE_FORMAT_R8G8B8_UINT and
+          * PIPE_FORMAT_R8G8B8X8_UNORM, for example, which will not work
+          * (different bpp).
+          */
+         return FALSE;
+      }
+   }
+
+   return TRUE;
+}
+
+
+static void
+panfrost_destroy_screen( struct pipe_screen *screen )
+{
+   FREE(screen);
+}
+
+#include "pan_context.h"
+
+static void
+panfrost_flush_frontbuffer(struct pipe_screen *_screen,
+                           struct pipe_resource *resource,
+                           unsigned level, unsigned layer,
+                           void *context_private,
+                           struct pipe_box *sub_box)
+{
+	/* TODO: Display target integration */
+}
+
+static uint64_t
+panfrost_get_timestamp(struct pipe_screen *_screen)
+{
+   return os_time_get_nano();
+}
+
+static void
+panfrost_fence_reference(struct pipe_screen *screen,
+                         struct pipe_fence_handle **ptr,
+                         struct pipe_fence_handle *fence)
+{
+   *ptr = fence;
+}
+
+static boolean
+panfrost_fence_finish(struct pipe_screen *screen,
+                      struct pipe_context *ctx,
+                      struct pipe_fence_handle *fence,
+                      uint64_t timeout)
+{
+   assert(fence);
+   return TRUE;
+}
+
+static const void *
+panfrost_screen_get_compiler_options(struct pipe_screen *pscreen,
+                                enum pipe_shader_ir ir,
+                                enum pipe_shader_type shader)
+{
+        return &midgard_nir_options;
+}
+
+struct pipe_screen *
+panfrost_create_screen(int fd, struct renderonly *ro)
+{
+   struct panfrost_screen *screen = CALLOC_STRUCT(panfrost_screen);
+
+   if (!screen)
+      return NULL;
+
+   screen->base.destroy = panfrost_destroy_screen;
+
+   screen->base.get_name = panfrost_get_name;
+   screen->base.get_vendor = panfrost_get_vendor;
+   screen->base.get_device_vendor = panfrost_get_device_vendor; 
+   screen->base.get_param = panfrost_get_param;
+   screen->base.get_shader_param = panfrost_get_shader_param;
+   screen->base.get_paramf = panfrost_get_paramf;
+   screen->base.get_timestamp = panfrost_get_timestamp;
+   screen->base.is_format_supported = panfrost_is_format_supported;
+   screen->base.context_create = panfrost_create_context;
+   screen->base.flush_frontbuffer = panfrost_flush_frontbuffer;
+   screen->base.get_compiler_options = panfrost_screen_get_compiler_options;
+   screen->base.fence_reference = panfrost_fence_reference;
+   screen->base.fence_finish = panfrost_fence_finish;
+
+   panfrost_init_screen_texture_funcs(&screen->base);
+
+   return &screen->base;
+}
diff --git a/src/gallium/drivers/panfrost/pan_screen.h b/src/gallium/drivers/panfrost/pan_screen.h
new file mode 100644
index 0000000..e8be0ab
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_screen.h
@@ -0,0 +1,50 @@
+/**************************************************************************
+ * 
+ * Copyright 2007 VMware, Inc.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ * 
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ * 
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ * 
+ **************************************************************************/
+
+/* Authors:  Keith Whitwell <keithw@vmware.com>
+ */
+
+#ifndef PAN_SCREEN_H
+#define PAN_SCREEN_H
+
+#include "pipe/p_screen.h"
+#include "pipe/p_defines.h"
+
+struct panfrost_context;
+
+struct panfrost_screen {
+   struct pipe_screen base;
+   struct panfrost_context *any_context;
+};
+
+static inline struct panfrost_screen *
+panfrost_screen( struct pipe_screen *pipe )
+{
+   return (struct panfrost_screen *)pipe;
+}
+
+#endif /* PAN_SCREEN_H */
diff --git a/src/gallium/drivers/panfrost/pan_slowfb.c b/src/gallium/drivers/panfrost/pan_slowfb.c
new file mode 100644
index 0000000..81ce240
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_slowfb.c
@@ -0,0 +1,73 @@
+#include <stdint.h>
+#include "pan_slowfb.h"
+
+#define USE_SHM
+
+#ifndef __ANDROID__
+
+#include <X11/Xlib.h>
+
+#ifdef USE_SHM
+#include <stdio.h>
+#include <stdlib.h>
+#include <memory.h>
+#include <sys/ipc.h>
+#include <sys/shm.h>
+#include <X11/extensions/XShm.h>
+#endif
+
+Display *d;
+Window w;
+XImage *image;
+GC gc;
+
+struct slowfb_info slowfb_init(uint8_t *framebuffer, int width, int height) {
+	d = XOpenDisplay(NULL);
+	int black = BlackPixel(d, DefaultScreen(d));
+	w = XCreateSimpleWindow(d, DefaultRootWindow(d), 0, 0, 200, 100, 0, black, black);
+	XSelectInput(d, w, StructureNotifyMask);
+	XMapWindow(d, w);
+	gc = XCreateGC(d, w, 0, NULL);
+	for (;;) {
+		XEvent e;
+		XNextEvent(d, &e);
+		if (e.type == MapNotify) break;
+	}
+
+
+#ifdef USE_SHM
+	XShmSegmentInfo *shminfo = calloc(1, sizeof(XShmSegmentInfo));
+	image = XShmCreateImage(d, DefaultVisual(d, 0), 24, ZPixmap, NULL, shminfo, width, height);
+	shminfo->shmid = shmget(IPC_PRIVATE, image->bytes_per_line * image->height, IPC_CREAT|0777);
+	shminfo->shmaddr = image->data = shmat(shminfo->shmid, 0, 0);
+	shminfo->readOnly = False;
+	XShmAttach(d, shminfo);
+#else
+	image = XCreateImage(d, DefaultVisual(d, 0), 24, ZPixmap, 0, (char *) framebuffer, 2048, 1280, 32, 0);
+#endif
+
+	struct slowfb_info info = {
+		.framebuffer = (uint8_t *) image->data,
+		.stride = image->bytes_per_line
+	};
+
+	return info;
+}
+void slowfb_update(uint8_t *framebuffer, int width, int height) {
+#ifdef USE_SHM
+	XShmPutImage(d, w, gc, image, 0, 0, 0, 0, 2048, 1280, False);
+	XFlush(d);
+#else
+	XPutImage(d, w, gc, image, 0, 0, 0, 0, 2048, 1280);
+#endif
+}
+
+#else
+
+struct slowfb_info slowfb_init(uint8_t *framebuffer, int width, int height) {
+}
+
+void slowfb_update(uint8_t *framebuffer, int width, int height) {
+}
+
+#endif
diff --git a/src/gallium/drivers/panfrost/pan_slowfb.h b/src/gallium/drivers/panfrost/pan_slowfb.h
new file mode 100644
index 0000000..659ef79
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_slowfb.h
@@ -0,0 +1,26 @@
+/*
+ * © Copyright 2018 Alyssa Rosenzweig
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __SLOW_FRAMEBUFFER_H__
+#define __SLOW_FRAMEBUFFER_H__
+
+struct slowfb_info {
+	uint8_t *framebuffer;
+	int stride;
+};
+
+struct slowfb_info slowfb_init(uint8_t *framebuffer, int width, int height);
+void slowfb_update(uint8_t *framebuffer, int width, int height);
+
+#endif /* __SLOW_FRAMEBUFFER_H__ */
diff --git a/src/gallium/drivers/panfrost/pan_swizzle.c b/src/gallium/drivers/panfrost/pan_swizzle.c
new file mode 100644
index 0000000..172bef2
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_swizzle.c
@@ -0,0 +1,229 @@
+/*
+ * Copyright (c) 2012-2013 Luc Verhaegen <libv@skynet.be>
+ * Copyright (c) 2018 Alyssa Rosenzweig <alyssa@rosenzweig.io>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sub license,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include <stdio.h>
+#include "pan_swizzle.h"
+
+/* Space a group of 4-bits out. For instance, 0x7 -- that is, 0b111 -- would
+ * become 0b10101 */
+
+static inline int
+space_bits_4(int i)
+{
+	return ((i & 0x8) << 3) |
+		((i & 0x4) << 2) |
+		((i & 0x2) << 1) |
+		((i & 0x1) << 0);
+}
+
+/* Generate lookup table for the space filler curve. Note this is a 1:1
+ * mapping, just with bits twiddled around. */
+
+uint32_t space_filler[16][16];
+uint32_t space_filler_packed4[16][4];
+
+void
+panfrost_generate_space_filler_indices()
+{
+	for (int y = 0; y < 16; ++y) {
+		for (int x = 0; x < 16; ++x) {
+			space_filler[y][x] =
+				space_bits_4(y ^ x) | (space_bits_4(y) << 1);
+		}
+
+		for (int q = 0; q < 4; ++q) {
+			space_filler_packed4[y][q] =
+				(space_filler[y][(q*4) + 0] << 0) |
+				(space_filler[y][(q*4) + 1] << 8) |
+				(space_filler[y][(q*4) + 2] << 16) |
+				(space_filler[y][(q*4) + 3] << 24);
+		}
+	}
+}
+
+static void
+swizzle_bpp1_align16(int width, int height, int source_stride, int block_pitch,
+		   const uint8_t *pixels,
+		   uint8_t *ldest)
+{
+	for (int y = 0; y < height; ++y) {
+		{
+			int block_y = y & ~(0x0f);
+			int rem_y = y & 0x0f;
+			uint8_t *block_start_s = ldest + (block_y * block_pitch);
+			const uint8_t *source_start = pixels + (y * source_stride);
+			const uint8_t *source_end = source_start + width;
+
+			/* Operate on blocks of 16 pixels to minimise bookkeeping */
+
+			for (; source_start < source_end; block_start_s += 16*16, source_start += 16) {
+				const uint32_t *src_32 = (const uint32_t *) source_start;
+
+				for (int q = 0; q < 4; ++q) {
+					uint32_t src = src_32[q];
+					uint32_t spaced = space_filler_packed4[rem_y][q];
+					uint16_t *bs = (uint16_t *) block_start_s;
+
+					int spacedA = (spaced >> 0) & 0xFF;
+					int spacedB = (spaced >> 16) & 0xFF;
+
+					bs[spacedA >> 1] = (src >> 0) & 0xFFFF;
+					bs[spacedB >> 1] = (src >> 16) & 0xFFFF;
+				}
+			}
+		}
+
+		++y;
+
+		{
+			int block_y = y & ~(0x0f);
+			int rem_y = y & 0x0f;
+			uint8_t *block_start_s = ldest + (block_y * block_pitch);
+			const uint8_t *source_start = pixels + (y * source_stride);
+			const uint8_t *source_end = source_start + width;
+
+			/* Operate on blocks of 16 pixels to minimise bookkeeping */
+
+			for (; source_start < source_end; block_start_s += 16*16, source_start += 16) {
+				const uint32_t *src_32 = (const uint32_t *) source_start;
+
+				for (int q = 0; q < 4; ++q) {
+					uint32_t src = src_32[q];
+					uint32_t spaced = space_filler_packed4[rem_y][q];
+
+					block_start_s[(spaced >> 0) & 0xFF] = (src >> 0) & 0xFF;
+					block_start_s[(spaced >> 8) & 0xFF] = (src >> 8) & 0xFF;
+
+					block_start_s[(spaced >> 16) & 0xFF] = (src >> 16) & 0xFF;
+					block_start_s[(spaced >> 24) & 0xFF] = (src >> 24) & 0xFF;
+				}
+			}
+		}
+
+	}
+}
+
+static void
+swizzle_bpp4_align16(int width, int height, int source_stride, int block_pitch,
+		   const uint32_t *pixels,
+		   uint32_t *ldest)
+{
+	for (int y = 0; y < height; ++y) {
+		int block_y = y & ~(0x0f);
+		int rem_y = y & 0x0f;
+		uint32_t *block_start_s = ldest + (block_y * block_pitch);
+		const uint32_t *source_start = pixels + (y * source_stride);
+		const uint32_t *source_end = source_start + width;
+
+		/* Operate on blocks of 16 pixels to minimise bookkeeping */
+
+		for (; source_start < source_end; block_start_s += 16*16, source_start += 16) {
+			for (int j = 0; j < 16; ++j)
+				block_start_s[space_filler[rem_y][j]] = source_start[j];
+		}
+	}
+}
+
+void
+panfrost_texture_swizzle(int width, int height, int bytes_per_pixel, int source_stride,
+		   const uint8_t *pixels,
+		   uint8_t *ldest)
+{
+	/* Calculate maximum size, overestimating a bit */
+	int block_pitch = ALIGN(width, 16) >> 4; 
+
+	/* Use fast path if available */
+	if (bytes_per_pixel == 4 /* && (ALIGN(width, 16) == width) */) {
+		swizzle_bpp4_align16(width, height, source_stride >> 2, (block_pitch * 256 >> 4), (const uint32_t *) pixels, (uint32_t *) ldest);
+		return;
+	} else if (bytes_per_pixel == 1 /* && (ALIGN(width, 16) == width) */) {
+		swizzle_bpp1_align16(width, height, source_stride, (block_pitch * 256 >> 4), pixels, (uint8_t *) ldest);
+		return;
+	}
+
+	/* Otherwise, default back on generic path */
+
+	for (int y = 0; y < height; ++y) {
+		int block_y = y >> 4;
+		int rem_y = y & 0x0F;
+		int block_start_s = block_y * block_pitch * 256;
+		int source_start = y * source_stride;
+
+		for (int x = 0; x < width; ++x) {
+			int block_x_s = (x >> 4) * 256;
+			int rem_x = x & 0x0F;
+
+			int index = space_filler[rem_y][rem_x];
+			const uint8_t *source = &pixels[source_start + bytes_per_pixel * x];
+			uint8_t *dest = ldest + bytes_per_pixel * (block_start_s + block_x_s + index);
+
+			for (int b = 0; b < bytes_per_pixel; ++b)
+				dest[b] = source[b];
+		}
+	}
+}
+
+
+unsigned
+panfrost_swizzled_size(int width, int height, int bytes_per_pixel)
+{
+	/* Calculate maximum size, overestimating a bit */
+	int block_pitch = ALIGN(width, 16) >> 4; 
+	unsigned sz = bytes_per_pixel * 256 * ((height >> 4) + 1) * block_pitch;
+
+	return sz;
+}
+
+#if 0
+#include <stdio.h>
+#include <stdlib.h>
+#include <memory.h>
+#define TW 1920
+#define TH 1080
+void main() {
+	panfrost_generate_space_filler_indices();
+
+	uint8_t in[TW*TH*4];
+	for(int i = 0; i < TW*TH*4; ++i) in[i] = i;
+
+	uint8_t *out = malloc(TW*TH*4*2);
+
+	for (int i = 0; i < 60; ++i) {
+		//swizzle_bpp4_align16(TW, TH, TW*4, TW>>4, (uint32_t *) in, (uint32_t *) out);
+		//panfrost_texture_swizzle_bpp4(TW, TH, TW*4, (uint32_t *) in, (uint32_t *) out);
+		//panfrost_texture_swizzle(TW, TH, 4, TW*4, (uint32_t *) in, (uint32_t *) out);
+
+		int block_pitch = ALIGN(TW, 16) >> 4; 
+		swizzle_bpp1_align16(TW, TH, TW, (block_pitch * 256 >> 4), in, (uint8_t *) out);
+	}
+
+#if 0
+	uint8_t *reference = malloc(TW*TH*4*2);
+	panfrost_texture_swizzle(TW, TH, 1, TW, (uint8_t *) in, (uint8_t *) reference);
+
+	if(memcmp(reference, out, TW*TH*4)) printf("XXX\n");
+#endif
+	printf("ref %X\n", out[0]);
+}
+#endif
diff --git a/src/gallium/drivers/panfrost/pan_swizzle.h b/src/gallium/drivers/panfrost/pan_swizzle.h
new file mode 100644
index 0000000..5951f3f
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_swizzle.h
@@ -0,0 +1,32 @@
+/*
+ * © Copyright 2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __TEXSWZ_H__
+#define __TEXSWZ_H__
+
+#include <stdint.h>
+#include "pan_nondrm.h"
+
+void
+panfrost_generate_space_filler_indices(void);
+
+void
+panfrost_texture_swizzle(int width, int height, int bytes_per_pixel, int source_stride, 
+		   const uint8_t *pixels,
+		   uint8_t *ldest);
+
+unsigned
+panfrost_swizzled_size(int width, int height, int bytes_per_pixel);
+
+#endif 
diff --git a/src/gallium/drivers/panfrost/pan_texture.c b/src/gallium/drivers/panfrost/pan_texture.c
new file mode 100644
index 0000000..f152b4f
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_texture.c
@@ -0,0 +1,118 @@
+/**************************************************************************
+ * 
+ * Copyright 2006 VMware, Inc.
+ * All Rights Reserved.
+ * 
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ * 
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ * 
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ * 
+ **************************************************************************/
+ /*
+  * Authors:
+  *   Keith Whitwell <keithw@vmware.com>
+  *   Michel Dänzer <daenzer@vmware.com>
+  */
+
+#include "pipe/p_defines.h"
+#include "util/u_inlines.h"
+
+#include "util/u_format.h"
+#include "util/u_math.h"
+#include "util/u_memory.h"
+#include "util/u_transfer.h"
+#include "util/u_surface.h"
+
+#include "pan_texture.h"
+#include "pan_screen.h"
+
+#include "renderonly/renderonly.h"
+
+static boolean
+panfrost_can_create_resource(struct pipe_screen *screen,
+                             const struct pipe_resource *res)
+{
+   return TRUE;
+}
+
+#define __PAN_GALLIUM
+#include <pan_context.h>
+
+static struct pipe_resource *
+panfrost_resource_create(struct pipe_screen *screen,
+                         const struct pipe_resource *templat)
+{
+   return panfrost_resource_create_front(screen, templat, NULL);
+}
+
+static void
+panfrost_resource_destroy(struct pipe_screen *pscreen,
+			  struct pipe_resource *pt)
+{
+   FREE(pt);
+}
+
+static struct pipe_resource *
+panfrost_resource_from_handle(struct pipe_screen *screen,
+                              const struct pipe_resource *templat,
+                              struct winsys_handle *whandle,
+                              unsigned usage)
+{
+	assert(0);
+}
+
+
+static boolean
+panfrost_resource_get_handle(struct pipe_screen *screen,
+                             struct pipe_context *ctx,
+                             struct pipe_resource *pt,
+                             struct winsys_handle *handle,
+                             unsigned usage)
+{
+	struct panfrost_resource *rsrc = (struct panfrost_resource *) pt;
+
+	handle->stride = rsrc->stride;
+
+	if (handle->type == WINSYS_HANDLE_TYPE_SHARED) {
+		printf("Missed shared handle\n");
+		return FALSE;
+		//return etna_bo_get_name(rsc->bo, &handle->handle) == 0;
+	} else if (handle->type == WINSYS_HANDLE_TYPE_KMS) {
+		printf("Missed nonrenderonly KMS\n");
+		return FALSE;
+		//handle->handle = etna_bo_handle(rsc->bo);
+	} else if (handle->type == WINSYS_HANDLE_TYPE_FD) {
+		printf("Missed dmabuf\n");
+		return FALSE;
+		//handle->handle = etna_bo_dmabuf(rsc->bo);
+	}
+
+	return FALSE;
+}
+
+void
+panfrost_init_screen_texture_funcs(struct pipe_screen *screen)
+{
+   screen->resource_create = panfrost_resource_create;
+   screen->resource_create_front = panfrost_resource_create_front;
+   screen->resource_destroy = panfrost_resource_destroy;
+   screen->resource_from_handle = panfrost_resource_from_handle;
+   screen->resource_get_handle = panfrost_resource_get_handle;
+   screen->can_create_resource = panfrost_can_create_resource;
+}
diff --git a/src/gallium/drivers/panfrost/pan_texture.h b/src/gallium/drivers/panfrost/pan_texture.h
new file mode 100644
index 0000000..828c9d4
--- /dev/null
+++ b/src/gallium/drivers/panfrost/pan_texture.h
@@ -0,0 +1,36 @@
+/**************************************************************************
+ * 
+ * Copyright 2007 VMware, Inc.
+ * All Rights Reserved.
+ * 
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ * 
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ * 
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ * 
+ **************************************************************************/
+
+#ifndef SP_TEXTURE_H
+#define SP_TEXTURE_H
+
+#include "pipe/p_state.h"
+
+extern void
+panfrost_init_screen_texture_funcs(struct pipe_screen *screen);
+
+#endif /* SP_TEXTURE */
diff --git a/src/gallium/drivers/panfrost/panwrap/meson.build b/src/gallium/drivers/panfrost/panwrap/meson.build
new file mode 100644
index 0000000..54d2441
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/meson.build
@@ -0,0 +1,18 @@
+srcs = [
+    'panwrap-syscall.c',
+    'panwrap-util.c',
+    'panwrap-mmap.c',
+    'panwrap-decoder.c',
+]
+
+shared_library(
+    'panwrap',
+    srcs,
+    include_directories: [inc_common, inc_include, inc_src, include_directories('../include')],
+    dependencies: [
+        cc.find_library('m', require: true),
+        cc.find_library('dl', require: true),
+	dependency('threads')
+    ],
+    install: true,
+)
diff --git a/src/gallium/drivers/panfrost/panwrap/panwrap-decoder.c b/src/gallium/drivers/panfrost/panwrap/panwrap-decoder.c
new file mode 100644
index 0000000..9af6052
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/panwrap-decoder.c
@@ -0,0 +1,1974 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#include <mali-ioctl.h>
+#include <mali-job.h>
+#include <stdio.h>
+#include <memory.h>
+#include "panwrap.h"
+
+#define MEMORY_PROP(obj, p) {\
+	char *a = pointer_as_memory_reference(obj->p); \
+	panwrap_prop("%s = %s", #p, a); \
+	free(a); \
+}
+
+#define MEMORY_COMMENT(obj, p) {\
+	char *a = pointer_as_memory_reference(obj->p); \
+	panwrap_msg("%s = %s\n", #p, a); \
+	free(a); \
+}
+
+#define DYN_MEMORY_PROP(obj, no, p) { \
+	if (obj->p) \
+		panwrap_prop("%s = %s_%d_p", #p, #p, no); \
+}
+
+#define FLAG_INFO(flag) { MALI_GL_##flag, "MALI_GL_" #flag }
+static const struct panwrap_flag_info gl_enable_flag_info[] = {
+	FLAG_INFO(CULL_FACE_FRONT),
+	FLAG_INFO(CULL_FACE_BACK),
+	{}
+};
+#undef FLAG_INFO
+
+#define FLAG_INFO(flag) { MALI_CLEAR_##flag, "MALI_CLEAR_" #flag }
+static const struct panwrap_flag_info clear_flag_info[] = {
+	FLAG_INFO(FAST),
+	FLAG_INFO(SLOW),
+	FLAG_INFO(SLOW_STENCIL),
+	{}
+};
+#undef FLAG_INFO
+
+#define FLAG_INFO(flag) { MALI_MASK_##flag, "MALI_MASK_" #flag }
+static const struct panwrap_flag_info mask_flag_info[] = {
+	FLAG_INFO(R),
+	FLAG_INFO(G),
+	FLAG_INFO(B),
+	FLAG_INFO(A),
+	{}
+};
+#undef FLAG_INFO
+
+#define FLAG_INFO(flag) { MALI_##flag, "MALI_" #flag }
+static const struct panwrap_flag_info u3_flag_info[] = {
+	FLAG_INFO(HAS_MSAA),
+	FLAG_INFO(CAN_DISCARD),
+	FLAG_INFO(HAS_BLEND_SHADER),
+	FLAG_INFO(DEPTH_TEST),
+	{}
+};
+
+static const struct panwrap_flag_info u4_flag_info[] = {
+	FLAG_INFO(NO_MSAA),
+	FLAG_INFO(NO_DITHER),
+	FLAG_INFO(DEPTH_RANGE_A),
+	FLAG_INFO(DEPTH_RANGE_B),
+	FLAG_INFO(STENCIL_TEST),
+	FLAG_INFO(SAMPLE_ALPHA_TO_COVERAGE_NO_BLEND_SHADER),
+	{}
+};
+#undef FLAG_INFO
+
+#define FLAG_INFO(flag) { MALI_FRAMEBUFFER_##flag, "MALI_FRAMEBUFFER_" #flag }
+static const struct panwrap_flag_info fb_fmt_flag_info[] = {
+	FLAG_INFO(MSAA_A),
+	FLAG_INFO(MSAA_B),
+	FLAG_INFO(MSAA_8),
+	{}
+};
+#undef FLAG_INFO
+
+extern char* replace_fragment;
+extern char* replace_vertex;
+
+static char *panwrap_job_type_name(enum mali_job_type type)
+{
+#define DEFINE_CASE(name) case JOB_TYPE_ ## name: return "JOB_TYPE_" #name
+	switch (type) {
+	DEFINE_CASE(NULL);
+	DEFINE_CASE(SET_VALUE);
+	DEFINE_CASE(CACHE_FLUSH);
+	DEFINE_CASE(COMPUTE);
+	DEFINE_CASE(VERTEX);
+	DEFINE_CASE(TILER);
+	DEFINE_CASE(FUSED);
+	DEFINE_CASE(FRAGMENT);
+	case JOB_NOT_STARTED:
+		return "NOT_STARTED";
+	default:
+		panwrap_log("Warning! Unknown job type %x\n", type);
+		return "!?!?!?";
+	}
+#undef DEFINE_CASE
+}
+
+static char *panwrap_gl_mode_name(enum mali_gl_mode mode)
+{
+#define DEFINE_CASE(name) case MALI_ ## name: return "MALI_" #name
+	switch(mode) {
+	DEFINE_CASE(GL_NONE);
+	DEFINE_CASE(GL_POINTS);
+	DEFINE_CASE(GL_LINES);
+	DEFINE_CASE(GL_TRIANGLES);
+	DEFINE_CASE(GL_TRIANGLE_STRIP);
+	DEFINE_CASE(GL_TRIANGLE_FAN);
+	DEFINE_CASE(GL_LINE_STRIP);
+	DEFINE_CASE(GL_LINE_LOOP);
+	default: return "MALI_GL_TRIANGLES /* XXX: Unknown GL mode, check dump */";
+	}
+#undef DEFINE_CASE
+}
+
+#define DEFINE_CASE(name) case MALI_FUNC_ ## name: return "MALI_FUNC_" #name
+static char *panwrap_func_name(enum mali_func mode)
+{
+	switch(mode) {
+	DEFINE_CASE(NEVER);
+	DEFINE_CASE(LESS);
+	DEFINE_CASE(EQUAL);
+	DEFINE_CASE(LEQUAL);
+	DEFINE_CASE(GREATER);
+	DEFINE_CASE(NOTEQUAL);
+	DEFINE_CASE(GEQUAL);
+	DEFINE_CASE(ALWAYS);
+	default: return "MALI_FUNC_NEVER /* XXX: Unknown function, check dump */";
+	}
+}
+#undef DEFINE_CASE
+
+/* Why is this duplicated? Who knows... */
+#define DEFINE_CASE(name) case MALI_ALT_FUNC_ ## name: return "MALI_ALT_FUNC_" #name
+static char *panwrap_alt_func_name(enum mali_alt_func mode)
+{
+	switch(mode) {
+	DEFINE_CASE(NEVER);
+	DEFINE_CASE(LESS);
+	DEFINE_CASE(EQUAL);
+	DEFINE_CASE(LEQUAL);
+	DEFINE_CASE(GREATER);
+	DEFINE_CASE(NOTEQUAL);
+	DEFINE_CASE(GEQUAL);
+	DEFINE_CASE(ALWAYS);
+	default: return "MALI_FUNC_NEVER /* XXX: Unknown function, check dump */";
+	}
+}
+#undef DEFINE_CASE
+
+
+
+#define DEFINE_CASE(name) case MALI_STENCIL_ ## name: return "MALI_STENCIL_" #name
+static char *panwrap_stencil_op_name(enum mali_stencil_op op)
+{
+	switch(op) {
+	DEFINE_CASE(KEEP);
+	DEFINE_CASE(REPLACE);
+	DEFINE_CASE(ZERO);
+	DEFINE_CASE(INVERT);
+	DEFINE_CASE(INCR_WRAP);
+	DEFINE_CASE(DECR_WRAP);
+	DEFINE_CASE(INCR);
+	DEFINE_CASE(DECR);
+	default: return "MALI_STENCIL_KEEP /* XXX: Unknown stencil op, check dump */";
+	}
+}
+
+#undef DEFINE_CASE
+
+#define DEFINE_CASE(name) case MALI_CHANNEL_## name: return "MALI_CHANNEL_" #name
+static char *panwrap_channel_name(enum mali_channel channel)
+{
+	switch(channel) {
+	DEFINE_CASE(RED);
+	DEFINE_CASE(GREEN);
+	DEFINE_CASE(BLUE);
+	DEFINE_CASE(ALPHA);
+	DEFINE_CASE(ZERO);
+	DEFINE_CASE(ONE);
+	DEFINE_CASE(RESERVED_0);
+	DEFINE_CASE(RESERVED_1);
+	default: return "MALI_CHANNEL_ZERO /* XXX: Unknown channel, check dump */";
+	}
+}
+#undef DEFINE_CASE
+
+#define DEFINE_CASE(name) case MALI_WRAP_## name: return "MALI_WRAP_" #name
+static char *panwrap_wrap_mode_name(enum mali_wrap_mode op)
+{
+	switch(op) {
+	DEFINE_CASE(REPEAT);
+	DEFINE_CASE(CLAMP_TO_EDGE);
+	DEFINE_CASE(CLAMP_TO_BORDER);
+	DEFINE_CASE(MIRRORED_REPEAT);
+	default: return "MALI_WRAP_REPEAT /* XXX: Unknown wrap mode, check dump */";
+	}
+}
+#undef DEFINE_CASE
+
+static inline char *
+panwrap_decode_fbd_type(enum mali_fbd_type type)
+{
+	if (type == MALI_SFBD)      return "SFBD";
+	else if (type == MALI_MFBD) return "MFBD";
+	else return "WTF!?";
+}
+
+static bool
+panwrap_deduplicate(const struct panwrap_mapped_memory *mem, uint64_t gpu_va, const char *name, int number)
+{
+	if (mem->touched[(gpu_va - mem->gpu_va) / sizeof(uint32_t)]) {
+		/* XXX: Is this correct? */
+		panwrap_log("mali_ptr %s_%d_p = %s_%d_p;\n", name, number, name, number - 1);
+		return true;
+	}
+
+	return false;
+}
+
+static void
+panwrap_replay_sfbd(uint64_t gpu_va, int job_no)
+{
+	struct panwrap_mapped_memory *mem = panwrap_find_mapped_gpu_mem_containing(gpu_va);
+	const struct mali_single_framebuffer *PANWRAP_PTR_VAR(s, mem, (mali_ptr) gpu_va);
+
+	/* FBDs are frequently duplicated, so watch for this */
+	//if (panwrap_deduplicate(mem, gpu_va, "framebuffer", job_no)) return;
+
+	panwrap_log("struct mali_single_framebuffer framebuffer_%d = {\n", job_no);
+	panwrap_indent++;
+
+	panwrap_prop("unknown1 = 0x%" PRIx32, s->unknown1);
+	panwrap_prop("unknown2 = 0x%" PRIx32, s->unknown2);
+
+	panwrap_log(".format = ");
+	panwrap_log_decoded_flags(fb_fmt_flag_info, s->format);
+	panwrap_log_cont(",\n");
+
+	panwrap_prop("width = MALI_POSITIVE(%" PRId16 ")", s->width + 1);
+	panwrap_prop("height = MALI_POSITIVE(%" PRId16 ")", s->height + 1);
+
+	MEMORY_PROP(s, framebuffer);
+	panwrap_prop("stride = %d", s->stride);
+
+	/* Earlier in the actual commandstream -- right before width -- but we
+	 * delay to flow nicer */
+
+	panwrap_log(".clear_flags = ");
+	panwrap_log_decoded_flags(clear_flag_info, s->clear_flags);
+	panwrap_log_cont(",\n");
+
+	if (s->depth_buffer | s->depth_buffer_enable) {
+		MEMORY_PROP(s, depth_buffer);
+		panwrap_prop("depth_buffer_enable = %s", DS_ENABLE(s->depth_buffer_enable));
+	}
+
+	if (s->stencil_buffer | s->stencil_buffer_enable) {
+		MEMORY_PROP(s, stencil_buffer);
+		panwrap_prop("stencil_buffer_enable = %s", DS_ENABLE(s->stencil_buffer_enable));
+	}
+
+	if (s->clear_color_1 | s->clear_color_2 | s->clear_color_3 | s->clear_color_4) {
+		panwrap_prop("clear_color_1 = 0x%" PRIx32, s->clear_color_1);
+		panwrap_prop("clear_color_2 = 0x%" PRIx32, s->clear_color_2);
+		panwrap_prop("clear_color_3 = 0x%" PRIx32, s->clear_color_3);
+		panwrap_prop("clear_color_4 = 0x%" PRIx32, s->clear_color_4);
+	}
+
+	if (s->clear_depth_1 != 0 || s->clear_depth_2 != 0 || s->clear_depth_3 != 0 || s->clear_depth_4 != 0) {
+		panwrap_prop("clear_depth_1 = %f", s->clear_depth_1);
+		panwrap_prop("clear_depth_2 = %f", s->clear_depth_2);
+		panwrap_prop("clear_depth_3 = %f", s->clear_depth_3);
+		panwrap_prop("clear_depth_4 = %f", s->clear_depth_4);
+	}
+
+	if (s->clear_stencil) {
+		panwrap_prop("clear_stencil = 0x%x", s->clear_stencil);
+	}
+
+	MEMORY_PROP(s, unknown_address_0);
+	MEMORY_PROP(s, unknown_address_1);
+	MEMORY_PROP(s, unknown_address_2);
+
+	panwrap_prop("resolution_check = 0x%" PRIx32, s->resolution_check);
+	panwrap_prop("tiler_flags = 0x%" PRIx32, s->tiler_flags);
+
+	MEMORY_PROP(s, tiler_heap_free);
+	MEMORY_PROP(s, tiler_heap_end);
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	panwrap_prop("zero0 = 0x%" PRIx64, s->zero0);
+	panwrap_prop("zero1 = 0x%" PRIx64, s->zero1);
+	panwrap_prop("zero2 = 0x%" PRIx32, s->zero2);
+	panwrap_prop("zero4 = 0x%" PRIx32, s->zero4);
+
+	int zero_sum_pun = 0;
+#if 0
+	zero_sum_pun += s->zero0;
+	zero_sum_pun += s->zero1;
+	zero_sum_pun += s->zero2;
+	zero_sum_pun += s->zero4;
+#endif
+	printf(".zero3 = {");
+	for (int i = 0; i < sizeof(s->zero3)/sizeof(s->zero3[0]); ++i)
+		printf("%X, ", s->zero3[i]);
+	printf("},\n");
+	
+	printf(".zero6 = {");
+	for (int i = 0; i < sizeof(s->zero6)/sizeof(s->zero6[0]); ++i)
+		printf("%X, ", s->zero6[i]);
+	printf("},\n");
+
+	if (zero_sum_pun)
+		panwrap_msg("Zero sum tripped (%d), replay may be wrong\n", zero_sum_pun);
+
+	TOUCH(mem, (mali_ptr) gpu_va, *s, "framebuffer", job_no, true);
+}
+
+static void
+panwrap_replay_mfbd_bfr(uint64_t gpu_va, int job_no)
+{
+	struct panwrap_mapped_memory *mem = panwrap_find_mapped_gpu_mem_containing(gpu_va);
+	const struct bifrost_framebuffer *PANWRAP_PTR_VAR(fb, mem, (mali_ptr) gpu_va);
+
+	if (fb->sample_locations) {
+		/* The blob stores all possible sample locations in a single buffer
+		 * allocated on startup, and just switches the pointer when switching
+		 * MSAA state. For now, we just put the data into the cmdstream, but we
+		 * should do something like what the blob does with a real driver.
+		 *
+		 * There seem to be 32 slots for sample locations, followed by another
+		 * 16. The second 16 is just the center location followed by 15 zeros
+		 * in all the cases I've identified (maybe shader vs. depth/color
+		 * samples?).
+		 */
+
+		struct panwrap_mapped_memory *smem = panwrap_find_mapped_gpu_mem_containing(fb->sample_locations);
+
+		const u16 *PANWRAP_PTR_VAR(samples, smem, fb->sample_locations);
+
+		panwrap_log("uint16_t sample_locations_%d[] = {\n", job_no);
+		panwrap_indent++;
+
+		for (int i = 0; i < 32 + 16; i++) {
+			panwrap_log("%d, %d,\n", samples[2*i], samples[2*i+1]);
+		}
+
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		TOUCH_LEN(smem, fb->sample_locations, 4 * (32 + 16),
+				"sample_locations", job_no, true);
+	}
+
+	panwrap_log("struct bifrost_framebuffer framebuffer_%d = {\n", job_no);
+	panwrap_indent++;
+
+	panwrap_prop("unk0 = 0x%x", fb->unk0);
+
+	if (fb->sample_locations)
+		panwrap_prop("sample_locations = sample_locations_%d", job_no);
+
+	/* Assume that unknown1 and tiler_meta were emitted in the last job for
+	 * now */
+	/*panwrap_prop("unknown1 = unknown1_%d_p", job_no - 1);
+	panwrap_prop("tiler_meta = tiler_meta_%d_p", job_no - 1);*/
+	MEMORY_PROP(fb, unknown1);
+	MEMORY_PROP(fb, tiler_meta);
+
+	panwrap_prop("width1 = MALI_POSITIVE(%d)", fb->width1 + 1);
+	panwrap_prop("height1 = MALI_POSITIVE(%d)", fb->height1 + 1);
+	panwrap_prop("width2 = MALI_POSITIVE(%d)", fb->width2 + 1);
+	panwrap_prop("height2 = MALI_POSITIVE(%d)", fb->height2 + 1);
+
+	panwrap_prop("unk1 = 0x%x", fb->unk1);
+	panwrap_prop("unk2 = 0x%x", fb->unk2);
+	panwrap_prop("rt_count_1 = MALI_POSITIVE(%d)", fb->rt_count_1 + 1);
+	panwrap_prop("rt_count_2 = %d", fb->rt_count_2);
+
+	panwrap_prop("unk3 = 0x%x", fb->unk3);
+	panwrap_prop("clear_stencil = 0x%x", fb->clear_stencil);
+	panwrap_prop("clear_depth = %f", fb->clear_depth);
+
+	if (fb->zero1 || fb->zero2 || fb->zero3 || fb->zero4 || fb->zero5 || fb->zero6 || fb->zero7 || fb->zero8 || fb->zero9 || fb->zero10 || fb->zero11 || fb->zero12) {
+		panwrap_msg("framebuffer zeros tripped\n");
+		panwrap_prop("zero1 = 0x%" PRIx32, fb->zero1);
+		MEMORY_PROP(fb, zero2);
+		panwrap_prop("zero3 = 0x%" PRIx32, fb->zero3);
+		panwrap_prop("zero4 = 0x%" PRIx32, fb->zero4);
+		MEMORY_PROP(fb, zero5);
+		MEMORY_PROP(fb, zero6);
+		MEMORY_PROP(fb, zero7);
+		MEMORY_PROP(fb, zero8);
+		panwrap_prop("zero9 = 0x%" PRIx64, fb->zero9);
+		panwrap_prop("zero10 = 0x%" PRIx64, fb->zero10);
+		panwrap_prop("zero11 = 0x%" PRIx64, fb->zero11);
+		panwrap_prop("zero12 = 0x%" PRIx64, fb->zero12);
+	}
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	TOUCH(mem, (mali_ptr) gpu_va, *fb, "framebuffer", job_no, true);
+
+	gpu_va += sizeof(struct bifrost_framebuffer);
+
+	if (fb->unk3 & MALI_MFBD_EXTRA) {
+		mem = panwrap_find_mapped_gpu_mem_containing(gpu_va);
+		const struct bifrost_fb_extra *PANWRAP_PTR_VAR(fbx, mem, (mali_ptr) gpu_va);
+
+		panwrap_log("struct bifrost_fb_extra fb_extra_%d = {\n", job_no);
+		panwrap_indent++;
+
+		MEMORY_PROP(fbx, checksum);
+		if (fbx->checksum_stride)
+			panwrap_prop("checksum_stride = %d", fbx->checksum_stride);
+
+		panwrap_prop("unk = 0x%x", fbx->unk);
+
+		/* TODO figure out if this is actually the right way to
+		 * determine whether AFBC is enabled
+		 */
+		if (fbx->unk & 0x10) {
+			panwrap_log(".ds_afbc = {\n");
+			panwrap_indent++;
+
+			MEMORY_PROP((&fbx->ds_afbc), depth_stencil_afbc_metadata);
+			panwrap_prop("depth_stencil_afbc_stride = %d",
+					fbx->ds_afbc.depth_stencil_afbc_stride);
+			MEMORY_PROP((&fbx->ds_afbc), depth_stencil);
+
+			if (fbx->ds_afbc.zero1 || fbx->ds_afbc.padding) {
+				panwrap_msg("Depth/stencil AFBC zeros tripped\n");
+				panwrap_prop("zero1 = 0x%" PRIx32,
+						fbx->ds_afbc.zero1);
+				panwrap_prop("padding = 0x%" PRIx64,
+						fbx->ds_afbc.padding);
+			}
+
+			panwrap_indent--;
+			panwrap_log("},\n");
+		} else {
+			panwrap_log(".ds_linear = {\n");
+			panwrap_indent++;
+
+			if (fbx->ds_linear.depth) {
+				MEMORY_PROP((&fbx->ds_linear), depth);
+				panwrap_prop("depth_stride = %d",
+						fbx->ds_linear.depth_stride);
+			}
+
+			if (fbx->ds_linear.stencil) {
+				MEMORY_PROP((&fbx->ds_linear), stencil);
+				panwrap_prop("stencil_stride = %d",
+						fbx->ds_linear.stencil_stride);
+			}
+
+			if (fbx->ds_linear.depth_stride_zero ||
+				fbx->ds_linear.stencil_stride_zero ||
+				fbx->ds_linear.zero1 || fbx->ds_linear.zero2) {
+				panwrap_msg("Depth/stencil zeros tripped\n");
+				panwrap_prop("depth_stride_zero = 0x%x",
+						fbx->ds_linear.depth_stride_zero);
+				panwrap_prop("stencil_stride_zero = 0x%x",
+						fbx->ds_linear.stencil_stride_zero);
+				panwrap_prop("zero1 = 0x%" PRIx32,
+						fbx->ds_linear.zero1);
+				panwrap_prop("zero2 = 0x%" PRIx32,
+						fbx->ds_linear.zero2);
+			}
+
+			panwrap_indent--;
+			panwrap_log("},\n");
+		}
+
+		if (fbx->zero3 || fbx->zero4) {
+			panwrap_msg("fb_extra zeros tripped\n");
+			panwrap_prop("zero3 = 0x%" PRIx64, fbx->zero3);
+			panwrap_prop("zero4 = 0x%" PRIx64, fbx->zero4);
+		}
+
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		TOUCH(mem, (mali_ptr) gpu_va, *fbx, "fb_extra", job_no, true);
+
+		gpu_va += sizeof(struct bifrost_fb_extra);
+	}
+
+
+	panwrap_log("struct bifrost_render_target rts_list_%d[] = {\n", job_no);
+	panwrap_indent++;
+
+	for (int i = 0; i < MALI_NEGATIVE(fb->rt_count_1); i++) {
+		mali_ptr rt_va = gpu_va + i * sizeof(struct bifrost_render_target);
+		mem = panwrap_find_mapped_gpu_mem_containing(rt_va);
+		const struct bifrost_render_target *PANWRAP_PTR_VAR(rt, mem, (mali_ptr) rt_va);
+
+		panwrap_log("{\n");
+		panwrap_indent++;
+
+		panwrap_prop("unk1 = 0x%" PRIx32, rt->unk1);	
+		panwrap_prop("format = 0x%" PRIx32, rt->format);	
+
+		/* TODO: How the actual heck does AFBC enabling work here? */
+		if (0) {
+#if 0
+		MEMORY_PROP(rt, afbc_metadata);
+		panwrap_prop("afbc_stride = %d", rt->afbc_stride);
+		panwrap_prop("afbc_unk = 0x%" PRIx32, rt->afbc_unk);
+#endif
+		} else {
+			panwrap_log(".chunknown = {\n");
+			panwrap_indent++;
+
+			panwrap_prop("unk = 0x%" PRIx64, rt->chunknown.unk);
+
+			char *a = pointer_as_memory_reference(rt->chunknown.pointer);
+			panwrap_prop("pointer = %s", a);
+			free(a);
+
+			panwrap_indent--;
+			panwrap_log("},\n");
+		} 
+
+		MEMORY_PROP(rt, framebuffer);
+		panwrap_prop("framebuffer_stride = %d", rt->framebuffer_stride);
+
+		if (rt->clear_color_1 | rt->clear_color_2 | rt->clear_color_3 | rt->clear_color_4) {
+			panwrap_prop("clear_color_1 = 0x%" PRIx32, rt->clear_color_1);
+			panwrap_prop("clear_color_2 = 0x%" PRIx32, rt->clear_color_2);
+			panwrap_prop("clear_color_3 = 0x%" PRIx32, rt->clear_color_3);
+			panwrap_prop("clear_color_4 = 0x%" PRIx32, rt->clear_color_4);
+		}
+
+		if (rt->zero1 || rt->zero2 || rt->zero3) {
+			panwrap_msg("render target zeros tripped\n");
+			panwrap_prop("zero1 = 0x%" PRIx64, rt->zero1);
+			panwrap_prop("zero2 = 0x%" PRIx32, rt->zero2);
+			panwrap_prop("zero3 = 0x%" PRIx32, rt->zero3);
+		}
+
+		panwrap_indent--;
+		panwrap_log("},\n");
+	}
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	/* XXX: This is wrong but fixes a compiler error in the replay. FIXME */
+	panwrap_log("struct bifrost_render_target rts_%d = rts_list_%d[0];\n", job_no, job_no);
+	TOUCH_LEN(mem, (mali_ptr) gpu_va, MALI_NEGATIVE(fb->rt_count_1) * sizeof(struct bifrost_render_target), "rts", job_no, true);
+}
+
+static void
+panwrap_replay_attributes(const struct panwrap_mapped_memory *mem,
+			       mali_ptr addr, int job_no, char *suffix,
+			       int count, bool varying)
+{
+	char *prefix = varying ? "varyings" : "attributes";
+
+	/* Varyings in particular get duplicated between parts of the job */
+	if (panwrap_deduplicate(mem, addr, prefix, job_no)) return;
+
+	struct mali_attr *attr = panwrap_fetch_gpu_mem(mem, addr, sizeof(struct mali_attr) * count); 
+
+	char base[128];
+	snprintf(base, sizeof(base), "%s_data_%d%s", prefix, job_no, suffix);
+
+	for (int i = 0; i < count; ++i) {
+		mali_ptr raw_elements = attr[i].elements & ~3;
+
+		/* gl_VertexID and gl_InstanceID do not have elements to
+		 * decode; we would crash if we tried */
+
+		if (!varying && i < MALI_SPECIAL_ATTRIBUTE_BASE && 0) {
+			/* TODO: Attributes are not necessarily float32 vectors in general;
+			 * decoding like this without snarfing types from the shader is unsafe all things considered */
+
+			panwrap_msg("i: %d\n", i);
+
+			panwrap_log("float %s_%d[] = {\n", base, i);
+
+#define MIN(a, b) ((a > b) ? b : a)
+
+#if 0
+			struct panwrap_mapped_memory *l_mem = panwrap_find_mapped_gpu_mem_containing(raw_elements);
+
+			size_t vertex_count = attr[i].size / attr[i].stride;
+			size_t component_count = attr[i].stride / sizeof(float);
+
+			float *buffer = panwrap_fetch_gpu_mem(l_mem, raw_elements, attr[i].size);
+			panwrap_indent++;
+			for (int row = 0; row < MIN(vertex_count, 16); row++) {
+				panwrap_log_empty();
+
+				for (int i = 0; i < component_count; i++)
+					panwrap_log_cont("%ff, ", buffer[i]);
+
+				panwrap_log_cont("\n");
+
+				buffer += component_count;
+			}
+#endif
+			panwrap_indent--;
+			panwrap_log("};\n");
+
+			TOUCH_LEN(mem, raw_elements, attr[i].size, base, i, true);
+		} else {
+			/* TODO: Allocate space for varyings dynamically? */
+
+			char *a = pointer_as_memory_reference(raw_elements);
+			panwrap_log("mali_ptr %s_%d_p = %s;\n", base, i, a);
+			free(a);
+		}
+	}
+
+	panwrap_log("struct mali_attr %s_%d[] = {\n", prefix, job_no);
+	panwrap_indent++;
+
+	for (int i = 0; i < count; ++i) {
+		panwrap_log("{\n");
+		panwrap_indent++;
+
+		panwrap_prop("elements = (%s_%d_p) | %d", base, i, (int) (attr[i].elements & 3));
+		panwrap_prop("stride = 0x%" PRIx32, attr[i].stride);
+		panwrap_prop("size = 0x%" PRIx32, attr[i].size);
+		panwrap_indent--;
+		panwrap_log("}, \n");
+	}
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	TOUCH_LEN(mem, addr, sizeof(*attr) * count, prefix, job_no, true);
+}
+
+static mali_ptr
+panwrap_replay_shader_address(const char *name, mali_ptr ptr)
+{
+	/* TODO: Decode flags */
+	mali_ptr shader_ptr = ptr & ~15;
+
+	char *a = pointer_as_memory_reference(shader_ptr);
+	panwrap_prop("%s = (%s) | %d", name, a, (int) (ptr & 15));
+	free(a);
+
+	return shader_ptr;
+}
+
+static void
+panwrap_replay_stencil(const char *name, const struct mali_stencil_test *stencil)
+{
+	const char *func = panwrap_func_name(stencil->func);
+	const char *sfail = panwrap_stencil_op_name(stencil->sfail);
+	const char *dpfail = panwrap_stencil_op_name(stencil->dpfail);
+	const char *dppass = panwrap_stencil_op_name(stencil->dppass);
+
+	if (stencil->zero)
+		panwrap_msg("Stencil zero tripped: %X\n", stencil->zero);
+
+	panwrap_log(".stencil_%s = {\n", name);
+	panwrap_indent++;
+	panwrap_prop("ref = %d", stencil->ref);
+	panwrap_prop("mask = 0x%02X", stencil->mask);
+	panwrap_prop("func = %s", func);
+	panwrap_prop("sfail = %s", sfail);
+	panwrap_prop("dpfail = %s", dpfail);
+	panwrap_prop("dppass = %s", dppass);
+	panwrap_indent--;
+	panwrap_log("},\n");
+}
+
+static void
+panwrap_replay_blend_equation(const struct mali_blend_equation *blend, const char *suffix)
+{
+	if (blend->zero1)
+		panwrap_msg("Blend zero tripped: %X\n", blend->zero1);
+
+	panwrap_log(".blend_equation%s = {\n", suffix);
+	panwrap_indent++;
+
+	panwrap_prop("rgb_mode = 0x%X", blend->rgb_mode);
+	panwrap_prop("alpha_mode = 0x%X", blend->alpha_mode);
+
+	panwrap_log(".color_mask = ");
+	panwrap_log_decoded_flags(mask_flag_info, blend->color_mask);
+	panwrap_log_cont(",\n");
+
+	panwrap_indent--;
+	panwrap_log("},\n");
+}
+
+static void
+panwrap_replay_attribute_meta(int job_no, int count, const struct mali_vertex_tiler_postfix *v, bool varying, char *suffix)
+{
+	char base[128];
+	char *prefix = varying ? "varying" : "attribute";
+	snprintf(base, sizeof(base), "%s_meta", prefix); 
+
+	panwrap_log("struct mali_attr_meta %s_%d%s[] = {\n", base, job_no, suffix);
+	panwrap_indent++;
+
+	struct mali_attr_meta *attr_meta;
+	mali_ptr p = varying ? (v->varying_meta & ~0xF) : v->attribute_meta;
+	mali_ptr p_orig = p;
+
+	struct panwrap_mapped_memory *attr_mem = panwrap_find_mapped_gpu_mem_containing(p);
+
+	for (int i = 0; i < count; ++i, p += sizeof(struct mali_attr_meta)) {
+		attr_meta = panwrap_fetch_gpu_mem(attr_mem, p,
+						  sizeof(*attr_mem));
+
+		panwrap_log("{\n");
+		panwrap_indent++;
+		panwrap_prop("index = %d", attr_meta->index);
+		panwrap_prop("type = %d", attr_meta->type);
+		panwrap_prop("nr_components = MALI_POSITIVE(%d)", MALI_NEGATIVE(attr_meta->nr_components));
+
+		/* TODO: Dissect correctly */
+		panwrap_prop("is_int_signed = %d", attr_meta->is_int_signed);
+
+		panwrap_prop("not_normalised = %d", attr_meta->not_normalised);
+		panwrap_prop("unknown1 = 0x%" PRIx64, (u64) attr_meta->unknown1);
+		panwrap_prop("unknown2 = 0x%" PRIx64, (u64) attr_meta->unknown2);
+		panwrap_prop("unknown3 = 0x%" PRIx64, (u64) attr_meta->unknown3);
+		panwrap_prop("src_offset = 0x%" PRIx64, (u64) attr_meta->src_offset);
+		panwrap_indent--;
+		panwrap_log("},\n");
+
+	}
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	TOUCH_LEN(attr_mem, p_orig, sizeof(struct mali_attr_meta) * count, base, job_no, true);
+}
+
+static void
+panwrap_replay_indices(uintptr_t pindices, uint32_t index_count, int job_no)
+{
+	struct panwrap_mapped_memory *imem = panwrap_find_mapped_gpu_mem_containing(pindices);
+
+	if (imem) {
+		/* Indices are literally just a u32 array :) */
+
+		uint32_t *PANWRAP_PTR_VAR(indices, imem, pindices);
+
+		panwrap_log("uint32_t indices_%d[] = {\n", job_no);
+		panwrap_indent++;
+
+		for(unsigned i = 0; i < (index_count + 1); i += 3)
+			panwrap_log("%d, %d, %d,\n",
+					indices[i],
+					indices[i + 1],
+					indices[i + 2]);
+
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		TOUCH_LEN(imem, pindices, sizeof(uint32_t) * (index_count + 1), "indices", job_no, false);
+	}
+}
+
+/* return bits [lo, hi) of word */
+static u32
+bits(u32 word, u32 lo, u32 hi)
+{
+	if (hi - lo >= 32)
+		return word; // avoid undefined behavior with the shift
+
+	return (word >> lo) & ((1 << (hi - lo)) - 1);
+}
+
+static void
+panwrap_replay_vertex_tiler_prefix(struct mali_vertex_tiler_prefix *p, int job_no)
+{
+	panwrap_log_cont("{\n");
+	panwrap_indent++;
+
+	panwrap_prop("invocation_count = %" PRIx32, p->invocation_count);
+	panwrap_prop("size_y_shift = %d", p->size_y_shift);
+	panwrap_prop("size_z_shift = %d", p->size_z_shift);
+	panwrap_prop("workgroups_x_shift = %d", p->workgroups_x_shift);
+	panwrap_prop("workgroups_y_shift = %d", p->workgroups_y_shift);
+	panwrap_prop("workgroups_z_shift = %d", p->workgroups_z_shift);
+	panwrap_prop("workgroups_x_shift_2 = 0x%" PRIx32, p->workgroups_x_shift_2);
+
+	/* Decode invocation_count. See the comment before the definition of
+	 * invocation_count for an explanation.
+	 */
+	panwrap_msg("size: (%d, %d, %d)\n",
+		bits(p->invocation_count, 0, p->size_y_shift) + 1,
+		bits(p->invocation_count, p->size_y_shift, p->size_z_shift) + 1,
+		bits(p->invocation_count, p->size_z_shift,
+					  p->workgroups_x_shift) + 1);
+	panwrap_msg("workgroups: (%d, %d, %d)\n",
+		bits(p->invocation_count, p->workgroups_x_shift,
+					  p->workgroups_y_shift) + 1,
+		bits(p->invocation_count, p->workgroups_y_shift,
+					  p->workgroups_z_shift) + 1,
+		bits(p->invocation_count, p->workgroups_z_shift,
+					  32) + 1);
+
+	panwrap_prop("unknown_draw = 0x%" PRIx32, p->unknown_draw);
+	panwrap_prop("workgroups_x_shift_3 = 0x%" PRIx32, p->workgroups_x_shift_3);
+
+	panwrap_prop("draw_mode = %s", panwrap_gl_mode_name(p->draw_mode));
+
+	/* Index count only exists for tiler jobs anyway */ 
+
+	if (p->index_count)
+		panwrap_prop("index_count = MALI_POSITIVE(%" PRId32 ")", p->index_count + 1);
+
+	DYN_MEMORY_PROP(p, job_no, indices);
+
+	if (p->zero1) {
+		panwrap_msg("Zero tripped\n");
+		panwrap_prop("zero1 = 0x%" PRIx32, p->zero1);
+	}
+
+	panwrap_indent--;
+	panwrap_log("},\n");
+}
+
+static void
+panwrap_replay_uniform_buffers(mali_ptr pubufs, int ubufs_count, int job_no)
+{
+	struct panwrap_mapped_memory *umem = panwrap_find_mapped_gpu_mem_containing(pubufs);
+
+	struct mali_uniform_buffer_meta *PANWRAP_PTR_VAR(ubufs, umem, pubufs);
+
+	for (int i = 0; i < ubufs_count; i++) {
+		mali_ptr ptr = ubufs[i].ptr << 2;
+		struct panwrap_mapped_memory *umem2 = panwrap_find_mapped_gpu_mem_containing(ptr);
+		uint32_t *PANWRAP_PTR_VAR(ubuf, umem2, ptr);
+		char name[50];
+		snprintf(name, sizeof(name), "ubuf_%d", i);
+		/* The blob uses ubuf 0 to upload internal stuff and
+		 * uniforms that won't fit/are accessed indirectly, so
+		 * it puts it in the batchbuffer.
+		 */
+		panwrap_log("uint32_t %s_%d[] = {\n", name, job_no);
+		panwrap_indent++;
+
+		for (int j = 0; j <= ubufs[i].size; j++) {
+			for (int k = 0; k < 4; k++) {
+				if (k == 0)
+					panwrap_log("0x%"PRIx32", ", ubuf[4*j+k]);
+				else
+					panwrap_log_cont("0x%"PRIx32", ", ubuf[4*j+k]);
+
+			}
+			panwrap_log_cont("\n");
+		}
+
+
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		TOUCH_LEN(umem2, ptr, 16 * (ubufs[i].size + 1), name, job_no, i == 0);
+	}
+
+	panwrap_log("struct mali_uniform_buffer_meta uniform_buffers_%d[] = {\n",
+			job_no);
+	panwrap_indent++;
+
+	for (int i = 0; i < ubufs_count; i++) {
+		panwrap_log("{\n");
+		panwrap_indent++;
+		panwrap_prop("size = MALI_POSITIVE(%d)", ubufs[i].size + 1);
+		panwrap_prop("ptr = ubuf_%d_%d_p >> 2", i, job_no);
+		panwrap_indent--;
+		panwrap_log("},\n");
+	}
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	TOUCH_LEN(umem, pubufs, sizeof(struct mali_uniform_buffer_meta) * ubufs_count, "uniform_buffers", job_no, true);
+}
+
+static void
+panwrap_replay_scratchpad(uintptr_t pscratchpad, int job_no, char *suffix)
+{
+
+	struct panwrap_mapped_memory *mem = panwrap_find_mapped_gpu_mem_containing(pscratchpad);
+
+	struct bifrost_scratchpad *PANWRAP_PTR_VAR(scratchpad, mem, pscratchpad);
+
+	if (scratchpad->zero)
+		panwrap_msg("XXX scratchpad zero tripped");
+
+	panwrap_log("struct bifrost_scratchpad scratchpad_%d%s = {\n", job_no, suffix);
+	panwrap_indent++;
+
+	panwrap_prop("flags = 0x%x", scratchpad->flags);
+	MEMORY_PROP(scratchpad, gpu_scratchpad);
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	TOUCH(mem, pscratchpad, *scratchpad, "scratchpad", job_no, true);
+}
+
+static void
+panwrap_shader_disassemble(mali_ptr shader_ptr, int shader_no, int type,
+				bool is_bifrost)
+{
+	/* TODO */
+}
+
+static void
+panwrap_replay_vertex_tiler_postfix_pre(const struct mali_vertex_tiler_postfix *p,
+					int job_no, enum mali_job_type job_type,
+					char *suffix, bool is_bifrost)
+{
+	mali_ptr shader_meta_ptr = (u64) (uintptr_t) (p->_shader_upper << 4);
+	struct panwrap_mapped_memory *attr_mem;
+
+	/* On Bifrost, since the tiler heap (for tiler jobs) and the scratchpad
+	 * are the only things actually needed from the FBD, vertex/tiler jobs
+	 * no longer reference the FBD -- instead, this field points to some
+	 * info about the scratchpad.
+	 */
+	if (is_bifrost)
+		panwrap_replay_scratchpad(p->framebuffer & ~FBD_TYPE, job_no, suffix);
+	else if (p->framebuffer & MALI_MFBD)
+		panwrap_replay_mfbd_bfr((u64) ((uintptr_t) p->framebuffer) & FBD_MASK, job_no);
+	else
+		panwrap_replay_sfbd((u64) (uintptr_t) p->framebuffer, job_no);
+
+	int varying_count = 0, attribute_count = 0, uniform_count = 0, uniform_buffer_count = 0;
+	int texture_count = 0, sampler_count = 0;
+
+	if (shader_meta_ptr) {
+		struct panwrap_mapped_memory *smem = panwrap_find_mapped_gpu_mem_containing(shader_meta_ptr);
+		struct mali_shader_meta *PANWRAP_PTR_VAR(s, smem, shader_meta_ptr);
+
+		panwrap_log("struct mali_shader_meta shader_meta_%d%s = {\n", job_no, suffix);
+		panwrap_indent++;
+
+		/* Save for dumps */
+		attribute_count = s->attribute_count;
+		varying_count = s->varying_count;
+		texture_count = s->texture_count;
+		sampler_count = s->sampler_count;
+		if (is_bifrost) {
+			uniform_count = s->bifrost2.uniform_count;
+			uniform_buffer_count = s->bifrost1.uniform_buffer_count;
+		} else {
+			uniform_count = s->midgard1.uniform_count;
+			/* TODO figure this out */
+			uniform_buffer_count = 1;
+		}
+
+		mali_ptr shader_ptr = panwrap_replay_shader_address("shader", s->shader);
+
+
+		panwrap_prop("texture_count = %" PRId16, s->texture_count);
+		panwrap_prop("sampler_count = %" PRId16, s->sampler_count);
+		panwrap_prop("attribute_count = %" PRId16, s->attribute_count);
+		panwrap_prop("varying_count = %" PRId16, s->varying_count);
+		if (is_bifrost) {
+			panwrap_log(".bifrost1 = {\n");
+			panwrap_indent++;
+
+			panwrap_prop("uniform_buffer_count = %" PRId32, s->bifrost1.uniform_buffer_count);
+			panwrap_prop("unk1 = 0x%" PRIx32, s->bifrost1.unk1);
+
+			panwrap_indent--;
+			panwrap_log("},\n");
+		} else {
+			panwrap_log(".midgard1 = {\n");
+			panwrap_indent++;
+
+			panwrap_prop("uniform_count = %" PRId16, s->midgard1.uniform_count);
+			panwrap_prop("work_count = %" PRId16, s->midgard1.work_count);
+			panwrap_prop("unknown1 = %s0x%" PRIx32,
+					s->midgard1.unknown1 & MALI_NO_ALPHA_TO_COVERAGE ? "MALI_NO_ALPHA_TO_COVERAGE | " : "",
+					s->midgard1.unknown1 & ~MALI_NO_ALPHA_TO_COVERAGE);
+			panwrap_prop("unknown2 = 0x%" PRIx32, s->midgard1.unknown2);
+
+			panwrap_indent--;
+			panwrap_log("},\n");
+		}
+
+		if (s->depth_units || s->depth_factor) {
+			if (is_bifrost)
+				panwrap_prop("depth_units = %f", s->depth_units);
+			else
+				panwrap_prop("depth_units = MALI_NEGATIVE(%f)", s->depth_units - 1.0f);
+			panwrap_prop("depth_factor = %f", s->depth_factor);
+		}
+
+		bool invert_alpha_coverage = s->alpha_coverage & 0xFFF0;
+		uint16_t inverted_coverage = invert_alpha_coverage ? ~s->alpha_coverage : s->alpha_coverage;
+
+		panwrap_prop("alpha_coverage = %sMALI_ALPHA_COVERAGE(%f)",
+				invert_alpha_coverage ? "~" : "",
+				MALI_GET_ALPHA_COVERAGE(inverted_coverage));
+
+		panwrap_log(".unknown2_3 = ");
+
+		int unknown2_3 = s->unknown2_3;
+		int unknown2_4 = s->unknown2_4;
+		
+		/* We're not quite sure what these flags mean without the depth test, if anything */
+
+		if (unknown2_3 & (MALI_DEPTH_TEST | MALI_DEPTH_FUNC_MASK)) {
+			const char *func = panwrap_func_name(MALI_GET_DEPTH_FUNC(unknown2_3));
+			unknown2_3 &= ~MALI_DEPTH_FUNC_MASK;
+
+			panwrap_log_cont("MALI_DEPTH_FUNC(%s) | ", func);
+		}
+
+
+		panwrap_log_decoded_flags(u3_flag_info, unknown2_3);
+		panwrap_log_cont(",\n");
+
+		panwrap_prop("stencil_mask_front = 0x%02X", s->stencil_mask_front);
+		panwrap_prop("stencil_mask_back = 0x%02X", s->stencil_mask_back);
+
+		panwrap_log(".unknown2_4 = ");
+		panwrap_log_decoded_flags(u4_flag_info, unknown2_4);
+		panwrap_log_cont(",\n");
+
+		panwrap_replay_stencil("front", &s->stencil_front);
+		panwrap_replay_stencil("back", &s->stencil_back);
+
+		if (is_bifrost) {
+			panwrap_log(".bifrost2 = {\n");
+			panwrap_indent++;
+
+			panwrap_prop("unk3 = 0x%" PRIx32, s->bifrost2.unk3);
+			panwrap_prop("preload_regs = 0x%" PRIx32, s->bifrost2.preload_regs);
+			panwrap_prop("uniform_count = %" PRId32, s->bifrost2.uniform_count);
+			panwrap_prop("unk4 = 0x%" PRIx32, s->bifrost2.unk4);
+
+			panwrap_indent--;
+			panwrap_log("},\n");
+		} else {
+			panwrap_log(".midgard2 = {\n");
+			panwrap_indent++;
+
+			panwrap_prop("unknown2_7 = 0x%" PRIx32, s->midgard2.unknown2_7);
+			panwrap_indent--;
+			panwrap_log("},\n");
+		}
+
+		panwrap_prop("unknown2_8 = 0x%" PRIx32, s->unknown2_8);
+
+		bool blend_shader = false;
+
+		if (!is_bifrost) {
+			if (s->unknown2_3 & MALI_HAS_BLEND_SHADER) {
+				blend_shader = true;
+				panwrap_replay_shader_address("blend_shader", s->blend_shader);
+			} else {
+				panwrap_replay_blend_equation(&s->blend_equation, "");
+			}
+		}
+
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		TOUCH(smem, shader_meta_ptr, *s, "shader_meta", job_no, false);
+
+		/* TODO while Bifrost always uses these MRT blend fields,
+		 * presumably Midgard does as well when using the MFBD. We need
+		 * to figure out the bit to enable it on Midgard.
+		 */
+
+#if 0
+#ifdef T8XX
+		if (job_type == JOB_TYPE_TILER) {
+			panwrap_log("struct mali_blend_meta blend_meta_%d[] = {\n",
+					job_no);
+			panwrap_indent++;
+
+			int i;
+			for (i = 0; i < 4; i++) {
+				/* TODO: MRT case */
+				if (i) break;
+
+				const struct mali_blend_meta *b = &s->blend_meta[i];
+				panwrap_log("{\n");
+				panwrap_indent++;
+
+#if 1
+				panwrap_prop("unk1 = 0x%" PRIx64, b->unk1);
+				panwrap_replay_blend_equation(&b->blend_equation_1, "_1");
+				panwrap_replay_blend_equation(&b->blend_equation_2, "_2");
+
+				if (b->zero1 || b->zero2 || b->zero3) {
+					panwrap_msg("blend zero tripped\n");
+					panwrap_prop("zero1 = 0x%x", b->zero1);
+					panwrap_prop("zero2 = 0x%x", b->zero2);
+					panwrap_prop("zero3 = 0x%x", b->zero3);
+				}
+#else
+
+				panwrap_prop("unk1 = 0x%" PRIx32, b->unk1);
+				/* TODO figure out blend shader enable bit */
+				panwrap_replay_blend_equation(&b->blend_equation);
+				panwrap_prop("unk2 = 0x%" PRIx16, b->unk2);
+				panwrap_prop("index = 0x%" PRIx16, b->index);
+				panwrap_prop("unk3 = 0x%" PRIx32, b->unk3);
+#endif
+
+				panwrap_indent--;
+				panwrap_log("},\n");
+
+#if 0
+				if (b->unk2 == 3)
+					break;
+#endif
+			}
+			panwrap_indent--;
+			panwrap_log("};\n");
+
+			/* This needs to be uploaded right after the
+			 * shader_meta since it's technically part of the same
+			 * (variable-size) structure.
+			 */
+
+			TOUCH_LEN(smem, shader_meta_ptr + sizeof(struct mali_shader_meta), i * sizeof(struct mali_blend_meta), "blend_meta", job_no, false);
+		}
+#endif
+#endif
+
+		panwrap_shader_disassemble(shader_ptr, job_no, job_type, is_bifrost);
+
+		if (!is_bifrost && blend_shader)
+			panwrap_shader_disassemble(s->blend_shader & ~0xF, job_no, job_type, false);
+
+	} else
+		panwrap_msg("<no shader>\n");
+
+	if (p->viewport) {
+		struct panwrap_mapped_memory *fmem = panwrap_find_mapped_gpu_mem_containing(p->viewport);
+		struct mali_viewport *PANWRAP_PTR_VAR(f, fmem, p->viewport);
+
+		panwrap_log("struct mali_viewport viewport_%d%s = {\n", job_no, suffix);
+		panwrap_indent++;
+		panwrap_log(".floats = {\n");
+		panwrap_indent++;
+
+		for (int i = 0; i < sizeof(f->floats) / sizeof(f->floats[0]); i += 2)
+			panwrap_log("%ff, %ff,\n", f->floats[i], f->floats[i + 1]);
+
+		panwrap_indent--;
+		panwrap_log("},\n");
+
+		panwrap_prop("depth_range_n = %f", f->depth_range_n);
+		panwrap_prop("depth_range_f = %f", f->depth_range_f);
+
+		/* Only the higher coordinates are MALI_POSITIVE scaled */
+
+		panwrap_prop("viewport0 = { %d, %d }",
+				f->viewport0[0], f->viewport0[1]);
+
+		panwrap_prop("viewport1 = { MALI_POSITIVE(%d), MALI_POSITIVE(%d) }",
+				f->viewport1[0] + 1, f->viewport1[1] + 1);
+
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		TOUCH(fmem, p->viewport, *f, "viewport", job_no, true);
+	}
+
+	if (p->attribute_meta) {
+		panwrap_replay_attribute_meta(job_no, attribute_count, p, false, suffix);
+
+		attr_mem = panwrap_find_mapped_gpu_mem_containing(p->attributes);
+		panwrap_replay_attributes(attr_mem, p->attributes, job_no, suffix, attribute_count, false);
+	}
+
+	/* Varyings are encoded like attributes but not actually sent; we just
+	 * pass a zero buffer with the right stride/size set, (or whatever)
+	 * since the GPU will write to it itself */
+
+	if (p->varyings) {
+		attr_mem = panwrap_find_mapped_gpu_mem_containing(p->varyings);
+
+		/* Number of descriptors depends on whether there are
+		 * non-internal varyings */
+
+		panwrap_replay_attributes(attr_mem, p->varyings, job_no, suffix, varying_count > 1 ? 2 : 1, true);
+	}
+
+	if (p->varying_meta) {
+		panwrap_replay_attribute_meta(job_no, varying_count, p, true, suffix);
+	}
+
+	if (p->uniforms) {
+		int rows = uniform_count, width = 4;
+		size_t sz = rows * width * sizeof(float);
+
+		struct panwrap_mapped_memory *uniform_mem = panwrap_find_mapped_gpu_mem_containing(p->uniforms);
+		panwrap_fetch_gpu_mem(uniform_mem, p->uniforms, sz);
+		float *PANWRAP_PTR_VAR(uniforms, uniform_mem, p->uniforms);
+
+		panwrap_log("float uniforms_%d%s[] = {\n", job_no, suffix);
+
+		panwrap_indent++;
+		for (int row = 0; row < rows; row++) {
+			panwrap_log_empty();
+
+			for (int i = 0; i < width; i++)
+				panwrap_log_cont("%ff, ", uniforms[i]);
+
+			panwrap_log_cont("\n");
+
+			uniforms += width;
+		}
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		TOUCH_LEN(uniform_mem, p->uniforms, sz, "uniforms", job_no, true);
+	}
+
+	if (p->uniform_buffers) {
+		panwrap_replay_uniform_buffers(p->uniform_buffers, uniform_buffer_count, job_no);
+	}
+	if (p->texture_trampoline) {
+		struct panwrap_mapped_memory *mmem = panwrap_find_mapped_gpu_mem_containing(p->texture_trampoline);
+
+		if (mmem) {
+			mali_ptr *PANWRAP_PTR_VAR(u, mmem, p->texture_trampoline);
+
+			panwrap_log("uint64_t texture_trampoline_%d[] = {\n", job_no);
+			panwrap_indent++;
+
+			for (int tex = 0; tex < texture_count; ++tex) {
+				mali_ptr *PANWRAP_PTR_VAR(u, mmem, p->texture_trampoline + tex*sizeof(mali_ptr));
+				char *a = pointer_as_memory_reference(*u);
+				panwrap_log("%s,\n", a);
+				free(a);
+			}
+
+			panwrap_indent--;
+			panwrap_log("};\n");
+
+			//TOUCH(mmem, p->texture_trampoline, *u, "texture_trampoline", job_no, true);
+
+			/* Now, finally, descend down into the texture descriptor */
+			for (int tex = 0; tex < texture_count; ++tex) {
+				mali_ptr *PANWRAP_PTR_VAR(u, mmem, p->texture_trampoline + tex*sizeof(mali_ptr));
+				struct panwrap_mapped_memory *tmem = panwrap_find_mapped_gpu_mem_containing(*u);
+
+				if (tmem) {
+					struct mali_texture_descriptor *PANWRAP_PTR_VAR(t, tmem, *u);
+
+					panwrap_log("struct mali_texture_descriptor texture_descriptor_%d_%d = {\n", job_no, tex);
+					panwrap_indent++;
+
+					panwrap_prop("width = MALI_POSITIVE(%" PRId16 ")", t->width + 1);
+					panwrap_prop("height = MALI_POSITIVE(%" PRId16 ")", t->height + 1);
+					panwrap_prop("depth = MALI_POSITIVE(%" PRId16 ")", t->depth + 1);
+
+					panwrap_prop("unknown3 = %" PRId16, t->unknown3);
+					panwrap_prop("unknown3A = %" PRId8, t->unknown3A);
+					panwrap_prop("nr_mipmap_levels = %" PRId8, t->nr_mipmap_levels);
+					
+					/* TODO: Should format printing be refactored */
+					struct mali_texture_format f = t->format;
+
+					panwrap_log(".format = {\n");
+					panwrap_indent++;
+					
+					panwrap_prop("bottom = 0x%" PRIx32, f.bottom);
+					panwrap_prop("unk1 = 0x%" PRIx32, f.unk1);
+					panwrap_prop("component_size = 0x%" PRIx32, f.component_size);
+					panwrap_prop("nr_channels = MALI_POSITIVE(%" PRId32 ")", f.nr_channels + 1);
+
+					panwrap_prop("typeA = %" PRId32, f.typeA);
+
+					panwrap_prop("usage1 = 0x%" PRIx32, f.usage1);
+					panwrap_prop("is_not_cubemap = %" PRId32, f.is_not_cubemap);
+					panwrap_prop("usage2 = 0x%" PRIx32, f.usage2);
+
+					panwrap_indent--;
+					panwrap_log("},\n");
+
+					panwrap_prop("swizzle_r = %s", panwrap_channel_name(t->swizzle_r));
+					panwrap_prop("swizzle_g = %s", panwrap_channel_name(t->swizzle_g));
+					panwrap_prop("swizzle_b = %s", panwrap_channel_name(t->swizzle_b));
+					panwrap_prop("swizzle_a = %s", panwrap_channel_name(t->swizzle_a));
+
+					if (t->swizzle_zero) {
+						/* Shouldn't happen */
+						panwrap_msg("Swizzle zero tripped but replay will be fine anyway");
+						panwrap_prop("swizzle_zero = %d", t->swizzle_zero);
+					}
+
+					panwrap_prop("unknown3 = 0x%" PRIx32, t->unknown3);
+
+					panwrap_prop("unknown5 = 0x%" PRIx32, t->unknown5);
+					panwrap_prop("unknown6 = 0x%" PRIx32, t->unknown6);
+					panwrap_prop("unknown7 = 0x%" PRIx32, t->unknown7);
+
+					panwrap_log(".swizzled_bitmaps = {\n");
+					panwrap_indent++;
+
+					int bitmap_count = 1 + t->nr_mipmap_levels + t->unknown3A;
+					int max_count = sizeof(t->swizzled_bitmaps)/sizeof(t->swizzled_bitmaps[0]);
+
+					if (bitmap_count > max_count) {
+						panwrap_msg("XXX: bitmap count tripped");
+						bitmap_count = max_count;
+					}
+
+					for (int i = 0; i < bitmap_count; ++i) {
+						char *a = pointer_as_memory_reference(t->swizzled_bitmaps[i]);
+						panwrap_log("%s, \n", a);
+						free(a);
+					}
+					panwrap_indent--;
+					panwrap_log("},\n");
+
+					panwrap_indent--;
+					panwrap_log("};\n");
+
+					//TOUCH(tmem, *u, *t, "texture_descriptor", job_no, false);
+				}
+			}
+		}
+	}
+
+	if (p->sampler_descriptor) {
+		struct panwrap_mapped_memory *smem = panwrap_find_mapped_gpu_mem_containing(p->sampler_descriptor);
+
+		if (smem) {
+			struct mali_sampler_descriptor *s;
+
+			mali_ptr d = p->sampler_descriptor;
+			
+			for (int i = 0; i < sampler_count; ++i) {
+				s = panwrap_fetch_gpu_mem(smem, d + sizeof(*s)*i, sizeof(*s)); 
+
+				panwrap_log("struct mali_sampler_descriptor sampler_descriptor_%d_%d = {\n", job_no, i);
+				panwrap_indent++;
+
+				/* Only the lower two bits are understood right now; the rest we display as hex */
+				panwrap_log(".filter_mode = MALI_GL_TEX_MIN(%s) | MALI_GL_TEX_MAG(%s) | 0x%" PRIx32",\n",
+						MALI_FILTER_NAME(s->filter_mode & MALI_GL_TEX_MIN_MASK),
+						MALI_FILTER_NAME(s->filter_mode & MALI_GL_TEX_MAG_MASK),
+						s->filter_mode & ~3);
+
+				panwrap_prop("min_lod = FIXED_16(%f)", DECODE_FIXED_16(s->min_lod));
+				panwrap_prop("max_lod = FIXED_16(%f)", DECODE_FIXED_16(s->max_lod));
+
+				panwrap_prop("wrap_s = %s", panwrap_wrap_mode_name(s->wrap_s));
+				panwrap_prop("wrap_t = %s", panwrap_wrap_mode_name(s->wrap_t));
+				panwrap_prop("wrap_r = %s", panwrap_wrap_mode_name(s->wrap_r));
+
+				panwrap_prop("compare_func = %s", panwrap_alt_func_name(s->compare_func));
+
+				if (s->zero || s->zero2) {
+					panwrap_msg("Zero tripped\n");
+					panwrap_prop("zero = 0x%X, 0x%X\n", s->zero, s->zero2);
+				}
+
+				panwrap_prop("unknown2 = %d", s->unknown2);
+
+				panwrap_prop("border_color = { %f, %f, %f, %f }",
+						s->border_color[0],
+						s->border_color[1],
+						s->border_color[2],
+						s->border_color[3]);
+
+				panwrap_indent--;
+				panwrap_log("};\n");
+			}
+
+			//TOUCH(smem, p->sampler_descriptor, *s, "sampler_descriptor", job_no, false);
+		}
+	}
+}
+
+static void
+panwrap_replay_vertex_tiler_postfix(const struct mali_vertex_tiler_postfix *p, int job_no, bool is_bifrost)
+{
+	panwrap_log_cont("{\n");
+	panwrap_indent++;
+
+	MEMORY_PROP(p, position_varying); 
+	MEMORY_COMMENT(p, position_varying); 
+	DYN_MEMORY_PROP(p, job_no, uniform_buffers); 
+	MEMORY_COMMENT(p, uniform_buffers); 
+	DYN_MEMORY_PROP(p, job_no, texture_trampoline);
+	MEMORY_COMMENT(p, texture_trampoline); 
+	DYN_MEMORY_PROP(p, job_no, sampler_descriptor);
+	MEMORY_COMMENT(p, sampler_descriptor); 
+	DYN_MEMORY_PROP(p, job_no, uniforms);
+	MEMORY_COMMENT(p, uniforms); 
+	DYN_MEMORY_PROP(p, job_no, attributes); 
+	MEMORY_COMMENT(p, attributes); 
+	DYN_MEMORY_PROP(p, job_no, attribute_meta); 
+	MEMORY_COMMENT(p, attribute_meta); 
+	DYN_MEMORY_PROP(p, job_no, varyings); 
+	MEMORY_COMMENT(p, varyings); 
+	DYN_MEMORY_PROP(p, job_no, varying_meta);
+	MEMORY_COMMENT(p, varying_meta); 
+	DYN_MEMORY_PROP(p, job_no, viewport);
+	MEMORY_COMMENT(p, viewport); 
+	MEMORY_COMMENT(p, framebuffer & ~1);
+	panwrap_msg("%" PRIx64 "\n", p->viewport);
+	panwrap_msg("%" PRIx64 "\n", p->framebuffer);
+	if (is_bifrost)
+		panwrap_prop("framebuffer = scratchpad_%d_p", job_no);
+	else
+		panwrap_prop("framebuffer = framebuffer_%d_p | %s", job_no, p->framebuffer & MALI_MFBD ?"MALI_MFBD" : "0");
+
+	panwrap_prop("_shader_upper = (shader_meta_%d_p) >> 4", job_no);
+	panwrap_prop("flags = %d", p->flags); 
+
+	panwrap_indent--;
+	panwrap_log("},\n");
+}
+
+static void
+panwrap_replay_vertex_only_bfr(struct bifrost_vertex_only *v)
+{
+	panwrap_log_cont("{\n");
+	panwrap_indent++;
+
+	panwrap_prop("unk2 = 0x%x", v->unk2);
+
+	if (v->zero0 || v->zero1) {
+		panwrap_msg("vertex only zero tripped");
+		panwrap_prop("zero0 = 0x%" PRIx32, v->zero0);
+		panwrap_prop("zero1 = 0x%" PRIx64, v->zero1);
+	}
+
+	panwrap_indent--;
+	panwrap_log("}\n");
+}
+
+static void
+panwrap_replay_tiler_heap_meta(mali_ptr gpu_va, int job_no)
+{
+	
+	struct panwrap_mapped_memory *mem = panwrap_find_mapped_gpu_mem_containing(gpu_va);
+	const struct bifrost_tiler_heap_meta *PANWRAP_PTR_VAR(h, mem, gpu_va);
+
+	/* The tiler_heap_meta structure is modified by the GPU, and it's
+	 * supposed to be shared by tiler jobs corresponding to the same
+	 * fragment job, so be careful to deduplicate it here.
+	 */
+	if (panwrap_deduplicate(mem, gpu_va, "tiler_heap_meta", job_no)) return;
+
+	panwrap_log("struct mali_tiler_heap_meta tiler_heap_meta_%d = {\n", job_no);
+	panwrap_indent++;
+
+	if (h->zero) {
+		panwrap_msg("tiler heap zero tripped\n");
+		panwrap_prop("zero = 0x%x", h->zero);
+	}
+
+	for (int i = 0; i < 12; i++) {
+		if (h->zeros[i] != 0) {
+			panwrap_msg("tiler heap zero %d tripped, value %x\n",
+					i, h->zeros[i]);
+		}
+	}
+
+	panwrap_prop("heap_size = 0x%x", h->heap_size);
+	MEMORY_PROP(h, tiler_heap_start);
+	MEMORY_PROP(h, tiler_heap_free);
+	
+	/* this might point to the beginning of another buffer, when it's
+	 * really the end of the tiler heap buffer, so we have to be careful
+	 * here.
+	 */
+	char *a = pointer_as_memory_reference(h->tiler_heap_end - 1);
+	panwrap_prop("tiler_heap_end = %s + 1", a);
+	free(a); 
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	TOUCH(mem, gpu_va, *h, "tiler_heap_meta", job_no, true);
+}
+
+static void
+panwrap_replay_tiler_meta(mali_ptr gpu_va, int job_no)
+{
+	struct panwrap_mapped_memory *mem = panwrap_find_mapped_gpu_mem_containing(gpu_va);
+	const struct bifrost_tiler_meta *PANWRAP_PTR_VAR(t, mem, gpu_va);
+
+	panwrap_replay_tiler_heap_meta(t->tiler_heap_meta, job_no);
+
+	panwrap_log("struct mali_tiler_meta tiler_meta_%d = {\n", job_no);
+	panwrap_indent++;
+
+	if (t->zero0 || t->zero1) {
+		panwrap_msg("tiler meta zero tripped");
+		panwrap_prop("zero0 = 0x%" PRIx64, t->zero0);
+		panwrap_prop("zero1 = 0x%" PRIx64, t->zero1);
+	}
+
+	panwrap_prop("unk = 0x%x", t->unk);
+	panwrap_prop("width = MALI_POSITIVE(%d)", t->width + 1);
+	panwrap_prop("height = MALI_POSITIVE(%d)", t->height + 1);
+	DYN_MEMORY_PROP(t, job_no, tiler_heap_meta);
+
+	for (int i = 0; i < 12; i++) {
+		if (t->zeros[i] != 0) {
+			panwrap_msg("tiler heap zero %d tripped, value %" PRIx64 "\n",
+					i, t->zeros[i]);
+		}
+	}
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	TOUCH(mem, gpu_va, *t, "tiler_meta", job_no, true);
+}
+
+static void
+panwrap_replay_gl_enables(uint32_t gl_enables, int job_type)
+{
+	panwrap_log(".gl_enables = ");
+	
+	if (job_type == JOB_TYPE_TILER) {
+		panwrap_log_cont("MALI_GL_FRONT_FACE(MALI_GL_%s) | ",
+		    gl_enables & MALI_GL_FRONT_FACE(MALI_GL_CW) ? "CW" : "CCW");
+
+		gl_enables &= ~(MALI_GL_FRONT_FACE(1));
+	}
+
+	panwrap_log_decoded_flags(gl_enable_flag_info, gl_enables);
+
+	panwrap_log_cont(",\n");
+}
+
+static void
+panwrap_replay_tiler_only_bfr(const struct bifrost_tiler_only *t, int job_no)
+{
+	panwrap_log_cont("{\n");
+	panwrap_indent++;
+
+	panwrap_prop("line_width = %f", t->line_width);
+	DYN_MEMORY_PROP(t, job_no, tiler_meta);
+	panwrap_replay_gl_enables(t->gl_enables, JOB_TYPE_TILER);
+
+	if (t->zero0 || t->zero1 || t->zero2 || t->zero3 || t->zero4 || t->zero5
+		|| t->zero6 || t->zero7 || t->zero8) {
+		panwrap_msg("tiler only zero tripped");
+		panwrap_prop("zero0 = 0x%" PRIx32, t->zero0);
+		panwrap_prop("zero1 = 0x%" PRIx64, t->zero1);
+		panwrap_prop("zero2 = 0x%" PRIx64, t->zero2);
+		panwrap_prop("zero3 = 0x%" PRIx64, t->zero3);
+		panwrap_prop("zero4 = 0x%" PRIx64, t->zero4);
+		panwrap_prop("zero5 = 0x%" PRIx64, t->zero5);
+		panwrap_prop("zero6 = 0x%" PRIx64, t->zero6);
+		panwrap_prop("zero7 = 0x%" PRIx32, t->zero7);
+		panwrap_prop("zero8 = 0x%" PRIx64, t->zero8);
+	}
+
+	panwrap_indent--;
+	panwrap_log("},\n");
+}
+
+static int
+panwrap_replay_vertex_job_bfr(const struct mali_job_descriptor_header *h,
+				const struct panwrap_mapped_memory *mem,
+				mali_ptr payload, int job_no)
+{
+	struct bifrost_payload_vertex *PANWRAP_PTR_VAR(v, mem, payload);
+
+	panwrap_replay_vertex_tiler_postfix_pre(&v->postfix, job_no, h->job_type, "", true);
+
+	panwrap_log("struct bifrost_payload_vertex payload_%d = {\n", job_no);
+	panwrap_indent++;
+
+	panwrap_log(".prefix = ");
+	panwrap_replay_vertex_tiler_prefix(&v->prefix, job_no);
+
+	panwrap_log(".vertex = ");
+	panwrap_replay_vertex_only_bfr(&v->vertex);
+
+	panwrap_log(".postfix = ");
+	panwrap_replay_vertex_tiler_postfix(&v->postfix, job_no, true);
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	return sizeof(*v);
+}
+
+static int
+panwrap_replay_tiler_job_bfr(const struct mali_job_descriptor_header *h,
+				const struct panwrap_mapped_memory *mem,
+				mali_ptr payload, int job_no)
+{
+	struct bifrost_payload_tiler *PANWRAP_PTR_VAR(t, mem, payload);
+
+	panwrap_replay_vertex_tiler_postfix_pre(&t->postfix, job_no, h->job_type, "", true);
+
+	panwrap_replay_indices(t->prefix.indices, t->prefix.index_count, job_no);
+	panwrap_replay_tiler_meta(t->tiler.tiler_meta, job_no);
+
+	panwrap_log("struct bifrost_payload_tiler payload_%d = {\n", job_no);
+	panwrap_indent++;
+
+	panwrap_log(".prefix = ");
+	panwrap_replay_vertex_tiler_prefix(&t->prefix, job_no);
+
+	panwrap_log(".tiler = ");
+	panwrap_replay_tiler_only_bfr(&t->tiler, job_no);
+
+	panwrap_log(".postfix = ");
+	panwrap_replay_vertex_tiler_postfix(&t->postfix, job_no, true);
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	return sizeof(*t);
+}
+
+static int
+panwrap_replay_vertex_or_tiler_job_mdg(const struct mali_job_descriptor_header *h,
+					const struct panwrap_mapped_memory *mem,
+					mali_ptr payload, int job_no)
+{
+	struct midgard_payload_vertex_tiler *PANWRAP_PTR_VAR(v, mem, payload);
+
+
+	char *a = pointer_as_memory_reference(payload);
+	panwrap_msg("vt payload: %s\n", a);
+	free(a);
+
+	panwrap_replay_vertex_tiler_postfix_pre(&v->postfix, job_no, h->job_type, "", false);
+
+	panwrap_replay_indices(v->prefix.indices, v->prefix.index_count, job_no);
+
+	panwrap_log("struct midgard_payload_vertex_tiler payload_%d = {\n", job_no);
+	panwrap_indent++;
+
+	panwrap_prop("line_width = %ff", v->line_width);
+	panwrap_log(".prefix = ");
+	panwrap_replay_vertex_tiler_prefix(&v->prefix, job_no);
+
+	panwrap_replay_gl_enables(v->gl_enables, h->job_type);
+	panwrap_prop("draw_start = %d", v->draw_start);
+
+#ifdef T6XX
+	if (v->zero3) {
+		panwrap_msg("Zero tripped\n");
+		panwrap_prop("zero3 = 0x%" PRIx32, v->zero3);
+	}
+#endif
+
+	if (v->zero5) {
+		panwrap_msg("Zero tripped\n");
+		panwrap_prop("zero5 = 0x%" PRIx64, v->zero5);
+	}
+
+	panwrap_log(".postfix = ");
+	panwrap_replay_vertex_tiler_postfix(&v->postfix, job_no, false);
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	return sizeof(*v);
+}
+
+static int panwrap_replay_fragment_job(const struct panwrap_mapped_memory *mem,
+					mali_ptr payload, int job_no,
+					bool is_bifrost)
+{
+	const struct mali_payload_fragment *PANWRAP_PTR_VAR(s, mem, payload);
+
+	bool fbd_dumped = false;
+
+	if (!is_bifrost && (s->framebuffer & FBD_TYPE) == MALI_SFBD) {
+		/* Only SFBDs are understood, not MFBDs. We're speculating,
+		 * based on the versioning, kernel code, etc, that the
+		 * difference is between Single FrameBuffer Descriptor and
+		 * Multiple FrmaeBuffer Descriptor; the change apparently lines
+		 * up with multi-framebuffer support being added (T7xx onwards,
+		 * including Gxx). In any event, there's some field shuffling
+		 * that we haven't looked into yet. */
+
+		panwrap_replay_sfbd(s->framebuffer & FBD_MASK, job_no);
+		fbd_dumped = true;
+	} else if ((s->framebuffer & FBD_TYPE) == MALI_MFBD) {
+		/* We don't know if Bifrost supports SFBD's at all, since the
+		 * driver never uses them. And the format is different from
+		 * Midgard anyways, due to the tiler heap and scratchpad being
+		 * moved out into separate structures, so it's not clear what a
+		 * Bifrost SFBD would even look like without getting an actual
+		 * trace, which appears impossible.
+		 */
+
+		panwrap_replay_mfbd_bfr(s->framebuffer & FBD_MASK, job_no);
+		fbd_dumped = true;
+	}
+
+	uintptr_t p = (uintptr_t) s->framebuffer & FBD_MASK;
+
+	panwrap_log("struct mali_payload_fragment payload_%d = {\n", job_no);
+	panwrap_indent++;
+
+	/* See the comments by the macro definitions for mathematical context
+	 * on why this is so weird */
+
+	if (MALI_TILE_COORD_FLAGS(s->max_tile_coord) || MALI_TILE_COORD_FLAGS(s->min_tile_coord))
+		panwrap_msg("Tile coordinate flag missed, replay wrong\n");
+
+	panwrap_prop("min_tile_coord = MALI_COORDINATE_TO_TILE_MIN(%d, %d)",
+			MALI_TILE_COORD_X(s->min_tile_coord) << MALI_TILE_SHIFT,
+			MALI_TILE_COORD_Y(s->min_tile_coord) << MALI_TILE_SHIFT);
+
+	panwrap_prop("max_tile_coord = MALI_COORDINATE_TO_TILE_MAX(%d, %d)",
+			(MALI_TILE_COORD_X(s->max_tile_coord) + 1) << MALI_TILE_SHIFT,
+			(MALI_TILE_COORD_Y(s->max_tile_coord) + 1) << MALI_TILE_SHIFT);
+
+	/* If the FBD was just decoded, we can refer to it by pointer. If not,
+	 * we have to fallback on offsets. */
+
+	const char *fbd_type = s->framebuffer & MALI_MFBD ? "MALI_MFBD" : "MALI_SFBD";
+
+	if (fbd_dumped)
+		panwrap_prop("framebuffer = framebuffer_%d_p | %s", job_no, fbd_type);
+	else
+		panwrap_prop("framebuffer = %s | %s", pointer_as_memory_reference(p), fbd_type);
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	return sizeof(*s);
+}
+
+static int job_descriptor_number = 0;
+
+int panwrap_replay_jc(mali_ptr jc_gpu_va, bool bifrost)
+{
+	struct mali_job_descriptor_header *h;
+
+	int start_number = 0;
+
+	bool first = true;
+	bool last_size;
+
+	do {
+		struct panwrap_mapped_memory *mem =
+			panwrap_find_mapped_gpu_mem_containing(jc_gpu_va);
+
+		void *payload;
+
+		h = PANWRAP_PTR(mem, jc_gpu_va, struct mali_job_descriptor_header);
+
+		/* On Midgard, for 32-bit jobs except for fragment jobs, the
+		 * high 32-bits of the 64-bit pointer are reused to store
+		 * something else.
+		 */
+		int offset = h->job_descriptor_size == MALI_JOB_32 &&
+			h->job_type != JOB_TYPE_FRAGMENT ? 4 : 0;
+		mali_ptr payload_ptr = jc_gpu_va + sizeof(*h) - offset;
+
+		payload = panwrap_fetch_gpu_mem(mem, payload_ptr,
+						MALI_PAYLOAD_SIZE);
+
+		int job_no = job_descriptor_number++;
+
+		if (first)
+			start_number = job_no;
+
+		panwrap_log("struct mali_job_descriptor_header job_%d = {\n", job_no);
+		panwrap_indent++;
+
+		panwrap_prop("job_type = %s", panwrap_job_type_name(h->job_type));
+
+		/* Save for next job fixing */
+		last_size = h->job_descriptor_size;
+
+		if (h->job_descriptor_size)
+			panwrap_prop("job_descriptor_size = %d", h->job_descriptor_size);
+
+		if (h->exception_status)
+			panwrap_prop("exception_status = %d", h->exception_status);
+
+		if (h->first_incomplete_task)
+			panwrap_prop("first_incomplete_task = %d", h->first_incomplete_task);
+
+		if (h->fault_pointer)
+			panwrap_prop("fault_pointer = 0x%" PRIx64, h->fault_pointer);
+
+		if (h->job_barrier)
+			panwrap_prop("job_barrier = %d", h->job_barrier);
+
+		panwrap_prop("job_index = %d", h->job_index);
+
+		if (h->unknown_flags)
+			panwrap_prop("unknown_flags = %d", h->unknown_flags);
+
+		if (h->job_dependency_index_1)
+			panwrap_prop("job_dependency_index_1 = %d", h->job_dependency_index_1);
+
+		if (h->job_dependency_index_2)
+			panwrap_prop("job_dependency_index_2 = %d", h->job_dependency_index_2);
+
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		/* Do not touch the field yet -- decode the payload first, and
+		 * don't touch that either. This is essential for the uploads
+		 * to occur in sequence and therefore be dynamically allocated
+		 * correctly. Do note the size, however, for that related
+		 * reason. */
+
+		int payload_size = 0;
+
+		switch (h->job_type) {
+		case JOB_TYPE_SET_VALUE:
+			{
+				struct mali_payload_set_value *s = payload;
+
+				panwrap_log("struct mali_payload_set_value payload_%d = {\n", job_no);
+				panwrap_indent++;
+				MEMORY_PROP(s, out);
+				panwrap_prop("unknown = 0x%" PRIX64, s->unknown);
+				panwrap_indent--;
+				panwrap_log("};\n");
+
+				payload_size = sizeof(*s);
+
+				break;
+			}
+		case JOB_TYPE_TILER:
+		case JOB_TYPE_VERTEX:
+		case JOB_TYPE_COMPUTE:
+			if (bifrost) {
+				if (h->job_type == JOB_TYPE_TILER)
+					payload_size = panwrap_replay_tiler_job_bfr(h, mem, payload_ptr, job_no);
+				else
+					payload_size = panwrap_replay_vertex_job_bfr(h, mem, payload_ptr, job_no);
+			} else
+				payload_size = panwrap_replay_vertex_or_tiler_job_mdg(h, mem, payload_ptr, job_no);
+
+			break;
+		case JOB_TYPE_FRAGMENT:
+			payload_size = panwrap_replay_fragment_job(mem, payload_ptr, job_no, bifrost);
+			break;
+		default:
+			break;
+		}
+
+		/* Touch the job descriptor fields, careful about 32/64-bit */
+		TOUCH_JOB_HEADER(mem, jc_gpu_va, sizeof(*h), offset, job_no);
+
+		/* Touch the payload immediately after, sequentially */
+		TOUCH_SEQUENTIAL(mem, payload_ptr, payload_size, "payload", job_no);
+
+		/* Handle linkage */
+
+		if (!first) {
+			panwrap_log("((struct mali_job_descriptor_header *) (uintptr_t) job_%d_p)->", job_no - 1);
+
+			if (last_size)
+				panwrap_log_cont("next_job_64 = job_%d_p;\n\n", job_no);
+			else
+				panwrap_log_cont("next_job_32 = (u32) (uintptr_t) job_%d_p;\n\n", job_no);
+		}
+
+		first = false;
+
+
+	} while ((jc_gpu_va = h->job_descriptor_size ? h->next_job_64 : h->next_job_32));
+
+	return start_number;
+}
+
+static void panwrap_replay_soft_replay_payload(mali_ptr jc_gpu_va, int job_no)
+{
+	struct mali_jd_replay_payload *v;
+
+	struct panwrap_mapped_memory *mem =
+		panwrap_find_mapped_gpu_mem_containing(jc_gpu_va);
+
+	v = PANWRAP_PTR(mem, jc_gpu_va, struct mali_jd_replay_payload);
+
+	panwrap_log("struct mali_jd_replay_payload soft_replay_payload_%d = {\n", job_no);
+	panwrap_indent++;
+
+	MEMORY_PROP(v, tiler_jc_list);
+	MEMORY_PROP(v, fragment_jc);
+	MEMORY_PROP(v, tiler_heap_free);
+
+	panwrap_prop("fragment_hierarchy_mask = 0x%" PRIx32, v->fragment_hierarchy_mask);
+	panwrap_prop("tiler_hierarchy_mask = 0x%" PRIx32, v->tiler_hierarchy_mask);
+	panwrap_prop("hierarchy_default_weight = 0x%" PRIx32, v->hierarchy_default_weight);
+
+	panwrap_log(".tiler_core_req = ");
+	if (v->tiler_core_req)
+		ioctl_log_decoded_jd_core_req(v->tiler_core_req);
+	else
+		panwrap_log_cont("0");
+	panwrap_log_cont(",\n");
+
+	panwrap_log(".fragment_core_req = ");
+	if (v->fragment_core_req)
+		ioctl_log_decoded_jd_core_req(v->fragment_core_req);
+	else
+		panwrap_log_cont("0");
+	panwrap_log_cont(",\n");
+
+	panwrap_indent--;
+	panwrap_log("};\n");
+
+	TOUCH(mem, jc_gpu_va, *v, "soft_replay_payload", job_no, false);
+}
+
+int panwrap_replay_soft_replay(mali_ptr jc_gpu_va)
+{
+	struct mali_jd_replay_jc *v;
+	int start_no;
+	bool first = true;
+
+	do {
+		struct panwrap_mapped_memory *mem =
+			panwrap_find_mapped_gpu_mem_containing(jc_gpu_va);
+
+		v = PANWRAP_PTR(mem, jc_gpu_va, struct mali_jd_replay_jc);
+
+		int job_no = job_descriptor_number++;
+
+		if (first)
+			start_no = job_no;
+
+		first = false;
+
+		panwrap_log("struct mali_jd_replay_jc job_%d = {\n", job_no);
+		panwrap_indent++;
+
+		MEMORY_PROP(v, next);
+		MEMORY_PROP(v, jc);
+
+		panwrap_indent--;
+		panwrap_log("};\n");
+
+		panwrap_replay_soft_replay_payload(jc_gpu_va /* + sizeof(struct mali_jd_replay_jc) */, job_no);
+
+		TOUCH(mem, jc_gpu_va, *v, "job", job_no, false);
+	} while ((jc_gpu_va = v->next));
+
+	return start_no;
+}
diff --git a/src/gallium/drivers/panfrost/panwrap/panwrap-decoder.h b/src/gallium/drivers/panfrost/panwrap/panwrap-decoder.h
new file mode 100644
index 0000000..2931fe8
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/panwrap-decoder.h
@@ -0,0 +1,25 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef PANWRAP_DECODER_H
+#define PANWRAP_DECODER_H
+
+#include <mali-ioctl.h>
+#include <mali-job.h>
+#include "panwrap.h"
+
+int panwrap_replay_jc(mali_ptr jc_gpu_va, bool bifrost);
+int panwrap_replay_soft_replay(mali_ptr jc_gpu_va);
+
+#endif /* !PANWRAP_DECODER_H */
diff --git a/src/gallium/drivers/panfrost/panwrap/panwrap-mmap.c b/src/gallium/drivers/panfrost/panwrap/panwrap-mmap.c
new file mode 100644
index 0000000..8f26fbd
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/panwrap-mmap.c
@@ -0,0 +1,282 @@
+/*
+ * © Copyright 2017 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+#include <stdio.h>
+#include <stdlib.h>
+#include <sys/mman.h>
+#include <stdbool.h>
+#include <stdarg.h>
+#include <memory.h>
+
+#include <mali-ioctl.h>
+#include "panwrap.h"
+#include "panwrap-mmap.h"
+#ifdef HAVE_LINUX_MMAN_H
+#include <linux/mman.h>
+#endif
+#include "util/list.h"
+
+static struct panwrap_allocated_memory allocations;
+static struct panwrap_mapped_memory mmaps;
+
+#define FLAG_INFO(flag) { flag, #flag }
+static const struct panwrap_flag_info mmap_flags_flag_info[] = {
+	FLAG_INFO(MAP_SHARED),
+	FLAG_INFO(MAP_PRIVATE),
+	FLAG_INFO(MAP_ANONYMOUS),
+	FLAG_INFO(MAP_DENYWRITE),
+	FLAG_INFO(MAP_FIXED),
+	FLAG_INFO(MAP_GROWSDOWN),
+	FLAG_INFO(MAP_HUGETLB),
+	FLAG_INFO(MAP_LOCKED),
+	FLAG_INFO(MAP_NONBLOCK),
+	FLAG_INFO(MAP_NORESERVE),
+	FLAG_INFO(MAP_POPULATE),
+	FLAG_INFO(MAP_STACK),
+#if MAP_UNINITIALIZED != 0
+	FLAG_INFO(MAP_UNINITIALIZED),
+#endif
+	{}
+};
+
+static const struct panwrap_flag_info mmap_prot_flag_info[] = {
+	FLAG_INFO(PROT_EXEC),
+	FLAG_INFO(PROT_READ),
+	FLAG_INFO(PROT_WRITE),
+	{}
+};
+#undef FLAG_INFO
+
+char* pointer_as_memory_reference(mali_ptr ptr)
+{
+	struct panwrap_mapped_memory *mapped;
+	char *out = malloc(128);
+
+	/* First check for SAME_VA mappings, then look for non-SAME_VA
+	 * mappings, then for unmapped regions */
+
+	if ((ptr == (uintptr_t) ptr && (mapped = panwrap_find_mapped_mem_containing((void*) (uintptr_t) ptr))) ||
+		(mapped = panwrap_find_mapped_gpu_mem_containing(ptr))) {
+
+		snprintf(out, 128, "alloc_gpu_va_%d + %d", mapped->allocation_number, (int) (ptr - mapped->gpu_va));
+		return out;
+	} 
+
+	struct panwrap_allocated_memory *mem = NULL;
+
+	/* Find the pending unmapped allocation for the memory */
+	list_for_each_entry(struct panwrap_allocated_memory, pos, &allocations.node, node) {
+		if (ptr >= pos->gpu_va && ptr < (pos->gpu_va + pos->length)) {
+			mem = pos;
+			break;
+		}
+	}
+
+	if (mem) {
+		snprintf(out, 128, "alloc_gpu_va_%d + %d", mem->allocation_number, (int) (ptr - mem->gpu_va));
+		return out;
+	}
+
+	/* Just use the raw address if other options are exhausted */
+
+	snprintf(out, 128, MALI_PTR_FMT, ptr);
+	return out;
+}
+
+void panwrap_track_allocation(mali_ptr addr, int flags, int number, size_t length)
+{
+	struct panwrap_allocated_memory *mem = malloc(sizeof(*mem));
+
+	//list_inithead(&mem->node);
+	mem->gpu_va = addr;
+	mem->flags = flags;
+	mem->allocation_number = number;
+	mem->length = length;
+
+	list_add(&mem->node, &allocations.node);
+
+	/* XXX: Hacky workaround for cz's board */
+	if (mem->gpu_va >> 28 == 0xb)
+		panwrap_track_mmap(addr, (void *) (uintptr_t) addr, length, PROT_READ | PROT_WRITE, MAP_SHARED);
+}
+
+void panwrap_track_mmap(mali_ptr gpu_va, void *addr, size_t length,
+			int prot, int flags)
+{
+	struct panwrap_mapped_memory *mapped_mem = NULL;
+	struct panwrap_allocated_memory *mem = NULL;
+
+	/* Find the pending unmapped allocation for the memory */
+	list_for_each_entry(struct panwrap_allocated_memory, pos, &allocations.node, node) {
+		if (pos->gpu_va == gpu_va) {
+			mem = pos;
+			break;
+		}
+	}
+	if (!mem) {
+		panwrap_msg("Error: Untracked gpu memory " MALI_PTR_FMT " mapped to %p\n",
+			    gpu_va, addr);
+		panwrap_msg("\tprot = ");
+		panwrap_log_decoded_flags(mmap_prot_flag_info, prot);
+		panwrap_log_cont("\n");
+		panwrap_msg("\tflags = ");
+		panwrap_log_decoded_flags(mmap_flags_flag_info, flags);
+		panwrap_log_cont("\n");
+
+		return;
+	}
+
+	mapped_mem = malloc(sizeof(*mapped_mem));
+	list_inithead(&mapped_mem->node);
+	
+	/* Try not to break other systems... there are so many configurations
+	 * of userspaces/kernels/architectures and none of them are compatible,
+	 * ugh. */
+
+#define MEM_COOKIE_VA 0x41000
+
+	if (mem->flags & MALI_MEM_SAME_VA && gpu_va == MEM_COOKIE_VA) {
+		mapped_mem->gpu_va = (mali_ptr) (uintptr_t) addr;
+	} else { 
+		mapped_mem->gpu_va = gpu_va;
+	}
+
+	mapped_mem->length = length;
+	mapped_mem->addr = addr;
+	mapped_mem->prot = prot;
+	mapped_mem->flags = mem->flags;
+	mapped_mem->allocation_number = mem->allocation_number;
+	mapped_mem->touched = calloc(length, sizeof(bool));
+
+	list_add(&mapped_mem->node, &mmaps.node);
+
+	list_del(&mem->node);
+	free(mem);
+
+	panwrap_msg("va %d mapped to %" PRIx64 "\n", mapped_mem->allocation_number,
+			mapped_mem->gpu_va);
+
+	/* Generate somewhat semantic name for the region */
+	snprintf(mapped_mem->name, sizeof(mapped_mem->name),
+			"%s_%d",
+			mem->flags & MALI_MEM_PROT_GPU_EX ? "shader" : "memory",
+			mapped_mem->allocation_number);
+
+	/* Map region itself */
+
+	panwrap_log("uint32_t *%s = mmap64(NULL, %zd, %d, %d, fd, alloc_gpu_va_%d);\n\n",
+		    mapped_mem->name, length, prot, flags, mapped_mem->allocation_number);
+
+	panwrap_log("if (%s == MAP_FAILED) printf(\"Error mapping %s\\n\");\n\n",
+			mapped_mem->name, mapped_mem->name);
+}
+
+void panwrap_track_munmap(void *addr)
+{
+	struct panwrap_mapped_memory *mapped_mem =
+		panwrap_find_mapped_mem(addr);
+
+	if (!mapped_mem) {
+		panwrap_msg("Unknown mmap %p unmapped\n", addr);
+		return;
+	}
+
+	list_del(&mapped_mem->node);
+
+	free(mapped_mem);
+}
+
+struct panwrap_mapped_memory *panwrap_find_mapped_mem(void *addr)
+{
+	list_for_each_entry(struct panwrap_mapped_memory, pos, &mmaps.node, node) {
+		if (pos->addr == addr)
+			return pos;
+	}
+
+	return NULL;
+}
+
+struct panwrap_mapped_memory *panwrap_find_mapped_mem_containing(void *addr)
+{
+	list_for_each_entry(struct panwrap_mapped_memory, pos, &mmaps.node, node) {
+		if (addr >= pos->addr && addr < pos->addr + pos->length)
+			return pos;
+	}
+
+	return NULL;
+}
+
+struct panwrap_mapped_memory *panwrap_find_mapped_gpu_mem(mali_ptr addr)
+{
+	list_for_each_entry(struct panwrap_mapped_memory, pos, &mmaps.node, node) {
+		if (pos->gpu_va == addr)
+			return pos;
+	}
+
+	return NULL;
+}
+
+struct panwrap_mapped_memory *panwrap_find_mapped_gpu_mem_containing(mali_ptr addr)
+{
+	list_for_each_entry(struct panwrap_mapped_memory, pos, &mmaps.node, node) {
+		if (addr >= pos->gpu_va && addr < pos->gpu_va + pos->length)
+			return pos;
+	}
+
+	return NULL;
+}
+
+void __attribute__((noreturn))
+__panwrap_fetch_mem_err(const struct panwrap_mapped_memory *mem,
+			mali_ptr gpu_va, size_t size,
+			int line, const char *filename)
+{
+	panwrap_indent = 0;
+	panwrap_msg("\n");
+
+	panwrap_msg("INVALID GPU MEMORY ACCESS @"
+		    MALI_PTR_FMT " - " MALI_PTR_FMT ":\n",
+		    gpu_va, gpu_va + size);
+	panwrap_msg("Occurred at line %d of %s\n", line, filename);
+
+	if (mem) {
+		panwrap_msg("Mapping information:\n");
+		panwrap_indent++;
+		panwrap_msg("CPU VA: %p - %p\n",
+			    mem->addr, mem->addr + mem->length - 1);
+		panwrap_msg("GPU VA: " MALI_PTR_FMT " - " MALI_PTR_FMT "\n",
+			    mem->gpu_va,
+			    (mali_ptr)(mem->gpu_va + mem->length - 1));
+		panwrap_msg("Length: %zu bytes\n", mem->length);
+		panwrap_indent--;
+
+		if (!(mem->prot & MALI_MEM_PROT_CPU_RD))
+			panwrap_msg("Memory is only accessible from GPU\n");
+		else
+			panwrap_msg("Access length was out of bounds\n");
+	} else {
+		panwrap_msg("GPU memory is not contained within known GPU VA mappings\n");
+
+		list_for_each_entry(struct panwrap_mapped_memory, pos, &mmaps.node, node) {
+			panwrap_msg(MALI_PTR_FMT " (%p)\n", pos->gpu_va, pos->addr);
+		}
+	}
+
+	panwrap_log_flush();
+	abort();
+}
+
+PANLOADER_CONSTRUCTOR {
+	list_inithead(&allocations.node);
+	list_inithead(&mmaps.node);
+}
diff --git a/src/gallium/drivers/panfrost/panwrap/panwrap-mmap.h b/src/gallium/drivers/panfrost/panwrap/panwrap-mmap.h
new file mode 100644
index 0000000..06577f3
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/panwrap-mmap.h
@@ -0,0 +1,132 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#ifndef __MMAP_TRACE_H__
+#define __MMAP_TRACE_H__
+
+#include <mali-ioctl.h>
+#include <stdlib.h>
+#include <stddef.h>
+#include "panwrap.h"
+#include "util/list.h"
+
+struct panwrap_allocated_memory {
+	struct list_head node;
+
+	mali_ptr gpu_va;
+	int flags;
+	int allocation_number;
+	size_t length;
+};
+
+struct panwrap_mapped_memory {
+	struct list_head node;
+
+	size_t length;
+
+	void *addr;
+	mali_ptr gpu_va;
+	int prot;
+        int flags;
+
+	int allocation_number;
+	char name[32];
+
+	bool* touched;
+};
+
+/* Set this if you don't want your life to be hell while debugging */
+#define DISABLE_CPU_CACHING 1
+
+#define TOUCH_MEMSET(mem, addr, sz, offset) \
+	memset((mem)->touched + (((addr) - (mem)->gpu_va) / sizeof(uint32_t)), 1, ((sz) - (offset)) / sizeof(uint32_t)); \
+	panwrap_log("\n");
+
+#define TOUCH_LEN(mem, addr, sz, ename, number, dyn) \
+	TOUCH_MEMSET(mem, addr, sz, 0) \
+	panwrap_log("mali_ptr %s_%d_p = pandev_upload(%d, NULL, alloc_gpu_va_%d, %s, &%s_%d, sizeof(%s_%d), true);\n\n", ename, number, (dyn && 0) ? -1 : (int) (((addr) - (mem)->gpu_va)), (mem)->allocation_number, (mem)->name, ename, number, ename, number);
+
+/* Job payloads are touched somewhat different than other structures, due to the
+ * variable lengths and odd packing requirements */
+
+#define TOUCH_JOB_HEADER(mem, addr, sz, offset, number) \
+	TOUCH_MEMSET(mem, addr, sz, offset) \
+	panwrap_log("mali_ptr job_%d_p = pandev_upload(%d, NULL, alloc_gpu_va_%d, %s, &job_%d, sizeof(job_%d) - %d, true);\n\n", number, (uint32_t) ((addr) - (mem)->gpu_va), mem->allocation_number, mem->name, number, number, offset);
+
+#define TOUCH_SEQUENTIAL(mem, addr, sz, ename, number) \
+	TOUCH_MEMSET(mem, addr, sz, 0) \
+	panwrap_log("mali_ptr %s_%d_p = pandev_upload_sequential(alloc_gpu_va_%d, %s, &%s_%d, sizeof(%s_%d));\n\n", ename, number, mem->allocation_number, mem->name, ename, number, ename, number);
+
+/* Syntax sugar for sanely sized objects */
+
+#define TOUCH(mem, addr, obj, ename, number, dyn) \
+	//TOUCH_LEN(mem, addr, sizeof(typeof(obj)), ename, number, dyn)
+
+void replay_memory();
+void replay_memory_specific(struct panwrap_mapped_memory *pos, int offset, int len);
+char *pointer_as_memory_reference(mali_ptr ptr);
+
+void panwrap_track_allocation(mali_ptr gpu_va, int flags, int number, size_t length);
+void panwrap_track_mmap(mali_ptr gpu_va, void *addr, size_t length,
+                        int prot, int flags);
+void panwrap_track_munmap(void *addr);
+
+struct panwrap_mapped_memory *panwrap_find_mapped_mem(void *addr);
+struct panwrap_mapped_memory *panwrap_find_mapped_mem_containing(void *addr);
+struct panwrap_mapped_memory *panwrap_find_mapped_gpu_mem(mali_ptr addr);
+struct panwrap_mapped_memory *panwrap_find_mapped_gpu_mem_containing(mali_ptr addr);
+
+void panwrap_assert_gpu_same(const struct panwrap_mapped_memory *mem,
+			     mali_ptr gpu_va, size_t size,
+			     const unsigned char *data);
+void panwrap_assert_gpu_mem_zero(const struct panwrap_mapped_memory *mem,
+				 mali_ptr gpu_va, size_t size);
+
+void __attribute__((noreturn))
+__panwrap_fetch_mem_err(const struct panwrap_mapped_memory *mem,
+			mali_ptr gpu_va, size_t size,
+			int line, const char *filename);
+
+static inline void *
+__panwrap_fetch_gpu_mem(const struct panwrap_mapped_memory *mem,
+			mali_ptr gpu_va, size_t size,
+			int line, const char *filename)
+{
+	if (!mem)
+		mem = panwrap_find_mapped_gpu_mem_containing(gpu_va);
+
+	if (!mem ||
+	    size + (gpu_va - mem->gpu_va) > mem->length ||
+	    !(mem->prot & MALI_MEM_PROT_CPU_RD))
+		__panwrap_fetch_mem_err(mem, gpu_va, size, line, filename);
+
+	return mem->addr + gpu_va - mem->gpu_va;
+}
+
+#define panwrap_fetch_gpu_mem(mem, gpu_va, size) \
+	__panwrap_fetch_gpu_mem(mem, gpu_va, size, __LINE__, __FILE__)
+
+/* Returns a validated pointer to mapped GPU memory with the given pointer type,
+ * size automatically determined from the pointer type
+ */
+#define PANWRAP_PTR(mem, gpu_va, type) \
+	((type*)(__panwrap_fetch_gpu_mem(mem, gpu_va, sizeof(type), \
+					 __LINE__, __FILE__)))
+
+/* Usage: <variable type> PANWRAP_PTR_VAR(name, mem, gpu_va) */
+#define PANWRAP_PTR_VAR(name, mem, gpu_va) \
+	name = __panwrap_fetch_gpu_mem(mem, gpu_va, sizeof(*name), \
+				       __LINE__, __FILE__)
+
+#endif /* __MMAP_TRACE_H__ */
diff --git a/src/gallium/drivers/panfrost/panwrap/panwrap-syscall.c b/src/gallium/drivers/panfrost/panwrap/panwrap-syscall.c
new file mode 100644
index 0000000..7bb080c
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/panwrap-syscall.c
@@ -0,0 +1,452 @@
+/*
+ * © Copyright 2017 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <stdbool.h>
+#include <stdarg.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <pthread.h>
+#include <linux/ioctl.h>
+#include <sys/ioctl.h>
+#include <math.h>
+#include <sys/mman.h>
+#include <unistd.h>
+#include <linux/limits.h>
+#include <sys/stat.h>
+#include <errno.h>
+#include <ctype.h>
+
+#include <assert.h>
+#include <mali-ioctl.h>
+#include "panwrap.h"
+
+static pthread_mutex_t l;
+PANLOADER_CONSTRUCTOR {
+	pthread_mutexattr_t mattr;
+
+	pthread_mutexattr_init(&mattr);
+	pthread_mutexattr_settype(&mattr, PTHREAD_MUTEX_RECURSIVE);
+	pthread_mutex_init(&l, &mattr);
+	pthread_mutexattr_destroy(&mattr);
+}
+
+#define IOCTL_CASE(request) (_IOWR(_IOC_TYPE(request), _IOC_NR(request), \
+				   _IOC_SIZE(request)))
+
+struct ioctl_info {
+	const char *name;
+};
+
+struct device_info {
+	const char *name;
+	const struct ioctl_info info[MALI_IOCTL_TYPE_COUNT][_IOC_NR(0xffffffff)];
+};
+
+typedef void* (mmap_func)(void *, size_t, int, int, int, loff_t);
+typedef int (open_func)(const char *, int flags, ...);
+
+#define IOCTL_TYPE(type) [type - MALI_IOCTL_TYPE_BASE] =
+#define IOCTL_INFO(n) [_IOC_NR(MALI_IOCTL_##n)] = { .name = #n }
+static struct device_info mali_info = {
+	.name = "mali",
+	.info = {
+		IOCTL_TYPE(0x80) {
+			IOCTL_INFO(GET_VERSION),
+		},
+		IOCTL_TYPE(0x82) {
+			IOCTL_INFO(MEM_ALLOC),
+			IOCTL_INFO(MEM_IMPORT),
+			IOCTL_INFO(JOB_SUBMIT),
+		},
+	},
+};
+#undef IOCTL_INFO
+#undef IOCTL_TYPE
+
+static inline const struct ioctl_info *
+ioctl_get_info(unsigned long int request)
+{
+	return &mali_info.info[_IOC_TYPE(request) - MALI_IOCTL_TYPE_BASE]
+	                      [_IOC_NR(request)];
+}
+
+static int mali_fd = 0;
+
+#define LOCK()   pthread_mutex_lock(&l);
+#define UNLOCK() panwrap_log_flush(); pthread_mutex_unlock(&l)
+
+#define FLAG_INFO(flag) { MALI_JD_REQ_##flag, "MALI_JD_REQ_" #flag }
+static const struct panwrap_flag_info jd_req_flag_info[] = {
+	FLAG_INFO(FS),
+	FLAG_INFO(CS),
+	FLAG_INFO(T),
+	FLAG_INFO(CF),
+	FLAG_INFO(V),
+	FLAG_INFO(FS_AFBC),
+	FLAG_INFO(EVENT_COALESCE),
+	FLAG_INFO(COHERENT_GROUP),
+	FLAG_INFO(PERMON),
+	FLAG_INFO(EXTERNAL_RESOURCES),
+	FLAG_INFO(ONLY_COMPUTE),
+	FLAG_INFO(SPECIFIC_COHERENT_GROUP),
+	FLAG_INFO(EVENT_ONLY_ON_FAILURE),
+	FLAG_INFO(EVENT_NEVER),
+	FLAG_INFO(SKIP_CACHE_START),
+	FLAG_INFO(SKIP_CACHE_END),
+	{}
+};
+#undef FLAG_INFO
+
+#define SOFT_FLAG(flag)                                  \
+	case MALI_JD_REQ_SOFT_##flag:                    \
+		panwrap_log_cont("MALI_JD_REQ_%s", "SOFT_" #flag); \
+		break
+/* Decodes the actual jd_core_req flags, but not their meanings */
+void
+ioctl_log_decoded_jd_core_req(mali_jd_core_req req)
+{
+	if (req & MALI_JD_REQ_SOFT_JOB) {
+		/* External resources are allowed in e.g. replay jobs */
+
+		if (req & MALI_JD_REQ_EXTERNAL_RESOURCES) {
+			panwrap_log_cont("MALI_JD_REQ_EXTERNAL_RESOURCES | ");
+			req &= ~(MALI_JD_REQ_EXTERNAL_RESOURCES);
+		}
+
+		switch (req) {
+		SOFT_FLAG(DUMP_CPU_GPU_TIME);
+		SOFT_FLAG(FENCE_TRIGGER);
+		SOFT_FLAG(FENCE_WAIT);
+		SOFT_FLAG(REPLAY);
+		SOFT_FLAG(EVENT_WAIT);
+		SOFT_FLAG(EVENT_SET);
+		SOFT_FLAG(EVENT_RESET);
+		SOFT_FLAG(DEBUG_COPY);
+		SOFT_FLAG(JIT_ALLOC);
+		SOFT_FLAG(JIT_FREE);
+		SOFT_FLAG(EXT_RES_MAP);
+		SOFT_FLAG(EXT_RES_UNMAP);
+		default: panwrap_log_cont("0x%010x", req); break;
+		}
+	} else {
+		panwrap_log_decoded_flags(jd_req_flag_info, req);
+	}
+}
+#undef SOFT_FLAG
+
+static int job_count = 0;
+
+static void emit_atoms(void *ptr, bool bifrost) {
+	const struct mali_ioctl_job_submit *args = ptr;
+	const struct mali_jd_atom_v2 *atoms = args->addr;
+
+	int job_no = job_count++;
+
+	int job_numbers[256] = { 0 };
+
+	for (int i = 0; i < args->nr_atoms; i++) {
+		const struct mali_jd_atom_v2 *a = &atoms[i];
+
+		if (a->jc) {
+			int req = a->core_req | a->compat_core_req;
+
+			if (!(req & MALI_JD_REQ_SOFT_JOB))
+				job_numbers[i] = panwrap_replay_jc(a->jc, bifrost);
+			else if (req & MALI_JD_REQ_SOFT_REPLAY)
+				job_numbers[i] = panwrap_replay_soft_replay(a->jc);
+		}
+	}
+
+	for (int i = 0; i < args->nr_atoms; i++) {
+		const struct mali_jd_atom_v2 *a = &atoms[i];
+
+		if (a->ext_res_list) {
+			panwrap_log("mali_external_resource resources_%d_%d[] = {\n", job_no, i);
+			panwrap_indent++;
+
+			for (int j = 0; j < a->nr_ext_res; j++) {
+				/* Substitute in our framebuffer */
+				panwrap_log("framebuffer_va | MALI_EXT_RES_ACCESS_EXCLUSIVE,\n");
+			}
+
+			panwrap_indent--;
+			panwrap_log("};\n\n");
+
+		}
+	}
+
+	panwrap_log("struct mali_jd_atom_v2 atoms_%d[] = {\n", job_no);
+	panwrap_indent++;
+
+	for (int i = 0; i < args->nr_atoms; i++) {
+		const struct mali_jd_atom_v2 *a = &atoms[i];
+
+		panwrap_log("{\n");
+		panwrap_indent++;
+
+		panwrap_prop("jc = job_%d_p", job_numbers[i]);
+
+		/* Don't passthrough udata; it's nondeterministic and for userspace use only */
+
+		panwrap_prop("nr_ext_res = %d", a->nr_ext_res);
+
+		if (a->ext_res_list)
+			panwrap_prop("ext_res_list = resources_%d_%d", job_no, i);
+
+		if (a->compat_core_req)
+			panwrap_prop("compat_core_req = 0x%x", a->compat_core_req);
+
+		if (a->core_req) {
+			/* Note that older kernels prefer compat_core_req... */
+			panwrap_log(".core_req = ");
+			ioctl_log_decoded_jd_core_req(a->core_req);
+			panwrap_log_cont(",\n");
+		}
+
+		panwrap_log(".pre_dep = {\n");
+		panwrap_indent++;
+		for (int j = 0; j < ARRAY_SIZE(a->pre_dep); j++) {
+			if (a->pre_dep[j].dependency_type || a->pre_dep[j].atom_id)
+				panwrap_log("{ .atom_id = %d, .dependency_type = %d },\n",
+					    a->pre_dep[j].atom_id, a->pre_dep[j].dependency_type);
+		}
+		panwrap_indent--;
+		panwrap_log("},\n");
+
+		/* TODO: Compute atom numbers dynamically and correctly */
+		panwrap_prop("atom_number = %d + %d*%s", a->atom_number, 3, "i");
+
+		panwrap_prop("prio = %d", a->prio);
+		panwrap_prop("device_nr = %d", a->device_nr);
+
+		panwrap_indent--;
+		panwrap_log("},\n");
+
+	}
+
+	panwrap_indent--;
+	panwrap_log("};\n\n");
+}
+
+static inline void
+ioctl_decode_pre_job_submit(unsigned long int request, void *ptr)
+{
+	const struct mali_ioctl_job_submit *args = ptr;
+	const struct mali_jd_atom_v2 *atoms = args->addr;
+
+	panwrap_prop("addr = atoms_%d", job_count - 1); /* XXX */
+	panwrap_prop("nr_atoms = %d", args->nr_atoms);
+	panwrap_prop("stride = %d", args->stride);
+
+	assert (args->stride == sizeof(*atoms));
+}
+
+/**
+ * Overriden libc functions start here
+ */
+static inline int
+panwrap_open_wrap(open_func *func, const char *path, int flags, va_list args)
+{
+	mode_t mode = 0;
+	int ret;
+
+	if (flags & O_CREAT) {
+		mode = (mode_t) va_arg(args, int);
+		ret = func(path, flags, mode);
+	} else {
+		ret = func(path, flags);
+	}
+
+	LOCK();
+	if (ret != -1 && strcmp(path, "/dev/mali0") == 0)
+		mali_fd = ret;
+	UNLOCK();
+
+	return ret;
+}
+
+//#ifdef IS_OPEN64_SEPERATE_SYMBOL
+int
+open(const char *path, int flags, ...)
+{
+	PROLOG(open);
+	va_list args;
+	va_start(args, flags);
+	int o = panwrap_open_wrap(orig_open, path, flags, args);
+	va_end(args);
+	return o;
+}
+//#endif
+
+#if 0
+int
+open64(const char *path, int flags, ...)
+{
+	PROLOG(open64);
+	va_list args;
+	va_start(args, flags);
+	int o = panwrap_open_wrap(orig_open64, path, flags, args);
+	va_end(args);
+	return o;
+}
+#endif
+
+int
+close(int fd)
+{
+	PROLOG(close);
+
+        /* Intentionally racy: prevents us from trying to hold the global mutex
+         * in calls from system libraries */
+        if (fd <= 0 || !mali_fd || fd != mali_fd)
+                return orig_close(fd);
+
+	LOCK();
+	if (!fd || fd != mali_fd) {
+		panwrap_log("/dev/mali0 closed\n");
+		mali_fd = 0;
+	}
+	UNLOCK();
+
+	return orig_close(fd);
+}
+
+/* Global count of ioctls, for replay purposes */
+
+static int ioctl_count = 0;
+
+/* HW version */
+static bool bifrost = false;
+
+/* XXX: Android has a messed up ioctl signature */
+int ioctl(int fd, unsigned long int _request, ...)
+{
+	int number;
+	PROLOG(ioctl);
+	unsigned long int request = _request;
+	int ioc_size = _IOC_SIZE(request);
+	int ret;
+	void *ptr;
+
+	if (ioc_size) {
+		va_list args;
+
+		va_start(args, _request);
+		ptr = va_arg(args, void *);
+		va_end(args);
+	} else {
+		ptr = NULL;
+	}
+
+	if (fd && fd != mali_fd)
+		return orig_ioctl(fd, request, ptr);
+
+	LOCK();
+
+	number = ioctl_count++;
+
+	if (IOCTL_CASE(request) == IOCTL_CASE(MALI_IOCTL_JOB_SUBMIT)) {
+		emit_atoms(ptr, bifrost);
+		ioctl_decode_pre_job_submit(request, ptr);
+	}
+
+
+	ret = orig_ioctl(fd, request, ptr);
+
+	/* Track memory allocation if needed  */
+	if (IOCTL_CASE(request) == IOCTL_CASE(MALI_IOCTL_MEM_ALLOC)) {
+		const struct mali_ioctl_mem_alloc *args = ptr;
+
+		panwrap_track_allocation(args->gpu_va, args->flags, number, args->va_pages * 4096);
+	}
+
+	/* Call the actual ioctl */
+
+	UNLOCK();
+	return ret;
+}
+
+static inline void *panwrap_mmap_wrap(mmap_func *func,
+				      void *addr, size_t length, int prot,
+				      int flags, int fd, loff_t offset)
+{
+	void *ret;
+
+	if (!mali_fd || fd != mali_fd)
+		return func(addr, length, prot, flags, fd, offset);
+
+	LOCK();
+	ret = func(addr, length, prot, flags, fd, offset);
+
+	switch (offset) { /* offset == gpu_va */
+	case MALI_MEM_MAP_TRACKING_HANDLE:
+		/* MTP is mapped automatically for us by pandev_open */
+		break;
+	default:
+		panwrap_track_mmap(offset, ret, length, prot, flags);
+		break;
+	}
+
+	UNLOCK();
+	return ret;
+}
+
+#if 0
+void *mmap64(void *addr, size_t length, int prot, int flags, int fd,
+	     loff_t offset)
+{
+	PROLOG(mmap64);
+
+	return panwrap_mmap_wrap(orig_mmap64, addr, length, prot, flags, fd,
+				 offset);
+}
+#endif
+
+//#ifdef IS_MMAP64_SEPERATE_SYMBOL
+void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset)
+{
+#ifdef __LP64__
+	PROLOG(mmap);
+
+	return panwrap_mmap_wrap(orig_mmap, addr, length, prot, flags, fd,
+				 offset);
+#else
+	return mmap64(addr, length, prot, flags, fd, (loff_t) offset);
+#endif
+}
+//#endif
+
+int munmap(void *addr, size_t length)
+{
+	int ret;
+	struct panwrap_mapped_memory *mem;
+	PROLOG(munmap);
+
+	if (!mali_fd)
+		return orig_munmap(addr, length);
+
+	LOCK();
+	ret = orig_munmap(addr, length);
+	mem = panwrap_find_mapped_mem(addr);
+	if (!mem)
+		goto out;
+
+	free(mem);
+out:
+	UNLOCK();
+	return ret;
+}
diff --git a/src/gallium/drivers/panfrost/panwrap/panwrap-util.c b/src/gallium/drivers/panfrost/panwrap/panwrap-util.c
new file mode 100644
index 0000000..1ef52ca
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/panwrap-util.c
@@ -0,0 +1,133 @@
+/*
+ * © Copyright 2017 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdbool.h>
+#include <stdarg.h>
+#include <string.h>
+#include <errno.h>
+#include <time.h>
+#include <ctype.h>
+#include "panwrap.h"
+
+static FILE *log_output;
+
+short panwrap_indent = 0;
+
+void
+panwrap_log_decoded_flags(const struct panwrap_flag_info *flag_info,
+			  u64 flags)
+{
+	bool decodable_flags_found = false;
+
+	for (int i = 0; flag_info[i].name; i++) {
+		if ((flags & flag_info[i].flag) != flag_info[i].flag)
+			continue;
+
+		if (!decodable_flags_found) {
+			decodable_flags_found = true;
+		} else {
+			panwrap_log_cont(" | ");
+		}
+
+		panwrap_log_cont("%s", flag_info[i].name);
+
+		flags &= ~flag_info[i].flag;
+	}
+
+	if (decodable_flags_found) {
+		if (flags)
+			panwrap_log_cont(" | 0x%" PRIx64, flags);
+	} else {
+		panwrap_log_cont("0x%" PRIx64, flags);
+	}
+}
+
+/**
+ * Grab the location of a symbol from the system's libc instead of our
+ * preloaded one
+ */
+void *
+__rd_dlsym_helper(const char *name)
+{
+	static void *libc_dl;
+	void *func;
+
+	if (!libc_dl)
+		libc_dl = dlopen("libc.so", RTLD_LAZY);
+	if (!libc_dl)
+		libc_dl = dlopen("libc.so.6", RTLD_LAZY);
+	if (!libc_dl) {
+		fprintf(stderr, "Failed to dlopen libc: %s\n", dlerror());
+		exit(-1);
+	}
+
+	func = dlsym(libc_dl, name);
+	if (!func) {
+		fprintf(stderr, "Failed to find %s: %s\n", name, dlerror());
+		exit(-1);
+	}
+
+	return func;
+}
+
+void
+panwrap_log_empty()
+{
+	for (int i = 0; i < panwrap_indent; i++) {
+		fputs("  ", log_output);
+	}
+}
+
+void
+panwrap_log_typed(enum panwrap_log_type type, const char *format, ...)
+{
+	va_list ap;
+
+	panwrap_log_empty();
+
+	if (type == PANWRAP_MESSAGE)
+		fputs("// ", log_output);
+	else if (type == PANWRAP_PROPERTY)
+		fputs(".", log_output);
+
+	va_start(ap, format);
+	vfprintf(log_output, format, ap);
+	va_end(ap);
+
+	if (type == PANWRAP_PROPERTY)
+		fputs(",\n", log_output);
+}
+
+/* Eventually this function might do more */
+void
+panwrap_log_cont(const char *format, ...)
+{
+	va_list ap;
+
+	va_start(ap, format);
+	vfprintf(log_output, format, ap);
+	va_end(ap);
+}
+
+void
+panwrap_log_flush()
+{
+	fflush(log_output);
+}
+
+PANLOADER_CONSTRUCTOR {
+	log_output = stdout;
+}
diff --git a/src/gallium/drivers/panfrost/panwrap/panwrap-util.h b/src/gallium/drivers/panfrost/panwrap/panwrap-util.h
new file mode 100644
index 0000000..533ab85
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/panwrap-util.h
@@ -0,0 +1,58 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+/**
+ * Miscellanious utilities
+ */
+
+#ifndef __PANLOADER_UTIL_H__
+#define __PANLOADER_UTIL_H__
+
+#include <mali-int.h>
+#include "util/macros.h"
+
+#define __PASTE_TOKENS(a, b) a ## b
+/*
+ * PASTE_TOKENS(a, b):
+ *
+ * Expands a and b, then concatenates the resulting tokens
+ */
+#define PASTE_TOKENS(a, b) __PASTE_TOKENS(a, b)
+
+#define PANLOADER_CONSTRUCTOR \
+       static void __attribute__((constructor)) PASTE_TOKENS(__panloader_ctor_l, __LINE__)()
+
+#define PANLOADER_DESTRUCTOR \
+       static void __attribute__((destructor)) PASTE_TOKENS(__panloader_dtor_l, __LINE__)()
+
+/* Semantic logging type.
+ *
+ * Raw: for raw messages to be printed as is.
+ * Message: for helpful information to be commented out in replays.
+ * Property: for properties of a struct
+ *
+ * Use one of panwrap_log, panwrap_msg, or panwrap_prop as syntax sugar.
+ */
+
+enum panwrap_log_type {
+	PANWRAP_RAW,
+	PANWRAP_MESSAGE,
+	PANWRAP_PROPERTY
+};
+
+#define panwrap_log(...)  panwrap_log_typed(PANWRAP_RAW,      __VA_ARGS__)
+#define panwrap_msg(...)  panwrap_log_typed(PANWRAP_MESSAGE,  __VA_ARGS__)
+#define panwrap_prop(...) panwrap_log_typed(PANWRAP_PROPERTY, __VA_ARGS__)
+
+#endif /* __PANLOADER_UTIL_H__ */
diff --git a/src/gallium/drivers/panfrost/panwrap/panwrap.h b/src/gallium/drivers/panfrost/panwrap/panwrap.h
new file mode 100644
index 0000000..b2b16ad
--- /dev/null
+++ b/src/gallium/drivers/panfrost/panwrap/panwrap.h
@@ -0,0 +1,61 @@
+/*
+ * © Copyright 2017-2018 The Panfrost Community
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * A copy of the license is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+/*
+ * Various bits and pieces of this borrowed from the freedreno project, which
+ * borrowed from the lima project.
+ */
+
+#ifndef __WRAP_H__
+#define __WRAP_H__
+
+#include <dlfcn.h>
+#include <stdbool.h>
+#include <time.h>
+#include "panwrap-util.h"
+#include "panwrap-mmap.h"
+#include "panwrap-decoder.h"
+
+struct panwrap_flag_info {
+	u64 flag;
+	const char *name;
+};
+
+#define PROLOG(func) 					\
+	static __typeof__(func) *orig_##func = NULL;	\
+	if (!orig_##func)				\
+		orig_##func = __rd_dlsym_helper(#func);	\
+
+void __attribute__((format (printf, 2, 3))) panwrap_log_typed(enum panwrap_log_type type, const char *format, ...);
+void __attribute__((format (printf, 1, 2))) panwrap_log_cont(const char *format, ...);
+void panwrap_log_empty(void);
+void panwrap_log_flush(void);
+
+void panwrap_log_decoded_flags(const struct panwrap_flag_info *flag_info,
+			       u64 flags);
+void ioctl_log_decoded_jd_core_req(mali_jd_core_req req);
+void panwrap_log_hexdump(const void *data, size_t size);
+void panwrap_log_hexdump_trimmed(const void *data, size_t size);
+
+void panwrap_timestamp(struct timespec *);
+
+bool panwrap_parse_env_bool(const char *env, bool def);
+long panwrap_parse_env_long(const char *env, long def);
+const char * panwrap_parse_env_string(const char *env, const char *def);
+
+extern short panwrap_indent;
+
+void * __rd_dlsym_helper(const char *name);
+
+#endif /* __WRAP_H__ */
-- 
2.7.4

